{"id": "K9rks4lk8CC", "review": "This work proposes to constrain the regularization term of Conservative Q-Learning by enforcing to lower-bound the learned Q value by the (FA-estimated) empirical Q value of the dataset. Such lower-bounded Q values are said to be _calibrated_. This change is motivated by the observation that pretrained Q-values obtained from CQL get very negative, and that when switching to the online phase the Q-values jump back to values closer to the average return. By avoiding these extremely negative values during training, Cal-QL empirically appears to learn faster during the online finetuning phase.\n\nSome thoughts:\n- The language around bounding is a bit confusing at times. From what I understand, $Q^\\mu$ acts as a **lower-bound** to $Q_\\theta$. We want $Q_\\theta(s, a)$ to be **at least as high as** $Q^\\mu(s,a)$. I'm not sure it's ideal to say that $Q_\\theta$ _upper bounds_ $Q_\\mu$, because $Q_\\theta$ is the quantity of interest. Usually the thing that is _bounded_ is what we are looking for, and it _bounded by_ other (often fixed) quantities. I think it would be easier to understand if the wording was \"we lower-bound $Q_\\theta$ by $Q^\\mu$\". \n- The word \"unlearning\" also may not be ideal, and feels like it's doing a lot of work, especially given that the evidence is about (a) the magnitude of Q-values (b) the success rate. During online training, the model is effectively learning about its environment, learning _not_ to take certain actions; that this happens in parameter space and not just in value & performance space matters. I would love to see a deeper analysis of what happens during this phase of low success rate.\n- In 4.2, I fail to see why this speculation is correct: \"the policy optimizer would not unlearn $\\pi$ in favor of a worse $\\mu$ upon observing new online data since $\\pi$ still attains a larger value under the learned $Q_\\theta$ function\". The only thing that a calibrated Q value tells us is that _predicted_ action-values are lower-bounded. This does _not_ imply that actual returns are. For both CQL and Cal-QL, the OOD $(s, a)$ are OOD, and the only difference is the prior we put on the solutions the function approximation is allowed to find, which may impact generalization or numerical stability.\n\nOverall the paper is a good workshop contribution. It takes a problem, investigates it, proposes an effective solution and benchmarks it. The writing is good. I think some aspects could be stronger, and things dug deeper, but this is otherwise good work by the simple metric that I learned something new.", "rating": "0", "sentences": [{"sentence_type": "positive", "sentence": "I would love to see a deeper analysis of what happens during this phase of low success rate."}], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Ariadne"], "models": [""]}}
{"id": "kf8TrSGeLr1", "review": "Summary:\nThe paper has two main contributions: \n1. It proposes a minimalist expansion to the BRAC algorithm that uses a mean-squared error for the action deviation penalty and has the popular TD3+BC algorithm as a special case.\n2. Explores design choices in the practical implementation of the algorithms, such as the use of layer normas and batch sizes.\n\nStrengths:\nThe proposed algorithm is pretty simple and straightforward. The implementation details are useful and seem to make a significant difference in the overall results. \n\n\nWeaknesses:\nThe paper evaluates it‚Äôs results on the standard d4rl benchmark locomotion tasks. With so many papers, approaches, design choices and hyper-parameters fine -tuning (from the paper: ‚ÄúŒ≤1 and Œ≤2 parameters from Equations 3 and 4 are carefully tuned‚Äù)  it is not clear whether the results are indeed significant or a matter of over-fitting to the datasets. It would be helpful to evaluate the more complex domains of the benchmark as well. This is also a completely offline single-task RL approach, which might not be the best fit for the workshop theme. \n\n\nConclusion:\nI would still recommend and accept as the proposed approach is simple and still achieves good results. The design experiments are useful to practical implementations and there is a renewed interest in this recently. \n", "rating": "1", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Ariadne"], "models": [""]}}
{"id": "BDjPqT77la", "review": "Strengths:\n\n- The paper tackles an interesting problem and it also applies when we have no access to the taxonomy.\n- The presentation is relatively clear and self contained.\n- The technical details appear sound.\n\nWeaknesses:\n\n- The novelty/originality is limited: box embeddings, probabilistic semantics of their intersections, gumbel boxes, bessel volume, etc are all adopted from existing literature. Propositions and the corollary appear more like observations rather than results. \n\n- The overall setup is not well-motivated. While I see why box embeddings can be useful for encoding class interactions and their hierarchies (which are in the form of directed acyclic graphs), it is not very clear to me whether one needs a probabilistic semantics as is used in this work. \n\n- The BoxE paper (Abboud et al, BoxE: A Box Embedding Model for Knowledge Base Completion, NeurIPS, 2020), which is also a box embedding model (proposed for link prediction), is very relevant for this work: It shows that boxes can capture arbitrary relational hierarchies in the space. This is shown for binary relations and therefore it is more general, but it clearly applies to class hierarchies which are only a special case. This suggests that one can model hierarchies using box embeddings in an even simpler way, where box containment implies a subclass relationship (without the probabilistic interpretation). If we already know the class taxonomy, we can even inject this information to the space, i.e., if C is a subclass of D, one can enforce the corresponding C-box to be contained in the corresponding D-box in the space, etc. That is, if the class taxonomy is known, one can provably enforce these using the ideas presented in (Abboud et al). If the taxonomy is not known, then it can still be learned.\n\n- The probabilistic interpretation of boxes may bring in some value, but it is not clear to me whether this is the case, after reading the paper. In fact, the above-outlined approach would achieve the similar goals in a simpler way in my understanding - This may possibly result in an even stronger baseline than the given baselines. Notice that when a C-box is contained in the D-box after training, any prediction for class C will be consistent, since it will also be a D by the space configuration (and there are many possible configurations that can achieve the same thing). \n\n\n", "rating": "1", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Ariadne"], "models": [""]}}
{"id": "rJghN08aFr", "review": "The paper proposes an approach to exploration by utilizing an intrinsic reward based on distances in a learned, evolving abstract representation space. The abstract space is learned utilizing both model-free and model-based losses, and the behaviour policy is based on planning combining the model-free and model-based components with an epsilon-greedy exploration strategy. Learning the abstract representation space itself is based on a previous work, but the contribution of this paper is the utility of it to design the reward bonus for exploration by utilizing distances in this evolving representation space.\n\nAs it stands, I am leaning towards rejecting the paper, for the following reasons.\n(1) while the idea proposed is interesting, the current work rather explores it in a limited manner which is unsatisfactory.\n(2) I think the presentation of the bonus itself -- novelty search (Section 4), which is the core of the paper, is rather unclear. (3) The assumption of deterministic transition dynamics may be ignored in favour of games which seem to be our benchmarks, but the results presented for the control tasks, Table 1, are not statistically significant, and the paper is missing details about the architecture/sweep for the baselines experimented with. \n(4) Parts of the paper is rather unclear/feels disconnected -- for instance, the interpretable abstract representation bit; this was a loss in the original work, and seems to be just mentioned arbitrarily here while the loss isn't really used (unless it is used, and not mentioned in the paper).\n(5) Overall, the proposed reward bonus is a heuristic whose specific design choice isn't statistically shown to be useful (Ablation in Appendix), and the empirical results comparing to other methods are underwhelming.\n\nHere are my main points of concern which I hope the authors address in the rebuttal:\n(1) Designing reward bonuses to induce exploratory behaviour in the agent has seen a surge of publications in the Deep RL literature in recent years. The key property all these methods aim for is a bonus that pushes the agent to the boundaries of its current \"known region\", and then rely on the stochasticity due to epsilon-greedy to cross that boundary -- pushing this boundary further. While this is different from exploration to reduce uncertainty, it is nonetheless a reasonable approach leading to competitive policies when evaluated in deep RL. But a characteristic all these bonuses aim for is that they fade away with time -- for instance count-based bonus are inversely proportional to visit counts, or prediction error bonuses go to 0 as the prediction becomes more accurate. But what do these novelty bonuses converge to? Is it just a stationary value based on consecutive loss parameter (in which case the hope is they don't affect the external reward scale, they just shift it uniformly)?\n(2) What exactly are the nearest neighbours? Is it a search based on the data in the buffer or is it a notion of temporal neighbours?\n(3) If it's temporal, why would there ever be biased for some states -- \"We do this in order..novel states\".\n(4) I was completely unable to understand the section in the Appendix which is making a case for the ranked weighting. If you have a succinct explanation for the heuristic it'd be great.\n(5) Further, as a heuristic it is mentioned that l2 norm may not be effective if the dimensionality of the representation space is increased. So why the heuristic? I think it either needs more empirical validation, or a theoretical justification.\n(6) While the evaluation scheme used in the paper to quantify the exploration of the behaviour policy is interesting -- y-axis of plots in Figure 4 for the Labyrinth task -- why/what exactly is the role of Figure 2? Is the interpretability loss used here? Is it to reason for utilizing e-greedy instead of a purely-greedy behaviour? I think this is a little unclear, and can be better clarified. Further, the distinction of primary and secondary features is interesting, but their clear demarcation is rather questionable in more complicated domains -- in the abstract space.\n(7) Do you have a hypothesis for why the 1-step value functions are not sufficient for decision making in this simple domain -- labyrinth - with the abstract representations?\n(8) If model-based algorithms get more steps to learn shouldn't model-free too? I'm not sure I understand the reasoning for the experiment design choice.\n(9) Whats the architecture used for Bootstrap DQN? It needs to have multiple heads -- but based on the current architecture that doesn't seem likely.\n(10) Are the extrinsic rewards ignored in learning -- \"only focus on intrinsic rewards\" (Section 6.2.2)? If they are for the proposed method, are they for the competitors too? If so why, and what is the reward for Bootstrap DQN?\n(11) I think the Discussion section raises interesting points about interpretability and metric learning, but I do think the conclusions drawn are a little inflated.\n(12) The ablation study in Section D of the Appendix is not statistically significant -- so why is wighted reward useful? Please comment.\n(13) How would stochasticity in transition dynamics affect the abstract representation space? Discussing this would be very interesting.\n(14) Learning curves for the control tasks?\n\nComments about typos/possible points of confusion:\n(1) The last para in Section 6.1 -- discusses \"open\" labyrinth heat map, then what do we mean by learning the dynamics of the wall? There is no wall in open, right?\n(2) In Section 4 -- I think x_{t+1} is an estimate from the unrolled model -- \\hat{x}_{t+1}? Further, it would be helpful to mention that it is an estimate based on the learned model.\n(3) n_freq is used in the pseudocode in the main paper -- but no mention of it to explain it is made in the main.\n(4) Contrasting the work to existing literature would be useful (in the Related Work section; as opposed to summarizing existing work).\n(5) buffered Q network --> target networks?\n", "rating": "1", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Ariadne"], "models": [""]}}
{"id": "ryYjvicxM", "review": "The authors propose to evaluate how well generative models fit the training set by analysing their data augmentation capacity, namely the benefit brought by training classifiers on mixtures of real/generated data, compared to training on real data only. Despite the the idea of exploiting generative models to perform data augmentation is interesting, using it as an evaluation metric does not constitute an innovative enough contribution. \n\nIn addition, there is a fundamental matter which the paper does not address: when evaluating a generative model, one should always ask himself what purpose the data is generated for. If the aim is to have realistic samples, a visual turing test is probably the best metric. If instead the purpose is to exploit the generated data for classification, well, in this case an evaluation of the impact of artificial data over training is a good option.\n\nPROS:\nThe idea is interesting. \n\nCONS:\n1. The authors did not relate the proposed evaluation metric to other metrics cited (e.g., the inception score, or a visual turing test, as discussed in the introduction). It would be interesting to understand how the different metrics relate. Moreover, the new metric is introduced with the following motivation ‚Äú[visual Turing test and Inception Score] do not indicate if the generator collapses to a particular mode of the data distribution‚Äù. The mode collapse issue is never discussed elsewhere in the paper. \n\n2. Only two datasets were considered, both extremely simple: generating MNIST digits is nearly a toy task nowadays. Different works on GANs make use of CIFAR-10 and SVHN, since they entail more variability: those two could be a good start. \n\n3. The authors should clarify if the method is specifically designed for GANs and VAEs. If not, section 2.1 should contain several other works (as in Table 1). \n\n4. One of the main statements of the paper ‚ÄúOur approach imposes a high entropy on P(Y) and gives unbiased indicator about entropy of both P(Y|X) and P(X|Y)‚Äù is never proved, nor discussed.\n\n5. Equation 2 (the proposed metric) is not convincing: taking the maximum over tau implies training many models with different fractions of generated data, which is expensive. Further, how many tau‚Äôs one should evaluate? In order to evaluate a generative model one should test on the generated data only (tau=1) I believe. In the worst case, the generator experiences mode collapse and performs badly. Differently, it can memorize the training data and performs as good as the baseline model. If it does actual data augmentation, it should perform better.\n\n6. The protocol of section 3 looks inconsistent with the aim of the work, which is to evaluate data augmentation capability of generative models. In fact, the limit of training with a fixed dataset is that the model ‚Äòsees‚Äô the data multiple times across epochs with the risk of memorizing. In the proposed protocol, the model ‚Äòsees‚Äô the generated data D_gen (which is fixed before training) multiple time across epochs. This clearly does not allow to fully evaluate the capability of the generative model to generate newer and newer samples with significant variability.\n\n\nMinor: \nSection 2.2 might be more readable it divided in two (exploitation and evaluation).   \n", "rating": "1", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Ariadne"], "models": [""]}}
{"id": "SJlghKO937", "review": "Summary: The paper proposes an algorithm for meta-learning which amounts to fixing the features (ie all hidden layers of a deep NN), and treating each task  as having its own final layer which could be a ridge regression or a logistic regression. The paper also proposes to separate the data for each task into a training set used to optimize the last, task specific layer, and a validation set used to optimize all previous layers and hyper parameters. \n\nNovelty: This texter is unsure what the paper claims as a novel contribution. In particular training multi-task neural nets with shared feature representation and task specific final layer is probably 20-30 years old by now and entirely common. It is also common freeze the feature representation learned from the first set of tasks, and to simply use it for new tasks by modifying the last (few) layer(s) which would according to this paper qualify as meta-learning since the new task can be learned with very few new examples. \n\n", "rating": "1", "sentences": [{"sentence_type": "2", "sentence": "In particular training multi-task neural nets with shared feature representation and task specific final layer is probably 20-30 years old by now and entirely common"}], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Ariadne"], "models": [""]}}
{"id": "GRm39Fj2E_", "review": "This paper presents, in a principled way, a novel and interesting way of applying Nesterov-alike Acceleration to adaptive DL optimizers. When applied on a number of optimizers, architectures and vision classification tasks, the accelerated version shows substantial improvements across the board. The authors claim that, thanks to the principled formulation, they are able to prove advantageous convergence properties in Theorem 1, appendix B, but I wasn't able to find said appendix nor the theorem on the paper (also, some of the provided references are unused, which may be a related issue).\n\nI think the idea of adaptive Nesterov is good and well presented, and experiments show clear advantages, so I propose to accept the paper as I consider it an interesting addition to the DL optimizer literature.\n\nComments\n* It is true that Adam is very popular and it generally helps a lot with convergence, but it has been theoretically shown to not converge, and even fail to generalize in some cases (https://opentext.net/forum?id=ryQu7f-RZ https://arxiv.org/abs/1509.01240)\n* While results look very compelling and the effort to is welcome, authors may want to consider extending to established optimizer benchmarks to enhance comparability (https://github.com/fsschneider/DeepOBS https://arxiv.org/abs/2007.01547)\n", "rating": "0", "sentences": [{"sentence_type": "positive", "sentence": "While results look very compelling and the effort to is welcome, authors may want to consider extending to established optimizer benchmarks to enhance comparability (https://github.com/fsschneider/DeepOBS https://arxiv.org/abs/2007.01547)"}], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Ariadne"], "models": [""]}}
{"id": "Nko_4qPt-o5", "review": "Through four studies, this paper proposes to lift a theoretical limitation in the application range of the Dual Gaussian Distribution Model, namely that it could also work when touch acquisition occurs from a touchscreen  to that same touchscreen.\n\nThis paper is well written and shows good experiment design and consistent analyses.  \nHowever I found the theoretical argument to use the DGDM in screen-to-screen pointing quite hard to follow, even though it is the main point of this article. I also have a number of concerns that I would like to see addressed in a revision.\n\n\n# BLAMING AGE\n\nHonestly, I found it quite a weak argument to put the lack of generalization of the approach on age (p. 10). Age difference is one among many possible explanations, but one in which this paper rushes in nevertheless, at the expense of any other. \nThe paper doesn't even acknowledge that this lack of success could simply be due to a lower external validity than the authors hoped for. As the authors state themselves p. 9, \"A common way to check external validity is to apply obtained parameters to data from different participants.\" Checking can also come up negative, and that is ok. These results remain valid, even if the proposed approach is not as context-independent as hoped.\nPerhaps worse, the paper immediately jumps from this patched-together explanation, straight to calling it a \"novel finding\", and then to suggesting design guidelines from it, as if it was now a proven fact.\nI think this part needs to be drastically shortened or even removed, in favor of a more realistic discussion about generalization---and possible lack thereof.\n\n\n# \"UNLIMITING\"\n\nI found it quite hard to understand the point of Bi et al. for rejecting screen-to-screen pointing, at least the way it is explained in this paper. That, in turn, makes it quite difficult to understand the counter-argument developed in this paper---and especially since \"The evidence comes from a study by Bi et al.\" (p. 4), which makes one wonder why Bi et al. put that \"limitation\" up in the first place.\n\nOne example, in the last paragraph before EXPERIMENTS (p. 4), a point is made that goes like this:\n- a lack of effect might be due to A values that are too close to each other, \n- even if A should in fact have an effect according to some model (Eq. 12), - and for some reason that makes it ok to consider that screen-to-screen pointing is compatible with Bi et al.'s model (which does not consider A).\n\n\n# DESIGN APPLICATIONS\n\nI am not sure that the possible applications of this model are well described or argued for in this paper. The described examples feel rather artificial.\n\n- In the example given in p. 1 (choosing between 5 or 7-mm circular icons), it is unclear why the designer would need a model, or to know by how much a 7-mm icon would improve accuracy. It seems that this sort of design issues can be solved using threshold values under which users simply cannot accurately acquire a target. I assume that strong design guidelines already exist for this?\n\n- Similar argument about the second and third paragraphs in p. 9. The level of detail argued here seems quite artificial, e.g. \"If designers want a hyperlink to have a 77% success rate\". I doubt many designers would consider a clickable, 2.4-mm high font or icon on a touch screen in any case. I might be wrong.\n\n- \"by reducing the time and cost of conducting user studies, our model will let them focus on other important tasks such as visual design and backend system development, which will indirectly contribute to implementing better, novel UIs.\" (p. 2)\nThat seems quite a stretched \"contribution\", at least in the absence of actual data about how long designers do spend on testing width values today.\n\n\n# AMOUNT OF ERROR\n\nThroughout the paper, prediction errors (additive) up to 10% are described as small, and that is surprising (5% in Exp 1, 10% in Exp 2, 7% in Exp 3, 10% in Exp 4).\n\nTo the best of my understanding, these are not percentages of prediction error (e.g. going from 50 to 55 is a 10% increase), which would be more ok. These are differences between values that are already expressed in percents. \nIn my experience, many pointing studies have error rates ranging from 0 to, say, 15%, perhaps more when the tasks or input devices make it particularly difficult. 2-mm targets on a touch device could definitely count as difficult. However, that still makes a 10% prediction error quite high in my book, and worthy of contextualization. Perhaps I misunderstood something.\n\n>> \"the error rate difference was |29 ‚àí 38| = 9%. Similarly, their 2D tasks showed only small differences in error rate, up to 2% at most.\"\n-\nFirst, for a metric that can often be between 0 and 15%, 2 and 9% are not \"similar\" values.\nSecond, 29% and 38% error seems alarmingly high.\n\n\n# CLARITY\n\nRemoving tap points that are further than a fixed distance away from the target center will likely affect W levels differently. I imagine that more of these errors occurred in the W=10mm condition. This would be good to report, either way, even though only a small number of trials was removed overall.\n\nFig. 12 should also show the actual success rates measured in these studies.", "rating": "2", "sentences": [{"sentence_type": "2", "sentence": "Perhaps worse, the paper immediately jumps from this patched-together explanation, straight to calling it a \"novel finding\", and then to suggesting design guidelines from it, as if it was now a proven fact."}], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Ariadne"], "models": [""]}}
{"id": "SklR0NgUpQ", "review": "---\nUpdate: I think the experiments are interesting and worthy of publication, but the exposition could be significantly improved. For example:\n\n- Not sure if Figure 1 is needed given the context.\n- Ablation study over the proposed method without sparse reward and hyperarameter \\alpha\n- Move section 7.3 into the main text and maybe cut some in the introduction\n- More detailed comparison with closely related work (second to last paragraph in related work section), and maybe reduce exposition on behavior cloning.\n\nI like the work, but I would keep the score as is.\n---\n\n\nThe paper proposes to use a \"minimal adversary\" in generative adversarial imitation learning under high-dimensional visual spaces. While the experiments are interesting, and some parts of the method has not been proposed (using CPC features / random projection features etc.), I fear that some of the contributions presented in the paper have appeared in recent literature, such as InfoGAIL (Li et al.).\n\n- Use of image features to facilitate training: InfoGAIL used pretrained ResNet features to deal with high-dimensional inputs, only training a small neural network at the end.\n- Tracking and warm restarts: InfoGAIL does not seem to require tracking a single expert trajectory, since it only classifies (s, a) pairs and is agnostic to the sequence.\n- Reward augmentation: also used in InfoGAIL, although they did not use sparse rewards for augmentation.\n\nAnother contribution claimed by this paper is that we could do GAIL without action information. Since we can shape the rewards for most of our environments that do not depend on actions, it is unsurprising that this could work when D only takes in state information. However, it is interesting that behavior cloning pretraining is not required in the high-dimensional cases; I am interested to see a comparison between with or w/o behavior cloning in terms of sample complexity. \n\nOne setting that could potentially be useful is where the expert and policy learner do not operate within the same environment dynamics (so actions could not be same) but we would still want to imitate the behavior visually (same state space). \n\nThe paper could also benefit from clearer descriptions, such as pointers to which part of the paper discusses \"special initialization, tracking, or warm starting\", etc., from the introduction.", "rating": "1", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Ariadne"], "models": [""]}}
{"id": "Clvpch4dzB2", "review": "This submission studies how the choice of activation function impacts the reproducibility of experiments involving deep networks. It proposes a new activation function with the goal of designing a smoothed ReLU, and provide experiments comparing it against other activations in terms of irreproducibility (measured via PD) and performance.\n\nThe problem of understanding how model design choices can have negative impacts on experimental reproducibility is interesting and timely, but I believe the paper does not provide a strong enough case for their approach and contributions.\n\nFirst, the adopted metric to measure irreproducibility, 'Prediction Difference (PD)', is never evaluated in terms of how sensible of a metric it is to capture reproducibility -- this also seems to be lacking in [1]. Actually , one can argue that it is not a sensible metric at all (except for its Hamming form), as it is not invariant to how the models are calibrated, as discussed below.\n\nFor example, take any binary classifier and consider two copies of it with different calibrations (i.e. scaling the output layer weights by positive scalars, one for each model): even though the models always agree on their predicted labels regardless of their calibrations, the PD can be made arbitrarily close to 0.5 by calibrating the models appropriately. Even more worrying is that the same can be done by taking a binary classifier and a copy of it with flipped predictions: the PD between the two can be made arbitrarily close to 0 by scaling their weights down. Note that this problem also happens with the relative PD.\n\nTo see how this is connected to the choice of activation functions (especially ReLU x SmeLU), note that for normally-distributed inputs (centered around the origin), gradient's variance of ReLU is 1/4 while for SmeLU it is approximately sigma^2 / (4 beta^2) for large enough beta (sigma^2 being the variance of the input distribution): this discrepancy can have a non-trivial impact on the model's calibration and cause differences in PD to be artifacts.\n\nSince this doesn't happen with the Hamming form of PD I believe Figure 15 in the Appendix to be the most informative one. However, it seems that different activations result in less than 1% prediction discrepancy across models, which is fairly insignificant and hence it is hard to argue that activations actually matter for reproducibility (at least from the presented experiments).\n\nLastly, it is hard to draw any conclusions from the presented experiments: the CTR results are based on a private dataset while the MNIST ones are extremely small-scale, with both the dataset and the model being arguably toy problems. There are numerous tasks where reproducibility is a prominent issue e.g. deep reinforcement learning, generative modelling (especially GANs), making training a 2-layer network on MNIST a poor choice to evaluate reproducibility problems.\n\nAs an additional note, the authors seem to rely heavily on the work of Shamir & Coviello '20 [1] which introduced the PD metric, even though the paper was only made publicly available on arXiv a week --after-- the texting period for this submission started. When citing papers which are yet to be made available it would be helpful to introduce and discuss the relevant content in a self-contained way -- while the authors avoided much of my confusion by presenting the full definition of the PD metric, the referred paper has useful information which was not discussed (such as which summand is normalized in its relative form and how the different variants compare).\n\nSince I have major concerns with the paper -- particularly on the reliability of PD as a metric and the unconvincing empirical results -- I am voting for rejection.\n\n[1] Shamir & Coviello, Anti-Distillation: Improving reproducibility of deep networks,\n\n\n------------\n\n\nUpdate after rebuttal:\n\n\"It appears that the comment made by the texter may stem from an assumption that two models which are compared for PD can be different in the operations they perform to generate the predictions.\"\n\nThis is incorrect, my text does not mention such assumption and my statements hold without it. As stated in my text, I consider models with different weight magnitudes, making no assumptions on the underlying cause.\n\n\"PD, as we defined in Section 2, is aimed explicitly at measuring differences between predictions of a set of models that are supposed to be identical in all their components\"\n\nIndeed, and my point is that comparing the PD of two sets of models that are not identical is also problematic **even if all models within each set are identical**, except for the PD in its Hamming form. More details below.\n\n\"Changing calibration between such models violates this assumption.\"\n\nPlease check the celebrated work of Guo et al., \"On Calibration of Modern Neural Networks\": calibration does not necessarily consist of an explicit, additional component that modifies the model, and the same model trained in different ways can present distinct calibrations. More specifically, two sets of models can have not only the same accuracy, but the exact same predictions (i.e. there is a 1-1 mapping from each model in one set to a model in the other set that has the exact same predictions for all data points) but vastly different internal calibrations, which will result in vastly different PDs (to be overly specific, the scalar PD of a set will be different from the scalar PD of the other set) even though the two sets agree \"point-wise\" in terms of predictions.\n\n\"If one changes something about one of the models (including how calibration is done), one would expect them to predict differently, and have different accuracies.\"\n\nThis is incorrect. First, I'm not assuming models are explicitly calibrated, only that they have distinct internal calibrations (confidences in terms of predicted probabilities, which depend mostly on the parameters' magnitudes). Second, \"scaling the output layer weights by positive scalars\" (quoting from my text) will not change a model's accuracy: while it changes the class-wise predicted probabilities, the rank of the logits is preserved. If the authors remain skeptical of this fact, let $\\phi(x)$ denote the activations of the previous to last layer of a model, and let $\\langle w_i, \\phi(x) \\rangle > \\langle w_j, \\phi(x) \\rangle$, where $w_i$ and $w_j$ are the weight vectors of output units respective to classes $i$ and $j$ (i.e. $p(y_i | x) > p(y_j | x)$ for probabilities produced by a softmax over logits). Then for any $\\alpha \\in \\mathbb R_+$, we have trivially that $\\langle \\alpha w_i, \\phi(x) \\rangle > \\langle \\alpha w_j, \\phi(x) \\rangle$ (hence $p'(y_i | x) > p'(y_j | x)$, for probabilities $p'$ computed from the new logits). Again, note that it is --not-- necessary for an external, explicit calibration factor $\\alpha$ to be employed: training the network differently, or even adopting a different activation function -- just consider $\\max(0, 10x)$ for clarity, which will scale $\\phi(x)$ by a positive factor and yield the same observation as above.\n\n\"Specifically, if one flips the predictions of a binary classifier, the flipped model will have much worse accuracy from the actual model of interest, and measuring PD at this point is irrelevant.\"\n\nThe fact that two classifiers with vastly different accuracies can have zero PD is worrying and shows that PD is not a trustworthy metric: claiming that such evaluation is 'irrelevant' and should not be done does not address the issue.\n\n\nSince the authors remained unconvinced that the PD is sensible to positive scalings of a model's parameters, and hence comparing the PDs of two sets of models with different activations (one activation per set) is not sensible, here is a more detailed explanation of this fact.\n\nAssume a fairly trivial example for clarity: two 1-d data points, $x_1 = +1, x_2 = -1$, and binary classification models $f_1, f_2$, where $f_1(x) = \\sigma(w_1 \\cdot \\phi(x))$ and $f_2(x) = \\sigma(w_2 \\cdot \\phi(x))$ are the assigned probabilities for the positive label, and $\\phi: \\mathbb R \\to \\mathbb R$ captures some notion of activation function and/or scale of weights before the final classification layer. For simplicity, let $\\phi(x) = \\alpha x$, for some $\\alpha \\in \\mathbb R_+$, and feel free to think of $\\alpha$ as a 'magnitude' of an activation function instead of some notion of internal calibration.\n\nThen, we have $P_{1,1} = (\\sigma(\\alpha w_1), \\sigma(-\\alpha w_1))$, $P_{1,2} = (\\sigma(\\alpha w_2), \\sigma(-\\alpha w_2))$, $P_{2,1} = (\\sigma(-\\alpha w_1), \\sigma(\\alpha w_1))$, and $P_{2,2} = (\\sigma(-\\alpha w_2), \\sigma(\\alpha w_2))$. The PD of the set consisting of the two defined models, after simplifying the 8 relevant terms, ends up being simply $\\Delta_1 = |\\sigma(\\alpha w_1) - \\sigma(\\alpha w_2)|$. Let's pick some numbers to make this crystal clear: let $\\alpha = 1, w_1 = 1.0, w_2 = 0.1$, so we get $\\Delta_1 = \\sigma(1) - \\sigma(0.1) \\approx 0.2$ (note that w.l.o.g. we can assume that $y_1 = +1, y_2 = -1$ so that for these weights both models achieve 100% accuracy).\n\nNow, take ANOTHER set, consisting of models $g_1, g_2$, defined similarly to $f_1, f_2$, but with $g_1(x) = \\sigma(w'_1 \\cdot \\phi'(x)), g_2(x) = \\sigma(w'_2 \\cdot \\phi'(x))$, where $\\phi'$ (not the derivative of $\\phi$) captures the the activation function and/or weight magnitude of layers preceding the classification head. Let $\\phi'(x) = \\beta x$ for simplicity. Consider the case where $\\beta = 0.1, w_1 = 1.0, w_2 = 0.1$, i.e. the weights of $g_1, g_2$ are *exactly the same* as the weights of $f_1, f_2$, but $\\phi'$ is a 'scaled-down' $\\phi$ (e.g. a different activation function): in this case (note that both $g_1$ and $g_2$ achieve 100% accuracy as well), **for this new set of models, consisting of the pair $g_1, g_2$**, we get $\\Delta_1 = \\sigma(0.1) - \\sigma(0.01) \\approx 0.02$, a value around 10 times smaller than the PD of the first set of models, **even though the second set predicts the exact same labels for each data point**, and claiming that the set $\\{g_1, g_2\\}$ is 'more robust' than the set $\\{f_1, f_2\\}$ in terms of reproducibility is simply factually wrong. If the idea of having $\\beta \\neq \\alpha$ sounds a bit of a stretch since the proposed activations are not simply 'scaled down' ReLUs, consider instead the case $\\beta = 1.0, w_1 = 0.1, w_2 = 0.01$ and note that we again get $\\Delta_1 \\approx 0.02$ for this second set of models: the discrepancy in terms of magnitude of weights can be caused by different optimizers, different strength of $\\ell_2$ regularization, or, as my original text already mentioned, smaller variance of gradients w.r.t. activation function.\n\nTo reiterate, in the above example we did **not**, at any point, compute the PD of a set of models that had different components: both $\\{f_1, f_2\\}$ (the first set) had the same 'activation function' $\\phi$, while $\\{g_1, g_2\\}$ had $\\phi'$.\n\nGoing a step further, which shows how problematic the PD is as a metric, consider an arbitrary set of binary classifiers $S_1 = \\{f_1, f_2, \\dots, f_M\\}$, where $f_i(x) = \\sigma( \\langle w_i, \\phi(x) \\rangle)$ is the probability assigned by the $i$'th model of $x$ belonging to the positive class. Now, take *another* set of binary classifiers $S_2 = \\{g_1, g_2, \\dots, g_M\\}$, with $g_i(x) = \\sigma(\\langle w_i, \\phi'(x) \\rangle)$, where $w_i$ is the **same** weight vector that model $f_i$ has (i.e. except for $\\phi'$, the set $S_2$ is 'point-wise' identical to the set $S_1$). Finally, let $\\phi'(x) = \\beta \\phi(x)$, where $\\beta \\in \\mathbb R_+$, and feel free to check that for any $\\beta$, every model $g_i$ from $S_2$ will agree with the model $f_i$ from $S_1$ in terms of predicted class (i.e. although the class probabilities will change, the rank is be preserved for any $\\beta$). This means that $S_2$ produces the **exact same** predictions as $S_1$ for **any possible data point**. Taking $\\beta \\to 0$ yields in $g_i(x) \\to 0.5$ for any $i \\in [M]$ and possible $x$, hence **the PD of $S_2$ will go to zero, even though the PD of $S_1$ can be arbitrarily large and the two model sets $S_1, S_2$ agree point-wise in terms of predicted classes**. In other words, taking an arbitrary set of models with ReLU activations, copying its weights and replacing the ReLU by $\\phi(x) = \\max(0, \\frac{x}{10^{10}})$, will yield a second model set with PD close to zero. Hopefully the authors agree with me that this trivial replacement of activation functions does not 'solve' any reproducibility problem in machine learning.\n\nWith the above in mind, I urge the authors to re-evaluate PD as a metric. As mentioned in my text, the Hamming form does not suffer from this issue, but the reported numbers in this case seem to indicate that there is little to no reproducibility challenge for the adopted tasks.", "rating": "1", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Ariadne"], "models": [""]}}
{"id": "_3U37vs8xfz", "review": "The paper is well structured and easy to follow. I also agree with the authors that prediction calibration is crucial in the M-open world, which is arguable always the case. The prospered conformal Bayesian computation method sheds some light on this direction. \n\nHowever, I am concerned about whether the novelty of the proposed method can be distinguishable from its existing building blocks. Conformal inference/distribution-free Predictive Inference‚Äîand thereby the main algorithm considered in this paper‚Äîare not new ideas. It is also not a new idea to use (regularized) importance sampling to avoid model refitting and obtain posterior integrals in the context of either the leave-data-out (LOO) or the add-one-data-in (AOI, e.g. B√ºrkner et al. 2020 used this importance-sampling-AOI idea in time series).  It appears the only methodology contribution this paper is making is to apply the importance sampling strategy to the model refitting step in conformal Bayesian inference. I am not convinced that such incremental contribution would grant a publication. \n\nIn terms of its practical usefulness, I have two concerns. First, throughout the paper, the authors seem to emphasize the situation in which x_{n+1} is fixed. But this is not a typical perdition task in which we have a known x_{n+1}. In the words of Lei  (2018): ‚ÄúIn some applications, where Xn+1 is not necessarily observed, prediction intervals are build by evaluating 1{y ‚àà Cconf (x)} over all pairs of (x,y) on a fine grid‚Äú. Perhaps x_{n+1} can also be evaluated on a grid, but then that method is rather related to jackknife-conformal-inference, which itself can be computed by importance sampling.  \n\nSecond, the ‚Äúcorrect frequentist coverage‚Äù is a probabilistic statement averaged over all observed data and x_{n+1}. This does not mean the interval estimate derived from conformal Bayes is exact for y_{n+1} conditional on a fixed x_{n+1}, nor necessarily optimal for interval predictions (optimal in the sense of interval scoring rules). Think about the Jackknife case in which the bandwidth is a constant across x_i. I suggest the author clearly state the definition of frequentist coverage as not all audience in the machine learning community can immediately tell the subtle difference.\n\nMinor:\n* L-82: why is the rank normalized by n+1? I think the rank statistic should the integer. \n\n* How is the grid of y chosen? Could you use Bayes prediction to design the grid?\n\n* What is the advantage of AOI against jackknife used in conformal Bayes? Is the only difference that IS-AOI is computationally easier than IS-LOO?\n\n* ‚ÄúThe CB and Bayes methods have comparable run-times‚Äù I think it is misleading. CB can have an arbitrarily larger running time if x_{n+1} is evaluated at many values.\n\n* Overall, the experiment section is relatively weak. The sample sizes in all three examples are too small to reflect a modern big data challenge. It is not common to see all MCMC running time < 1 second from ML conferences.\n\n* L-336: I think ‚Äúleverage‚Äù is the wrong concept here. The leverage only depends on x. Here you have the influence of y too.\n\n* \"If only approximate posterior samples are available, e.g. through variational Bayes (VB), then an AOI scheme may still be feasible, where one includes an additional correction term in the IS weights for the VB approximation\". I disagree. Note that Magnusson et al. (2019) are on the LOO of exact Bayes when there are only VB samples. In your context, you need not only adjust for the VB approximation for sigma_{n+1}, but also all sigma_{1}, ... sigma_{n} for lack of exact posteriors. Such a task is generally not feasible.\n", "rating": "2", "sentences": [{"sentence_type": "2", "sentence": "It appears the only methodology contribution this paper is making is to apply the importance sampling strategy to the model refitting step in conformal Bayesian inference. "}], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Ariadne"], "models": [""]}}
{"id": "H1e2XwOit4", "review": "This paper presents the DRS, a class of random functions. In contrast to GPs, DRS uses splines to define the function; in particular it defines a piecewise function using a spline on each interval, making sure that the overall function is continuous (and so are its derivatives). The parameters of each spline are modeled using a deep neural network that takes Gaussian noise as input. Importantly, the parameters of each spline can be chosen so that the modeled function satisfies certain desired shape properties, such as non-negativity, monotonicity, or convexity. As a use case, the paper shows an example of application that uses non-negative splines to model the rate of a Poisson process, analogous to the log-Gaussian Cox process. Inference is carried out with amortized variational inference.\n\nI found this is a strong paper and therefore recommend acceptance. It is well explained and the method is of interest to the community.\n\nOne question that I had is how this model performs/scales with the dimensionality of the output space (i.e., the analogous to the multi-output GP).\n\nAnother interesting point would be to compare against the log-Gaussian Cox process in the experiments.\n\nAs a minor comment, from page 3 I didn't understand why the Q matrices are of size 2x2 when d=3. On page 2, it says that each matrix Q if of size k-by-k when d=2k+1, so for d=3 shouldn't we obtain k=1?\n", "rating": "0", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Ariadne"], "models": [""]}}
{"id": "rylBeSoS3X", "review": "The paper discusses clustering sparse sequences using some mixture model. It discusses results about clustering data obtained from a restaurant loyalty program.\n\nIt is not clear to me what the research contribution of the paper is. What I see is that some known techniques were used to cluster the loyalty program data and some properties of the experiments conducted noted down. No comparisons are made. I am not sure what to evaluate in this paper. ", "rating": "1", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Ariadne"], "models": [""]}}
{"id": "HygW6Tz7ar", "review": "[Additional text]\nThis paper proposes a technique to incorporate document-level topic model information into language models. \n\nWhile the underlying idea is interesting, my biggest issue is with the misleading assertions at the very beginning of the paper. In the second paragraph of Section 1, the paper claims that RNN-based LMs often make independence assumptions between sentences, hence why they develop a topic modelling approach to model document-level information. Some issues with this claim:\n\n1. Pretty much every LM paper that evaluates on language modelling benchmark (PTB, WT-103, Wikitext-2) uses LSTMs/Transformers incorporate cross-sentential, document-level information as context, through a very simple approach of just concatenating all the sentences and adding a unique token to mark sentence boundaries.\n\n2. Prior work has shown that LSTMs/Transformers with cross-sentential context can, and in fact do, make use of information from previous sentences.\n\na. Evidence 1: Khandelwal et al. (2018) showed that LSTMs memorise word orders from the past ~50 tokens, and retain semantic information from the past ~200 tokens; both of which extend far beyond the length of an average sentence, suggesting that information from the previous sentences is used in the predictions of the current sentence.\n\nb. Evidence 2: Language models that operate on single sentences typically do worse than language models that take into account cross-sentential context, e.g. the language model of Kim et al. (2019) that operates on single sentences gets ~90 ppl. on PTB test set, while LSTMs that condition on multiple sentences get a much better ~50-something ppl. on Mikolov PTB.\n\nCrucially, these prior works defeat the paper‚Äôs motivation of why it claims to need topic models in the first place (i.e. to model cross-sentential context), while just concatenating multiple sentences as context would do, and in fact has been done many times.\n\n2. Prior work (mostly in Transformer-land) has come up with ways to make use of very long-range context, from Transformer-XL to the more recent compressive Transformer (https://opentext.net/forum?id=SylKikSYDH) that can condition on entire books. While these are done for Transformers, in principle one can also apply similar techniques to LSTMs.\n\n3. While Transformer-XL has the potential to make use of word orders in the preceding sentences, it seems that this paper‚Äôs approach cannot do that, since they only take the bag-of-words from the preceding sentences. It thus seems that their bag-of-word approach is less expressive, and hence less powerful, than the simpler alternative of concatenating sentences.\n\n4. The perplexity results (Table 1) are not done on very standard datasets (no PTB evaluation for instance). It is thus hard to evaluate the strength of the baseline models. In the paper's defense, it seems that they were following the experimental setup of Wang et al. (2019), but the paper should elaborate more on the choice of evaluation datasets.\n\n5. The inference part is not particularly self-contained. The paper simply refers the TLASGR-MCMC method (which is an important part to make inference scalable) to prior work (Cong et al., 2017; Zhang et al., 2018), yet does not explain (even briefly) how the approach works, and how it can be combined with their recurrent topic model formulation.  \n\n6. Evaluation of the induced topic hierarchy (Figure 4) is only done through qualitative samples, and the paper does not really explain how to pick the samples (i.e. possible cherry-picking). I am not very familiar with the topic modelling literature, but it would be nice if the induced hierarchy can be evaluated quantitatively.\n\nReferences:\n1. Urvashi Khandelwal, He He, Peng Qi, and Dan Jurafsky. Sharp nearby, fuzzy far away. In Proc. of ACL 2018.\n2. Yoon Kim, Alexander Rush, Lei Yu, Adhiguna Kuncoro, Chris Dyer, and Gabor Melis. Unsupervised recurrent neural network grammars. In Proc. of NAACL 2019.\n3. Wenlin Wang, Zhe Gan, Hongteng Xu, Ruiyi Zhang, Guoyin Wang, Dinghan Shen, Changyou Chen, and Lawrence Carin. Topic-guided variational autoencoders for text generation. In Proc. of NAACL 2019.\n4. Yulai Cong, Bo Chen, Hongwei Liu, and Mingyuan Zhou. Deep latent Dirichlet allocation with topic-layer-adaptive stochastic gradient Riemannian MCMC. In Proc. of ICML 2017\n5. Hao Zhang, Bo Chen, Dandan Guo, and Mingyuan Zhou. WHAI: Weibull hybrid autoencoding inference for deep topic modeling. In Proc. of ICLR 2018", "rating": "1", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Ariadne"], "models": [""]}}
{"id": "ryxG7NUmcS", "review": "#rebuttal responses\n \nThe authors' reply does not convince me, and I still think the paper has some problems:\n(1) I do not believe that the cumulative model-error can not be learned efficiently;\n(2) Experimental results are weak as some baselines do not converge! \n\nThus I keep my rating as reject.\n\n#text \nThis paper proposes a new adaptive model-based value-expansion method, AdaMVE, that decides the planning horizon of the learned model by learning the model-error. The model-error is learned by temporal difference methods.\nExperimental results show that AdaMVE beats MVE, STEVE, and DDPG in several environments.  \n\nOverall the paper is well written.  The paper proposes an interesting question: how to adaptively change the planning horizon based on the state-dependent model-error? Firstly, The authors upper bound the cumulative target error by the cumulative model-error.  Then the cumulative model-error is learned by the temporal difference method. With the learned cumulative model-error function over different rollout steps, the planning horizon is decided by a softmax policy.  \n\nHowever, I do not think that learning an upper bound of the target error helps to determine the value of H, as there is no justification that the gap between the target error and the model error is small theoretically.  I also doubt that the cumulative model-error can be learned without large loss, as there are no plots of W in this paper. \n\nThe authors claim that it is expensive to retrain the model error for the current policy at every step, thus they use some reference policy. I think it is ok, but I want to see the results of AdaMVE using the updated current policy, or updating the Q function before improving the policy. Adding a figure showing the change of H in the training helps to motivate this paper.\n\nFinally, AdaMVE is only compared in two MuJoCo environments. Baselines in other environments are only trained in 1e5 steps, thus the experimental results are not convincing.\n\nI am happy to change my opinion on this paper if authors give better motivation and the detail of the learning.", "rating": "1", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Ariadne"], "models": [""]}}
{"id": "QdKxMcoxfmA", "review": "This paper uses the intrinsic dimension of layers (estimated by [1]) to show that the intrinsic dimension of the last layer correlates with the regularization applied. This metric can be used to measure a model's generalization capability. Further, they show that good generalization requires a trade-off between decreasing the intrinsic dimension of the last layer and keeping a high intrinsic dimension for the layer with the highest one. \n\nThe paper is clearly written and understandable throughout. \nThe topic fits well with this workshop, and the results might be of high interest to the whole community. Considering the introduced metric might pave the way to a better understanding of and new regularization methods.\n\nWhat should be changed/added:\n- There are no error bars/uncertainty intervals reported. Please repeat your experiments with multiple seeds to prove that your results are significant.\n- Add a detailed description of the ID estimator you are using. One does not want to read an other full paper to understand what you are measuring.\n\nI like that you added hyperlinks to almost all of your references! (Should become a new standard) \n\nTypos, grammar, and formatting:\n- l 13 to 16: hard to understand sentence\n- Figure 3: Please add a description of the red curve to the figure. Took me a while to find it in the caption.\n- l 348: Typo in 060\n\n[1] E. Facco, M. d‚ÄôErrico, A. Rodriguez, and A. Laio. Estimating the intrinsic dimension of\ndatasets by a minimal neighborhood information. Scientific Reports, 7(1):12140, Sep 2017.\nISSN 2045-2322. doi: 10.1038/s41598-017-11873-y. URL https://doi.org/10.1038/\ns41598-017-11873-y.", "rating": "0", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Ariadne"], "models": [""]}}
{"id": "HylaQFF8cH", "review": "This paper develops a new few-shot image classification algorithm. It has two main contributions. The first one is to use a metric-softmax loss used to train on the meta-training dataset without episodic updates. The second is that the features learnt thereby are further modified using a linear transformation to fit the few-shot training data and the metric soft-max loss is again used for classifying the query samples. The authors provide experimental results for 5-way-1-shot and 5-way-5-shot testing on mini-Imagenet and CUB-200-2011 datasets.\n\nI think this paper is below the acceptance threshold. The reasons are:\n\n1. The contributions of this paper are marginal: both learning centroids for each meta-training class and projecting the few-shot features have been used before in published work (https://arxiv.org/abs/1905.04398). The empirical results are weaker than existing work (see for instance, https://arxiv.org/abs/1904.03758, https://arxiv.org/abs/1909.02729 etc.); also see #3 below.\n2. The authors should provide experimental results on other few-shot learning datasets like tiered-Imagenet.\n3. The image-size used here for Resnet-12 is 224x224, the authors should report results using 84x84 image size so that one can compare against existing literature fairly. Are the results for Resnet-12 so good because of the larger image size?\n4. The training procedure is task-agnostic, why do you train a different model for the 1-shot and the 5-shot case?\n\nI will consider increasing my score if some of the concerns above are addressed. I am listing some more comments below which I would like the authors to consider.\n\n\n1. Contributions: ‚Äúconsistency between training and inference‚Äù, do you instead mean consistency between meta-training and few-shot training? There are no weight updates at inference time.\n2. How essential is the metric-softmax loss? Training on the meta-training dataset without episodic updates has also been done in https://arxiv.org/abs/1909.02729. These authors seem to use standard soft-max training and perform standard fine-tuning, they report empirical performance that is significantly better than that in Table 4 and Figure 2. I am very skeptical as to why the accuracy of fine-tuning is only 21% in Figure 2.\n3. Section 3.2 does not motivate or explain the metric-softmax loss. Why should one have the network learn the centroids of the meta-training dataset? Can you draw a TSNE of the centroids learnt during meta-training? The features of the support samples (or their transformations) can be the centroids of the few-shot classes in the prototypical loss so inference phase does not need these centroids.\n4. I am not sure whether the matrix M is changed non-trivially during few-shot training. The weights W are already initialized to be the centroid of the features (eqn. 9). So the metric-softmax loss in eqn. 10 is expected to be small for the support samples after initialization. Why should the additional expression power afforded by M matter? There is no incentive for the network to change the matrix M. Can you show results on how much M changes from the identity?\n5. I believe the reported numerical results for LEO (Rusu et al. 2019) are for a WRN-28-10 architecture, not ResNet-12.\n6. The accuracy using Resnet-12 seem extremely high. I believe this is because the results reported in the literature, e.g., https://arxiv.org/abs/1904.03758, use images of size 84x84, not 224x224 as the authors here have used. Can you report results using 84x84 sized images?\n7. I don‚Äôt understand the explanation at the end of Section 5. Since the prototypical loss is being used to classify the query datum, it should not matter whether the cluster is shrunk in the 5-shot case, or whether simply the distances between the clusters are increased as in the 1-shot case.\n8. Table 1 is quite incomplete, the authors should mention other existing few-shot classification results are similar to the performance of this paper, e.g., https://arxiv.org/abs/1805.10123, among the ones listed above.\n9. The entries in Table 1 and 2 are not made bold appropriately. All entries with overlapping standard error should be bold.", "rating": "1", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Ariadne"], "models": [""]}}
{"id": "r7bhZSitrVI", "review": "\nSummary: Employing several different models, this paper demonstrates how aggregated wastewater data from across the US can be used to forecast COVID-19 cases. This paper also evaluates the optimal horizon for forecasting COVID-19 case data from wastewater signals. \n\nClarity:  This paper was well written and the paper‚Äôs objectives are clear. There are also clear descriptions for why given models were chosen for this evaluation.\n\nTo improve upon the clarity, I would suggest the following: \n\n--Further explain why case counts were used instead of hospitalization counts as the COVID-19 outcome metric. The given explanation in the paper is that, ‚Äú...case count data becomes an effective indicator of the strain on the healthcare system and the potential long-term effects of SARS-CoV-2 infection‚Äù. However, this same logic applies to COVID-19 hospitalization data, which did not suffer from the same notorious underreporting as case data did. This is not to say case data shouldn‚Äôt be used, just it is not clear why this was the outcome metric chosen. \n\n\n-- it is unclear at what time stamp ‚Äúground truth‚Äù data was being pulled. Did the authors use case data as-of the date of model evaluation (i.e., potentially revised case data)? Or only case data available the week of wastewater data collection? \n\n\n--Based on figures 5a - 5c, the error (measured in NRMSE) does not look much worse at 9 days vs. 6 days. It would help if the authors could clarify more objectively how the cutoff for 6-8 days was determined as the optimal horizon period. \n\n\nMinor comments on clarity: \n\n--A short sentence or two about what is being measured in the wastewater would be beneficial. What gene is being targeted / measured to determine COVID-19 concentrations?\n\n--Because there is so much variability in wastewater data, it might be helpful to mention how the prediction intervals of models are impacted by the changes in the wastewater data.\n\n--In section 4.1.1, definitions are needed for variables in equation 1. It is not immediately apparent what each different ‚Äúx‚Äù represents. Additionally, in section 4.1.1, the authors mention an evaluation of data delays in section 5.5, however section 5.5 is about horizons, and not data delays. \n\n--A note that the hyperlinks for footnotes 1 and 2 are broken. \n\n\nOriginality: There are similar articles that have compared modeling approaches on their ability to predict COVID-19 cases from wastewater data, however, this paper adds to the growing body of literature by using a segmentation approach on publicly available data, as well as determining the ideal forecast based on prediction accuracy and maximizing a longer forecasting horizon.\n\nSignificance: The significance of this paper is that it demonstrates that numerous modeling approaches provide similar results when using wastewater data to predict COVID-19 cases at a national level. This paper could be improved by adding discussion of the biases and limitations of using data aggregated at a national level, and demonstrating how well these models perform at a smaller geographic scale. Discussion of the confidence levels of these models would also be beneficial, as would using multiple metrics to evaluate model performance.\n\nPros:\n\n--Well written paper with many clear discussions about decisions made in the experimental process. \n\n-- Use of publicly available data makes methods replicable. \n\n-- Project opens the door for additional analyses that can be done using wastewater data, as well as additional variables that can be added to the analysis. \n\nCons:\n\n--As noted in sections above, the paper could expand on the limitations of using wastewater and case data at the national level, such as heterogeneity in COVID-19 across the county, non-standardized collection approaches across counties / states, variation in sampling sites, etc. \n\n\n--The determination of why 6-8 days is the optimal horizon is not clear from the figures presented. \n", "rating": "1", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Ariadne"], "models": [""]}}
{"id": "rNxxIICCaZ9", "review": "This paper is relevant for the workshop.\nThe authors explore which transforms need to be applied to a source object to match a target object.\nThe \"Interventional Framework\" section is described well.\nIt would be nice to see the qualitative results on the synthetic data.\nIn Figure 4: It is not clear how the MSE is computed? Also, to improve the paper for future submissions it would be worth comparing to some of the methods mentioned in the related work.\n\nWhy is the model better at scale y than scale x? Is this to do with bias in the data? Similar for translation x and translation y?\nHow do you disentangle depth from scale? Do you assume a fixed depth? This would be worth discussing.", "rating": "1", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Ariadne"], "models": [""]}}
{"id": "fnD4GR-s73", "review": "===== Strength ======\n1. The paper points out an interesting fact and problem: most of the parameters in a neural net are in the last fully connected (FC) layer, while in a large-number-of-class and non-IID setting, each client may only have data from a small portion of classes. The authors thus accordingly proposed the federated sampled softmax to save the communication cost by local training with only a small number of \"negative\" classes in addition to the \"positive\" classes.\n\n2. The authors conduct experiments on three large-scale datasets. The authors conduct the ablation study on how many negative classes are needed. \n\n==== Weakness =====\n1. The technical contribution is not sufficient. There have been many methods like (Bengio & Sen¬¥ecal,2008) and [a-c] that subsample the negative classes in the softmax objective. Specifically, in [c], the proposed method did include the positive classes in a minibatch, together with a set of subsampled classes, to compute the softmax. While the authors compare different combinations of positive and negative classes in 3.4, to me, it seems pretty obvious that we should combine the clients' positive classes with subsampled negative classes. Besides, while the authors discussed several advanced methods in 3.3, they ended up choosing uniform sampling. I was wondering if any of these advanced approaches can be approximately implemented in a federated setting to further improve the performance.\n\n[a] Joulin et al., Learning Visual Features from Large Weakly Supervised Data, ECCV 2016\n\n[b] Mikolov et al., Distributed representations of words and phrases and their compositionality, NeurIPS 2013\n\n[c] Hu et al., Learning answer embeddings for visual question answering, CVPR 2018\n\n2. One potential direction to improve the novelty or technical contributions of the paper is to develop a series of methods for different kinds of representation learning in a federated setting, including learning a fully connected layer and metric learning. If we do not consider the computational cost at the client end, will metric learning (without learning the FC layer) outperform the proposed method? In [a], the authors proposed to learn multiple one-vs-all classifiers rather than a softmax classifier. Will it be effective in a federated setting?\n\n3. The experimental setup can be improved. There is no clear description of how many classes, images, local epochs for each dataset. I also have concerns about using the pre-trained features for 4.2 and 4.3, which makes 4.2 and 4.3 like downstream tasks rather than representation learning. I would suggest that the authors include a fixed feature baseline in Table 1 and Table 2 to demonstrate that fine-tuning the feature extractor in a federated setting could outperform the pre-trained features. \n\n4. The authors only apply the proposed method to the FedAvg baseline. Will the proposed method be applicable/effective to more advanced federated learning algorithms like FedProx [d], Scaffold [e], and FedDyn [f]?\n\n[d] Li et al., Federated optimization in heterogeneous networks. In MLSys, 2020\n\n[e] Karimireddy et al., Scaffold: Stochastic controlled averaging for federated learning. In ICML, 2020\n\n[f] Acar et al., Federated learning based on dynamic regularization. In ICLR, 2021\n\n==== Other comments ====\n1. I would suggest that the authors add \"supervised\" into their title, to contrast to many recent works on \"unsupervised\" representation learning.\n\n2. I would suggest that the authors cite some more papers on federated learning.", "rating": "2", "sentences": [{"sentence_type": "2", "sentence": "While the authors compare different combinations of positive and negative classes in 3.4, to me, it seems pretty obvious that we should combine the clients' positive classes with subsampled negative classes."}], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Ariadne"], "models": [""]}}
{"id": "kUETMc3Fq6f", "review": "The paper presents an approach for learning policies for long-horizon tasks (e.g., peg insertion) that relies on predicting \"key states\" along the trajectory. The main idea is that key states are a) easily predictable and b) useful for conditioning action predictions. The results show improvements over baseline methods on four tasks: pick cube, turn faucet, stack cube, and peg insertion. The approach uses ChatGPT to describe the steps for solving each task and then the authors design heuristic rules for each step to identify \"key states\" in expert demonstrations, which are used for learning (via an auxiliary training objective). The paper is clearly written.\n\nThe paper does not use RL and the use of prior knowledge (from ChatGPT) is limited. However, the approach should still foster interesting conversations at the workshop. Specifically, the high-level idea of using an LLM to decompose tasks and then using that decomposition for learning seems highly relevant.\n\nThe authors could consider other ways in which LLMs could be used in the propose approach. For example, can the LLM produce the heuristics rules for identifying the keys states after the steps for solving the task have been listed? Or can an LLM be used to propose methods for identifying key states without the need for privileged simulator information? Such techniques may help scale the proposed approach to more tasks and more domains.", "rating": "1", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Ariadne"], "models": [""]}}
{"id": "BJx5KwRTiN", "review": "The paper presents 3 approaches for on-board scheduling of activities in a planetary rover under reservoir resource constraints.\n\nTwo of them, the Max Duration and Probe algorithms, are sound, but incomplete, while the third, the Linear algorithm, is sound and complete for linear rate resource consumption (but more computationally expensive).\n\nAn empirical evaluation shows that the Probe algorithm (the one currently baselined for use in the onboard scheduler for NASA‚Äôs next planetary rover, the Mars 2020 rover) performs competitively and compares the runtime differences between the three algorithms.\n\nThe topic is interesting, definitely up-to-date given the current efforts in planning and scheduling for the Mars 2020 mission, and it fits the SPARK workshop.\n\nWeakness of this paper in my opinion are in the presentation and in the justification given for the proposed approaches.\n\nRegarding the presentation, I found the paper not easy to follow: some definitions on Timeline Representations (like the \"impact\" for instance) are not standard in timeline based planning (meant as the systems like EUROPA, ASPEN, APSI and others surveyed in Chien et al, SpaceOps 2012) and would have maybe deserved better explanation besides the citation to Rabideau and Benowitz 2017. Also the Problem Definition presents some issues: what are exactly the \"zones\" for instance?\n\nMoreover, I would suggest to add an example, or figure, about the Probe Algorithm (at the end, since this is the chosen baseline, I think it would deserve better presentation).\n\nOn the empirical evaluation, Figure 4b, it is said that \"In Figure 4b, we see that the Linear algorithm strictly outperforms the Probe method while the Max Duration method strictly under-performs\". It does not look like honestly that Linear strictly outperform Probe, they look like almost the same... \n\nPlease check also the bibliography style, many citation appears incomplete.\n\nAbout the justification, it is understood that the system has to operate on-board under strict computational constraints (and in this case the Probe algorithm makes sense), but some words to justify such an ad-hoc approach would have been probably appropriate. This looks like an integrated P&S problem, broken into a sequence of steps aimed at satisfying a sub-set of the problem constraints. How do you control the impact of this specific step (the calculation of awaken states) on the whole plan? There is no side-effect that could violate other logical or resource constraints?\n\nTo conclude a curiosity: in alternative to the Linear Algorithm, would have been computationally too expensive to try a proper calculation of the resource envelope (min and max subject to flexible allocation of consumptions) and then calculating a minimal/maximal flexible duration for the Asleep activities with a Max-Flow algorithm for instance?\n\n", "rating": "1", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Ariadne"], "models": [""]}}
{"id": "Byg4aCzE5S", "review": "Thank the authors for the response. I agree with R2 that the paper lacks comparisons with previous works. I will stick to my previous decision.\n----------------------------------------\nSummary\nThis paper presents a new approach for single-objective reinforcement learning by preferencing multi-objective reinforcement learning. The general idea is to first figure out a few important objectives, add some helper-objectives to the original problem, and learn the weights for each individual objective by trying to keep the same order as Pareto dominance. This paper has potential, but I lean to vote for rejecting this paper now, since it is still not ready. I might change my score based on the texts from other texters.\nStrengths\n- The idea is novel. Learning weights for each objective by keeping the order as Pareto dominance is an interesting idea to me.\nWeaknesses\n- The lack of experiments. The authors tested their method in only one scenario, which makes me feel unsafe. Only testing on one simple scenario does not demonstrate the effectiveness. The authors are supposed to test their method on more (complex) scenarios to show the effectiveness of their method.\nPossible Improvements\nAs mentioned before, the proposed method can be tested on more scenarios (e.g., Deep Sea Treasure, SuperMario, etc.).", "rating": "1", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Ariadne"], "models": [""]}}
{"id": "HyxtTsDjKH", "review": "Pros:\nThis paper proposed a new method for zero-shot transfer learning under the reinforcement learning setting. The use of attention weights to regularize the latent states was fairly interesting.\n\nCons:\nLimited applicability of the proposed methods\n- The paper was restricted in a setting where rewards, actions, and true states were identical between source and target environments, and only the observed states differed due to differing renderers. Working under such a restricted setting was interesting in its own right, but it might also lead to limited applicability of the proposed method in the real-world setting.\n- The proposed method focused on solving a very specific problem: learning a dis-entangled latent representation for images. As a result, the potential impact of the proposed methods could be minimal.\n\nLimited technical novelty\n- The proposed method, SADALA, was built on top of Higgins et al., 2017 (DARLA). The only difference was an added attention layer to the learning of latent states. As a result, the novelty of the proposed method was very incremental and limited from a technology perspective.\n- Even with additional attention layer, the paper could have performed a more thorough study to help the readers understand and appreciate the idea. For example, this paper didn‚Äôt discuss the tradeoff between training SADALA over separate stages, versus training it from end to end. For example, why the weights of the pre-trained beta-VAE had to be frozen and used as weights in the state representation stage.\n\nInsufficient experiments\n-More thorough discussion of the qualitative results should be helpful to understand whether the attention weights helped the model to focus on the right thing. For example, this paper did study the quality of reconstruction in Figure 3-5 of the proposed method. When comparing Figure 3 and Figure 5, it appeared to me that the reconstructed the angle of the pole was different from the original one. And it seemed like attention weights did successfully ignored the color of the cart and pole, but it ignored the angle of the pole, which should be important to the learning task. Unfortunately, the paper didn't further explain the implication of such misrepresentation.\n\n-Quantitative results \n* It would be interesting to all compare the proposed methods against model-agonistic methods like MAML\n* It would be useful to include confidence intervals over different tasks.\n* It would be useful to compare different methods with different parameter settings\n* The authors mentioned ‚ÄúVisual Pendulum tasks‚Äù but didn‚Äôt include them in the paper\n\n\nReproducibility\n- It's unclear to me how reproducible the research conducted in this paper was, and it would be useful to open source the code used to conduct the experiments.", "rating": "1", "sentences": [{"sentence_type": "positive", "sentence": "More thorough discussion of the qualitative results should be helpful to understand whether the attention weights helped the model to focus on the right thing"}], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Ariadne"], "models": [""]}}
{"id": "RqQ76OXx9l", "review": "The authors present a method to assess the similarity between pairs of images for large datasets that relies on the scale invariant feature transform. Both the method and the results described are interesting and of high quality. However, this work does not seem to be in line with the topic of the MIDL conference as no deep learning seems to be used in the analysis.", "rating": "1", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Ariadne"], "models": [""]}}
{"id": "MvmApzelGe", "review": "In the paper, the authors apply geometric deep learning methods to predict the functional organization of the human visual cortex from MRI data. Curvature data, myeling values, and connectivity of vertices on the cortical surface are passed through spline-based convolution layeres to predict the retinotopic map. The network is tested on data from the HCP dataset.\n\nThe paper is clearly written, presents an application of geometric deep learning methodology on a medically relevant dataset, with network architecture that seems well-suited for the task. The methodology and application is relevant for the MIDL audience. I believe the paper fits very well the intended focus and scope of a MIDL short paper.", "rating": "0", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Ariadne"], "models": [""]}}
{"id": "pIWDwAVUjoy", "review": "In this paper, the authors studied the problem of identifying meaningful clinical patterns among patients who had been prescribed opioids and subsequently had the doses reduced over different lengths of time. Overall the paper has several strong aspects\n- It was able to identify several dosage patterns that maybe of interest towards clinical determination\n- The initial analysis seems to point towards differing health outcomes for patients with slow vs rapid tapering (see more below)\n- The paper covered sufficient details about cohort characteristics to let the texters judge the impact of the findings\n\nHowever, from a health economic outcome research aspect, the paper is currently at an early stage and may need further followups to support the validity of the identified patterns. The authors have acknowledged the limitation of not considering other factors that may capture the intent to reduce/increase dosing. However, this is a key aspect that may need to be validated, perhaps with certain assumptions such as IPW, to satisfy the significance of the findings.  Further, the authors may want to considering survival analysis methods, especially with the possibility of right censored events, to further analyze the clinical outcomes of the identified cohorts. \n\nOverall, this papers has certain promises but may be improved upon from a modeling and analysis aspect.", "rating": "0", "sentences": [{"sentence_type": "positive", "sentence": "However, from a health economic outcome research aspect, the paper is currently at an early stage and may need further followups to support the validity of the identified patterns. "}], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Ariadne"], "models": [""]}}
{"id": "SJemyeYW5r", "review": "The paper describes a new dual method for graph convolutional networks that combines the features from the graph and it's dual, in two pipelines. The paper builds on the architecture as in GCN and in addition to the dual pipelines, one from the graph and other it's dual, employs KL divergence to achieve the final prediction.\n\nThe paper leaves inadequate explanation on the results, where the proposed TwinGCN comes short in 2 of 3 methods compared to other methods in Table 1, which cannot be ignored considering the slow convergence and marginal improvements, compared to GCN with double pipeline in Table 3. This leaves the premise of the authors on adding improvements to the learning ability by bringing in features from dual graph on shaky grounds.", "rating": "1", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Ariadne"], "models": [""]}}
{"id": "XBnlTjsPGa", "review": "This work presents a new approach for efficiently training multi-modal (image-text) models by aligning existing pre-trained unimodal encoders. The paper is very clear and the results are impressive, showing improvements over state-of-the-art models such as LiT (let alone traditional multi-modal training methods such as CLIP) on the challenging zero-shot ImageNet classification task.", "rating": "0", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Ariadne"], "models": [""]}}
{"id": "ptzNzYVnTKS", "review": "The paper explores the question of whether a successful idea in vision and language - of learning representations with masking can also extend to the reinforcement learning setting. To test this they combine masking with the transformers to predict masked trajectory sequences from the rest of the trajectory. They utilize (state-action-returns) as each element of the trajectory. This model allows them to solve a number of problems in RL in a unified way - Forward dynamics, inverse dynamics, BC, and offline RL.\n\nStrengths:\n1. With a variety of experiments authors successfully show that the method if able to achieve improved performance across all tasks using a single model - showing the importance of features learned when training using masking.\n2. The authors present an ablation of important components of their method which is a good addition.\n\nQuestions:\n1. In the purely offline setting with downstream representations, should an offline RL algorithm have been compared instead of TD3 since it is not clear if the vanilla method is failing due to lack of good representations or the overestimation issue.\n", "rating": "1", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Ariadne"], "models": [""]}}
{"id": "IOUzN8U1RM", "review": "The main claim of the paper is that benign overfitting occurs in the adversarial case as well--that is, even though the training data are overfit perfectly, one can still attain reasonable generalization error. However, in practice it is well-documented that overfitting is indeed worse for adversarially-trained models, so I think an adequate analysis in this case would explain this phenomenon. I was not able to get such an explanation from the paper, although I may have missed it.\n\nAside from explaining empirical results, another potential contribution of theory papers is introducing new proof techniques. The paper does not include a discussion of what new technical contributions or proof techniques it contributes, so it was difficult to assess this aspect. While there is a proof outline in Section 5, it is not clear what is novel and what follows the previous proofs.", "rating": "1", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Ariadne"], "models": [""]}}
{"id": "rGfgm6tlhZq", "review": "**Summary**\nThis work proposes a novel factored architecture that aims to leverage the reward factorization structure. The main idea is to learn a mixture of state encoders to compute factor representations. The computed factor representations are used to compute factor value which is aggregated to compute the state value, and also used to compute the policy via attention mechanism. The experiment was conducted on Procgen and Minigrid domains where the architecture of the baseline model is replaced with the proposed architecture.The result shows that the proposed architecture improves the sample efficiency of the baseline method.\n\n**Pros**\n* The proposed idea is interesting and well-motivated\n* The paper reads well\n* The experiment was conducted on diverse tasks\n\n**Cons**\n* The performance improvement is marginal\n* It is unclear whether it is a fair comparison in terms of the architecture capacity. (see below)\n* The presentation can be improved (see below)\n\n**Major comments**\n* Is it a fair comparison in terms of the capacity of model architecture? It seems the proposed architecture multiplies the capacity of the model by a number of factors. It should be made sure that both architectures of the baseline and baseline+AFaR should have the same capacity for fair comparison.\n* Why disconnect gradients from policy for learning representation? The gradient coming from the policy loss is a major source of learning representation. It would be better if authors provide more intuitive reasoning behind this design choice.\n* Although it is a design choice, it may be more natural to use summation instead of mean for computing state-value following the factored reward MDP definition. So it would be better to present a more intuitive justification for such a design choice.\n* It would be more interesting if authors can analyze the different ‚Äúmode‚Äù of policy learned for each factor.\n\n\n**Minor comments**\n* The experiment result for ‚Äúsparse‚Äù is missing in Figure 2, but ‚Äúsparse‚Äù is mentioned in the comment of Figure 2 and section 4.2.", "rating": "1", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Ariadne"], "models": [""]}}
{"id": "mwf_pCynhc9", "review": "Summary:\n\nThe authors proposed fractional variational autoencoder (FVAE) for the learning of disentangled representation where the action sequences can be extracted step-by-step. Experiments are shown to illustrate how the algorithm works.\n\n#################\n\n\n. The authors proposed FVAE but the associated objective function is not introduced explicitly, which is confusing. Is it the same as the objective of \\beta-VAE?\n\n. Fig.3: 1) What's the KL divergence here? Is it between the posterior and the prior? 2) It's claimed that the trend of KL divergence is consistent with that of entropy. But it is hard to see from Fig. 3. 3) It is claimed that the significance of action is related to the capacity of learned latent information. Based on Fig. 3, this conclusion is not convincing. Also, Fig. 3 is obtained based on a toy dataset. To claim it as a main contribution, the conclusion needs to be verified on other datasets as well.  \n\n. Section 4.1: What's definition of the label here? It's not clear. Is it like the types of shapes on dSprites?\n\n. Section 4.1: The training on dSprites includes two phases: find thresholds and then train different stages. 1) The authors arranged  three stages for dSprites. This seems arbitrary. Why not four or five stages? 2) What's the training objective function of each stage? 3) How are the curves in Fig. 5 derived? More explanation is required.\n\n. Section 4.2: It is claimed that ``One can recognize three points where the latent information suddenly increases: 60, 20, 4.'' This is hard to see from Fig. 5b) as all curves look smooth. Thus, the following three-stage training process is questionable. The training for unlabeled task needs more study.\n\n. The experiments are limited. There are a lot of papers regarding disentangled representation, and the authors only compared with \\beta-VAE. \n\n\n", "rating": "1", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Ariadne"], "models": [""]}}
{"id": "HzfxxRV0pZ5", "review": "Given a video of a real-world system, the aim of this paper is to set up a corresponding system in simulation. To that end, the paper proposes a combination of methods that detect individual objects and infer their physical parameters and mutual joint types. The inference of articulation as a tree of bodies and joints is the main contribution.\n\nStrengths:\n* The proposed pipeline can correctly infer the individual bodies and joints of a real-world pendulum system with three bodies and three joints. This is done given only a video and prior knowledge of what objects we can encounter.\n* While the method assumes access to the geometries of individual objects, the actual object recognition and pose detection from images is done without taking any shortcuts.\n* The method is shown to enable control of a simulated cartpole.\n\nWeaknesses:\n* The evaluation of the individual components of the proposed system is lacking. It is unclear what the limitations are, except for only modeling simple objects.\n\nOverall, this paper fits the theme of the workshop well and it reports interesting results.\n", "rating": "1", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Ariadne"], "models": [""]}}
{"id": "_PRjdxzjmq", "review": "The paper aims to improve the training of ViT models under a fixed computational budget (one GPU, 24 hours)---much smaller than what is usually used. Progress on this problem would enable shorter development cycles of new models, and make them more widely accessible to researchers with less compute hardware.\n\nThe work proposes modifications of the architecture (locality) and order in which data is presented to the model (curriculum learning) that speed up training. These modifications are evaluated, both jointly and independently, on a new benchmark derived from ImageNet1k by addition of constraints on the computational budget. Results indicate that both mechanisms help achieve faster training within the given budget.\n\n(I am not very familiar with the architectures used in the paper, hence the low confidence score.)\n\n---\n\nDetailed comments:\n\nSome of the architecture modifications are motivated, but exchanging the GeLU activations seems arbitrary. It would be good if the authors could provide a motivation in section 2 how this modification helps to speed up training.\n\nIn section 3.1 it would be helpful to add a remark about how the training hyperparameters were chosen.\n\nA satisfying extension of Figure 2 (left) would be to extend these plots to times longer than 24 hours. This should be doable with little effort. If the proposed model reaches the final performance of DeiT-S in less time, this would further strengthen the paper's contribution.\n\nI also think that the manuscript could benefit from a short \"Conclusion\" section at the end where the most important findings are restated. This will make the paper easier to skim.\n\n---\n\nMiscellaneous comments:\n\n- L104: \"excepts\" ‚Üí \"except\"\n- L110: Missing article before \"well-known\"\n- L115: \"is\" ‚Üí \"are\"\n", "rating": "0", "sentences": [{"sentence_type": "positive", "sentence": "It would be good if the authors could provide a motivation in section 2 how this modification helps to speed up training"}], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Ariadne"], "models": [""]}}
{"id": "HyxRBRMZ94", "review": "This paper proposes a new lower bound for variational inference based on q-deformed logarithms, a generalization of the logarithm that augments it with a q parameter that controls its concavity. The authors train VAEs with this bound and report performance improvements over the ELBO, but not the IWAE bound.\n\nThe paper has issues with clarity, and rigor, but is overall a reasonable contribution.\n\nSpecific Feedback:\n\n1. The derivation of the q-deformed lower bounds is lacking in rigor. The new bounds are just stated by swapping the q-logarithm for the standard logarithm without discussing whether that is possible. A proof that the new bound is a valid lower bound would be useful.\n\n2. Similarly, it is not clear if swapping in the q-logarithm gives a lower bound on the log likelihood of the data (q=1) or the q-deformed log likelihood of the data for a specific value of q. The latter seems more likely. If that is the case, a discussion of the benefits and drawbacks of optimizing a lower bound on the q-deformed log likelihood of the data would be helpful to the reader.\n\n3. In the actual training procedure the authors optimize bounds with different values of q for each batch. An argument should be made that this procedure is still optimizing a valid lower bound on the (possibly q-deformed) log likelihood.\n\n4. The optimization procedure for q is not clearly stated. It seems like q* is set to make the qELBO evaluated with q=q^* match qELBO* as closely as possible. So perhaps the optimization procedure attempts to minimize (qELBO* - qELBO(q=q*))^2 w.r.t. q*. This should be stated clearly, and the optimization procedure should be clearly motivated.\n\n5. In evaluation, what method is used to estimate log \\hat{p}_x?\n\n", "rating": "1", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Ariadne"], "models": [""]}}
{"id": "HyeOL_lGcB", "review": "Summary:\nThe paper proposes two new regularizers for adversarial robustness inspired by literature on verification of ReLU neural networks for resilience to epsilon perturbations using convex relaxations. The paper shows empirically that the proposed method leads to better robustness than previous works.\n\nStrengths:\n+ The paper seems to have an interesting perspective (with the proposed looser relaxation) of the convex relaxation of an adversary adding noise at every layer in the network\n\nWeaknesses:\n\n*Sec. 4.1: Eqn. (O) does not have a convex relaxation, it is the exact problem which is intractable. Why are we comparing the optimal values of p*(O) and p*(C)? The paper from Salman et.al. already shows that there is a convex relaxation barrier, which essentially corresponds to this difference. In general, in Sec. 4, it is often unclear whether when we talk about p(O) if we are referring to the unrelaxed original problem or the tightest convex relaxation. For example, at the start of Sec. 4.1, it seems like we are talking about the convex relaxation and then in Sec. 4.3 it seems like we are talking about the unrelaxed problem.\n\n*It is not clear how/ why the proposed method of relaxing (which by the way seems identical to Fast-Lin (Weng et.al.) is better than the optimal convex relaxation. Would this not lead to looser bounds? Is that the thing we are looking to investigate? Making that more clear would be useful. Perhaps it would be good to argue the proposed regularizer in this work cannot be constructed with the optimal convex relaxation. Is that true? A discussion on this would be helpful.\n\n* The crux of the contribution seems to rest on the premise that identifying the optimal perturbation in the input space with the relaxed model, and then computing the activations with respect to that and forcing the forward pass to saturate near the margins of the relu polytope (relaxation) is a good idea. In general, it seems very unclear why this should work based on the evidence presented in the paper. Specifically with the relaxation, it might not even be guaranteed (as far as I understand) that the value of \\delta_0^* that is found from problem C is even going to lie inside the L\\inf norm ball around the point x, for example. Thus it is not clear to me if this is an approach for verification or a regularizer based on verification.\n\n* Ultimately, the value of the approach in this context (as per my understanding) comes from the experiments and the results which show that there is increased robustness. It would be great to clarify a couple of details in the experiments:\n1. Is the method of Wong et.al. using the looser convex relaxation (used here) or the tight convex relaxation when reporting the numbers in Table. 1? \n2. If the optimal convex relaxation can be used to construct the same regularizer as the one proposed here, it would be good to evaluate how well that does.\n\nOverall, I am not an expert in the area but a lot of details from the writing (such as point 1 under weakness) and the theoretical justification of the regularizer are unclear to me. Thus given these (perceived) weaknesses I would lean towards weak rejection. Clarifications on these points would help me revise my score.", "rating": "0", "sentences": [{"sentence_type": "positive", "sentence": "The paper seems to have an interesting perspective (with the proposed looser relaxation) of the convex relaxation of an adversary adding noise at every layer in the network"}], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Ariadne"], "models": [""]}}
{"id": "rJxJvdtzcE", "review": "The paper proposes a framework for human evaluation of generative models of images. It is based on samples, so it is compatible with any flavour of generative model (likelihood-based, adversarial or otherwise). Two different evaluation strategies are proposed: one based on the time it takes for humans to distinguish generated images from real images, and another which simply measures the percentage of images that are wrongly classified.\n\nThe implementation of the human evaluation setup is described in appropriate detail, and attention to cost is also given. The results are comprehensive and statistical tests are used to show their significance. The approach is also compared to FID, a computational evaluation metric that is currently popular.\n\nOverall, this is work is timely and it is well presented, so I am in favour of acceptance. Nevertheless I have a few more comments and suggested improvements below:\n\n* It is demonstrated that the correlation of HYPE and FID is relatively poor, and it is implied that this demonstrates that FID is a poor metric. However, as the authors state earlier on in the paper, HYPE can only measure realism, not diversity. FID is explicitly constructed to also be affected by sample diversity, so in that light it is not surprising that the two do not correlate very well, and that higher truncation leads to improved HYPE but worse FID scores -- it is well known that truncation reduces diversity of the samples, in favour of improved fidelity. (I do not wish to imply that FID is actually a good metric -- I do believe it is a poor metric, but not for this particular reason.)\n\n* While the authors state clearly that HYPE does not measure diversity, I think it would be worth discussing in more detail how one could use human evaluation to measure diversity, as it is arguably a more interesting challenge. As it stands, the HYPE metric could probably be fooled by a \"model\" which simply stores a few training examples and randomly selects them with equal probability. Also measuring the diversity of the samples in some way would prevent this kind of cheating.\n\n* A common issue with human evaluation is ambiguity in the task specification: the raters are instructed to determine which images are real, but they may be prone to misinterpreting the task in a way that biases the results. While rater training and immediate feedback undoubtedly help to limit this effect, it is still worth considering this carefully, and I think a few diagrams or screenshots of the rater interface would be useful additions to the manuscript in this respect.\n\n* In the introduction, it is implied that likelihood (measured in the input space) would be the ideal metric for generative models if it were always easy to compute (which it often isn't). Theis et al. (2015), cited there, also call this into question. I find the juxtaposition of this citation and the sentence before it a bit misleading.", "rating": "0", "sentences": [{"sentence_type": "positive", "sentence": "While the authors state clearly that HYPE does not measure diversity, I think it would be worth discussing in more detail how one could use human evaluation to measure diversity, as it is arguably a more interesting challenge."}], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Ariadne"], "models": [""]}}
{"id": "p5rtDzJ7Fdr", "review": "While this submission presents an internally valid research effort I have major doubts on how the work scales to real world usability scenarios, and thus on how valid and interesting the findings will be to the GI audience and readership. \n\nThe paper's main strength is the introduction of a Bayesian approach to gaze target selection. There is a body of past work on Bayesian frameworks for eye tracking and eye control but, as far as I know, not in the HCI context of target selection on a phone/tablet.\n\nThe paper has a lot going for it: it is well written, the related work section is thorough, and the researchers seem to follow an overall solid methodology (though I was quite confused by the WOz  approach to study 1, please see more below).\n\nThe main reason I am not supportive of acceptance is what I believe to be a lack of attention to external validity in the current manuscript. Several aspects of the work seem to be narrowing down the scope of the research to lab settings with little insight on whether the findings will have much meaning in the real world, and with little attention to how the findings might scale to real task scenarios and real settings. This is a major flaw in my eyes and I hope the authors will at least attempt to provide more insight and discuss these aspects and possible implications of their work in case the paper is accepted to GI'21.\n\nMore specifically, the 1D target selection tasks, which are core to the paper and its two studies, are very limiting and questionable. I was not sure if the authors suggest that a 1D target selection task is valid in some settings? And if yes, which ones? If the 1D target selection task is just a precursor to 2D target selection, why are the authors convinced that their findings will scale? \n\nWhile the paper's motivating Figure 1 seems to suggest a phone form factor, the study experiments are using an ipad which can maybe qualify as a large screen phablet, placed on a desktop stand. Again, the experimental settings are not invalid, but are very constrained, and arguably do not map to real world phone usage settings. I was hoping the authors could provide more insight on how their work could possibly scale to actual usage scenarios, where a a much smaller handheld phone is being handled and addressing various head poses (and yes, mostly 2D target selection tasks...) and maybe thinking of hybrid approaches that do not rely only on gaze for target selection. To clarify, I am not necessary suggesting an expansion of the studies, rather a much richer discussion of the limited scope studies presented in the paper in a wider context, allowing the reader to think of ways of applying the paper's findings in real world settings. \n\nI was confused by the authors suggesting that they used a Wizard of Oz approach for their first study (only?). The details are lacking and I am uncertain how a wizard could operate such a study, and whether the introduction of a wizard would not compound the findings. In case of acceptance I suggest the authors please elaborate on these points.\n\nFinally, a video figure would have helped clarify aspects of the studies and the findings.\n\n\n  \n\n", "rating": "1", "sentences": [{"sentence_type": "2", "sentence": "I was confused by the authors suggesting that they used a Wizard of Oz approach for their first study (only?)"}], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Ariadne"], "models": [""]}}
{"id": "HEnxep5kRWq", "review": "### Summary\nThis paper proposes an object-centric approach to \"imagination\" where a compositional inductive bias in the architecture is leveraged to generate new data points that are novel combinations of known concepts. Training models of such imagined scenarios helps them generalize more systematically. \n\nThe setting explored here is a simplified version of the Abstraction and Reasoning Corpus (ARC), named Sort-of-ARC, that uses the same input space, but considers a simpler (more restricted) set of operations to map inputs to outputs. In particular, the model is given access to a support set of correct input/output pairs, where the outputs are the result of applying a transformation (eg. spatial translation) to the set of objects contained in the inputs that satisfy some condition (eg. all red objects). To succeed at this task in manner that leads to systematic generalization, the model first has to infer the underlying program that generated the support set and then apply it to the query input to generate the right output. This is analogous to other abstract visual reasoning settings, such as Raven's progressive matrices.\n\nThe proposed model consists of a controller and an executor. The controller encodes the input/output images in the support set to produce a set of latents, called the instruction embedding, to give to the executor. The executor decompose the query input to obtain a set of entity-centric latent representations and updates them based on a neural program, after which they are decoded. The neural program is obtained by using the instruction embedding to query from a set of learned condition and transformation embeddings. The program is applied separately to each entity embedding to update them. Here the condition embedding gates the proposed update produced by the transformation to select only relevant entities to which the program applies. \n\nCompositional imagination can now acts as a learning paradigm, through applying arbitrary condition-transformation pairs to the inferred entities of the inputs from the support set to produce new outputs (this amounts to sampling indices for these embeddings, and applying the executor), The controller can then be trained on this new support set to yield program instructions that select condition and transformation embeddings that were used. This imagination loss is combined with the regular query output prediction loss.\n\nThe experiments indicate that (1) it is important that the mechanisms that operate on the object representations are modular themselves, and (2) that training with the imagination loss improves out of distribution generalization. \n\n### text\n\nThis is a nice workshop paper that explores compositional imagination as a framework for out of distribution generalization. The setting considered requires novel compositions to obtained through applying (condition, transformation) pairs, where it is further demonstrated that modularity at the level of mechanisms is also important. The proposed architecture for solving these tasks is intuitive, although perhaps more development could go into the design of the controller to produce the instructions. The novelty of the proposed mechanisms is limited, but this is okay. The paper is quite clear and easy to read. Some pro's and con's:\n\npro's\n\n* interesting results on novel benchmark, promising step towards ARC\n* compositional imagination is an interesting training paradigm adapted from prior work to the object-centric setting\n\ncon's\n\n* experiments indicate benefit of modular mechanism application, but the OOD results are not fully convincing yet. A potential reason for this is given by not explored\n* additional exploration regarding why selection hurts within distribution generalization or further exploration of imagination-based training (when it works, when it doesn't) would be helpful.\n\nWith regards to further developing this work I would encourage the authors to consider different modes of OOD generalization, eg. as considered in [2]. \n \n ", "rating": "0", "sentences": [{"sentence_type": "positive", "sentence": "The proposed architecture for solving these tasks is intuitive, although perhaps more development could go into the design of the controller to produce the instructions."}], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Ariadne"], "models": [""]}}
{"id": "B1xyY2gFhQ", "review": "This paper proposes to use a  stochastically quantized network combined with adversarial training to improve the robustness of models against adversarial examples. The main finding is that, compared to a full precision network, the quantized network can generalize to unseen adversarial attacks better while training only on FGSM-perturbed input. This provides a modest speedup over traditional adversarial training.\n\nWhile the findings are certainly interesting, the method lacks experimental validation in certain aspects. The comparison with other adversarial training methods is not standardized across networks, making the efficiency claims questionable. Furthermore, I am uncertain whether the authors implemented expectation over transformations (EoT) for the C&W attack.  Since the network produces randomized output, vanilla gradient descent against an adversarial loss is likely to fail. It is conceivable that by taking an average over gradients from different quantizations, the C&W adversary would be able to circumvent the defense better. I would be willing to reconsider my text if the authors can address the above weaknesses.\n\nPros:\n- Surprising result showing that quantization leads to improved generalization to unseen attack methods.\n\nCons:\n- Invalid comparison to other adversarial training techniques since the evaluated models are very different.\n- Lack of evaluation against EoT adversary.\n- Algorithm 1 is poorly presented. I'm sure there are better ways of expressing such a simple quantization scheme.\n- Figures 2 and 3 are uninteresting. The fact that the model is robust against adversaries implies that the activations remain unchanged when presented with perturbed input.", "rating": "1", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Ariadne"], "models": [""]}}
{"id": "BJg5O9zd2Q", "review": "The paper is easy to read and the presentation is clear, and I really appreciate this.\n\nThe authors address the very important topic of feature extraction and state representation learning. New results in this area are always valuable and welcome. However, my feeling is that the paper falls short in terms of making sufficient new contributions for an ICLR paper. \n\n1. The authors propose to learn a state representation by either training using a combined loss function, or training several representations using multiple loss functions followed by stacking. These are standard and well-known techniques in machine learning. The key contribution one looks for is in terms of new insights on why and when each approach works. The paper fails to provide much insight in this regard. Take this simple scenario: Suppose my input image is actually generated by a linear map plus gaussian noise on the true states. Then I can simply use a PCA as my \"auto encoder\" and happily learn a high quality state representation close to the ground truth. We know why this works. In the real task, the image is a complex non-linear transformation of the true states. What insights do I gain from this work in terms of how I should tackle this?\n\n2. Section 3 states some desirable characteristics in constructing a state representation. These are well-known and fundamental aspects of machine learning -- applicable to almost all models that we want to learn. In this sense, I do not find the section very informative.\n\n3. The empirical results (say, Table 1) seem too noisy to interpret (other than that using the ground truth provides the best performance). It almost seems to suggest that one should simply use random features (as done in the \"extreme learning machine\" approach). Again, not much insight to draw from this.\n\n4. Last comment. Suppose I have a new robotic goal-directed task and my inputs are camera images. Does this work tell me something that I don't already know in terms of learning new feature representation that is highly suitable for my task?\n\n\n\n", "rating": "1", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Ariadne"], "models": [""]}}
{"id": "0DFsJu3urT", "review": "Interesting parallelism setup for small batch sizes. Some nits:\n\n- \"In general, distributed training requires a minimum batch size per GPU (Œ≤min), which is generally close to one.\" why is this the case? many pipelines train with >>1 example per GPU.\n- \"On the other hand, increasing the batch size hurts the effectiveness of stochastic gradient descent\" the citation you include for that does not say that larger batches always hurt the effectiveness. in fact, many papers have shown that large batches train just fine with standard optimizers https://arxiv.org/abs/2102.06356 https://arxiv.org/abs/1811.03600\n", "rating": "1", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Ariadne"], "models": [""]}}
{"id": "HyjN-YPlz", "review": "The manuscript proposes two objective functions based on the manifold assumption as defense mechanisms against adversarial examples. The two objective functions are based on assigning low confidence values to points that are near or off the underlying (learned) data manifold while assigning high confidence values to points lying on the data manifold. In particular, for an adversarial example that is distinguishable from the points on the manifold and assigned a low confidence by the model, is projected back onto the designated manifold such that the model assigns it a high confidence value. The authors claim that the two objective functions proposed in this manuscript provide such a projection onto the desired manifold and assign high confidence for these adversarial points. These mechanisms, together with the so-called shell wrapper around the model (a deep learning model in this case) will provide the desired defense mechanism against adversarial examples.\n\nThe manuscript at the current stage seems to be a preliminary work that is not well matured yet. The manuscript is overly verbose and the arguments seem to be weak and not fully developed yet. More importantly, the experiments are very preliminary and there is much more room to deliver more comprehensive and compelling experiments.", "rating": "1", "sentences": [{"sentence_type": "2", "sentence": " More importantly, the experiments are very preliminary and there is much more room to deliver more comprehensive and compelling experiments."}], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Ariadne"], "models": [""]}}
{"id": "HfMgcgv6Xf5", "review": "# Summary\n\nThis paper proposes task imagination mechanism at the top of object-centric representation learning while also considering the scenario of out-of-distribution adaptation. The whole scheme consists of several steps:\n\n- A conditional model (controller) that generates a task embedding based on the given datasets.\n- A object-centric encoder where the k-slot representations will be updated according to the task embedding given by the controller.\n- An imagination mechanism to generate unseen tasks based on the combination of learned `condition c` and `transform p`.\n\nIn my opinion, the methodology is very promising. Also the application (ARC) is quite special since the image pairs represent some underlying transformation, and it is more difficult than image reconstruction task in some recent object-centric learning works. I vote for acceptance. \n\n# Some Questions\nProbably I miss something. There are also some unclear parts in the experiments that may need some explanation. \n1. How the hyperparams are set (e.g. $\\alpha_{rec}$, $\\alpha_{im}$ in the loss, the total number $c$ and $p$.)\n2. What's the difference between query output loss and support set output loss.\n3. Do we really need both `condition c` and `transform p` to modulate the object-centric representations? A ablation comparison against using only one (i.e. $h_k^{new} = h_k + \\tilde{h}_k$) will be convincing.\n4. More explanation is need for the metrics in Table 1:\n    - Although accuracy is improved for OOD tasks, the performance of *With Imagination* has $~3$% drop for in-domain tasks. Is it caused by the suboptimal setting of the regularization coefficient ($\\alpha_{im}$)? If so, a hyperparameter search could be helpful to provide more insights.\n    - The performance of *With Imagination* and *No Imagination* in OOD tasks is not convincing for me. Considering the super large variance of  *With Imagination*, it is hard to say it is better. Nevertheless, the advantage of the other components is obvious.", "rating": "0", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Ariadne"], "models": [""]}}
{"id": "HfY19gtnbL", "review": "Originality: I believe the theoretical results on distributed saddle point problem with $\\delta$-related assumptions are new.\n\nClarity: The structure and writing of the paper look fine. However, in the section 4, the authors provide little explanation for their algorithms. Although the algorithms are similar to some existing ones, it is easier for the readers to understand if there are some intuitions put in the paper. Same for the lower bound part, there is no overview for the hard instance in the main text. \n\nComments and questions:\n1. For the lower bounds in Theorem 1 and 2, on the right hand sides of the equations include only $\\|y^\\star\\|$. First, I think the authors did not define $\\|y^\\star\\|$ in the paper and I assume it is the optimal value for $y$. Second, the right hand side is not in terms of $\\|x_t-x_0\\|^2 + \\|y_t-y_0\\|^2$. Here if $\\|y^*\\|=0$, the lower bounds are not informative. \n\n2. The paper is based on many existing work, for example the lower bounds for smooth strongly-convex-strongly-concave minmax optimization and lower bounds for distributed minimization problem with $\\delta$-related assumption. Can the authors explain what is the technical novelty in this paper?\n\nminor:\n1. There is a typo under line 154 in the equation. \n2.  In line 17, the authors assume $X, Y$ to be compact, but in lower bound the case has unbounded domain. I think in the main text, the paper only discusses strongly-convex-strongly-concave setting and does not need to assume boundedness. It seems it is only needed in the convex-concave setting in the appendix. \n", "rating": "1", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Ariadne"], "models": [""]}}
{"id": "BJg2jRSKhm", "review": " This paper proposed a so-called ISS-GAN framework for data hiding in images, which  integrates steganography and steganalysis processes in GAN. The discriminative model simulate the steganalysis process, and the steganography generative model is to generate stego image, and confuse steganalysis discriminative model. \n\nOverall the application seems interesting. My concern is its use in real secure information transmission systems: it can fool human eyes but what is its capacity against decoding algorithms; if the intent is to transmit some hidden information, how the receiver is supposed to decode it; is there something similar to the public key in encryption systems? These basic questions/concepts should be made clear to the reader to avoid confusion. \n\nThe evaluation protocol should be clarified and especially on how the PSNR is calculated (i.e., using the reconstructed secret image and real one?) ", "rating": "1", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Ariadne"], "models": [""]}}
{"id": "QxkRd5ozRRy", "review": "This paper tries to interpret PER from the lens of replaying experience that has the most \"surprise\" and shows how it connects to some notions of value such as the expected value of the backup and policy improvement value and evaluation improvement value and argue. The authors also derive a max-ent version of this and show that this can improve performance on some Atari games (though this is not that convincing).\n\nMy score for this paper is based on these points:\n\n Motivation: I do not see the motivation for introducing these metrics and why that explains PER in the first place. Agreed that PER is a reasonable choice, and it can upper bound the EIB and EVB metrics (i have issues with this too, more on this next), it just seems to me that the paper doesn't make any convincing claim for why this helps us understand why PER works. If the focus of the paper is on understanding PER, then the paper does not do a good job of it. If it is to introduce these prioritization based on these metrics -- and the paper focuses entirely on them -- then I then have several concerns next.\n\n- Definition of the value metrics: The cited definition of these metrics requires using the true Q-function or the true value function of the resulting policy. If we end up approximating this using the learned model, what is the guarantee that these metrics are indeed useful? Also theorem 1 should be restated to say that they care about the \"empirical\" EIB and EVB, that is computed using the learned Q-function, else it doesn't make sense to me. Moreover, if the TD error is a bound (which I think isn't with neural networks as I discuss in the next point) on the empirical EVB, can't I just drastically overestimate Q-values and get a larger empirical EVB value to be super high and prioritize on those examples? Why is that good? Why won't that promote overestimation? \n\n- Why is the update on the Q-value assumed to be tabular if the experiments are with a deep network on Atari? In a non-tabular setting Theorem 1 does not hold so either that should be rederived for the case of DQN or the experiments should be adjusted to do it on tabular settings.   In any case, now it is not clear to me why the method works with DQN, since the update in this setting isn't equal to $Q(s, a) \\leftarrow Q(s, a) + \\alpha TD(s, a)$. In general, the solution isn't known with neural networks, so the upper bound story doesn't hold there. With the NTK (Jacot et al.) assumption, I can obtain a somewhat similar update but pre-conditioned with the kernel Gram matrix (see Achiam et al. Towards characterizing divergence in deep Q-learning). However, Theorem 1 doesn't hold anymore now. So, it is unclear why the method works.\n\n- Even if I were to look at the experiments only, the results are not that impressive. The method is generally close to PER, and maybe a little better, but no comparison is made on a more efficient method such as Rainbow, and there are only 9 Atari games, which is too little. So, that is not super convincing yet.\n\nI would suggest the authors make some of the changes above.", "rating": "2", "sentences": [{"sentence_type": "2", "sentence": "Even if I were to look at the experiments only, the results are not that impressive."}], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Ariadne"], "models": [""]}}
{"id": "GNDflfA73P", "review": "The paper proposes an extension to the DARTS method for neural architecture search, by augmenting it with constraints, in particular constraints on the model size and memory consumption. The DARTS method continuously relaxes the combinatorial NAS problem and is hence able to use gradient-based optimizers to find a good architecture. The proposed modification of DARTS simply uses a different loss function that penalizes high model complexity whenever the constraints are not fulfilled. The experiments presented are very limited, even for a workshop paper, and it is hard to draw any conclusion from them. Nevertheless, the problem that the paper aims to address is certainly relevant and the proposed method seems promising enough. ", "rating": "1", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Ariadne"], "models": [""]}}
{"id": "ukUut7k50d", "review": "Different from existing standard encoder-decoder networks, it proposes an explicit model for correlating fixed (inhale) and moving (exhale) image features. The features are extracted at sparse key points and transformed into a compact representation by CNN. Then a displacement map is calculated to measure their dissimilarity and further represented as displacement embedding.\n\nI think the performance of the proposed method could be affected by the number of key points and the size of the displace locations, although it is understandable not to include all the details due to the page limit. Also, there might be a possibility of estimating unnaturally aggressive deformations since they are estimated based on key points only. In the experiment shown in Table 1, it shows improved performance compared to other deep learning-based methods, including VoxelMorph and less obviously improved result compared to other algorithms for large deformations. ", "rating": "1", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Brad"], "models": [""]}}
{"id": "uineFvBjwqD", "review": "Summary: The paper  presents a data augmentation framework for zero-shot cross-lingual transfer learning. The framework uses different types of data (labeled source data, unlabeled source data, automatically generated augmented data) for training a model for the target language. Experiments are conducted on three different tasks: Named Entity Recognition (NER), Natural Language Inference (NLI) and paraphrase identification (PAWS). The approach combines multiple components together namely self-training, augmented sentence generation and confidence penalty.\n\nData Augmentation:\nThe paper states that data augmentation has been successfully used in images but not so much in  text (excluding back-translation). In fact, replacing creating augmenting text by masking and replacing word has been used in NLP both before and after pre-trained LMs such as BERT, etc.\n\n- Contextual Augmentation: Data Augmentation by Words with Paradigmatic Relations. Kobayshi. NAACL 2018\n- Conditional BERT Contextual Augmentation. Wu et al. 2018.\n- AUG-BERT: An Efficient Data Augmentation Algorithm for Text Classification. Shi et al. CSPS 2019\n- A lexical and frame-semantic embedding based data augmentation approach to automatic categorization .... Wang and Yang. EMNLP 2015.\n\nThe paper argues that generating new samples for data augmentation using the vicinity distribution of the source and target samples is better than back-translation since you cannot transfer the labels in sequence tagging tasks with back translation. However, similar problems would occur with vicinity distributions based augmentation since even changing a single word may result in changing the meaning of the sentence and hence the label (as the paper argues in Section 3.3.) . Given that, it would be useful to see more discussion/experiments comparing the two types of augmentation strategies especially that the self-training step can leverage augmented data without labels\n\nExperiments:\nThe paper provide a lot of interesting ablation and analysis. However one of the key questions that I couldn't get an answer to is what is the value of each source of data and could they be used in any different way. For example, what happens if we only do data augmentation for the source only, or the target only or by using translation or back-translation, etc. \n\nOn a related note, it looks like the specific order in which the datasets are used is important as shown in the experiments but it is not very clear what is the intuition behind that choice and whether other choices were considered or tried.\n\nOther questions:\n\nWhat is the benefit of Successive cross (vs. successive max)?\n\nWikiann is much bigger and has covers 40 languages. any reason why only 3 languages were considered from Wikiann and other languages from CoNLL?\n\n -----\nEdited after authors responses:\nI would like to thank the authors for the detailed response and the changes they have made to the paper. \n \n- Regarding contextual data augmentation (Kobayshi et al., Wu et al., etc.): Thanks for pointing out that these method use the labeled data to finetune the LM to make sure that words are replaced with other \"label-compatible\" words. Note that the comparison is not intended to necessarily show that the proposed method outperforms these baselines. Rather it is intended to guide the reader into making a decision about which method is more appropriate for which problem. If the findings are that the performance is comparable but one method will eliminate the additional finetuning step, that would be a useful finding to share. Also , it is not clear that  these methods would require labeled data in the target language for the finetuning or not.  For example, can the source labeled data be used for the finetuning step?\n\n- Regarding translation: Cross-Lingual transfer via Machine Translation does suffer from the label transfer problem for sequence tagging tasks (transferring labels for sentence-level tasks is straightforward). However, there has been several methods to address this in the literature by using unsupervised word alignment  (e.g., Yarowsky et al., 2001; Ni et al., 2017) or attention weights from NMT models (Schuster et al., 2019), heuristic approaches (e.g., Ehrmann et al., 2011) or co-learning alignment and tagging (Xu et al., 2020).\n\nA detailed comparison and discussion of the trade-off between the performance of each method and the resources they require would make the paper much stronger ", "rating": "0", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Brad"], "models": [""]}}
{"id": "SMMg9-chRzc", "review": "This submission is very badly written, it is not clear even on a basic level what is going on. The allusions to word2vec perhaps seem as if there is a 1-hot encoding of datapoints in the first layer which are embedded and then penalized to generate other datapoints, but which ones? neighbors? distal points? If it is just the point itself then this is no different than an autoencoder with a reconstruction loss. This has to be clarified. \n\nThe results of the MNIST embedding look very unconvincing as tSNE is showing better seperation. Also to compare visualizations I would suggest both looking at Moon et al. https://www.nature.com/articles/s41587-019-0336-3  for both the PHATE method as well as metrics such as ARI and DeMAP that are used for comparison.  ", "rating": "2", "sentences": [{"sentence_type": "2", "sentence": "This submission is very badly written, it is not clear even on a basic level what is going on."}, {"sentence_type": "ify", "sentence": "The allusions to word2vec perhaps seem as if there is a 1-hot encoding of datapoints in the first layer which are embedded and then penalized to generate other datapoints, but which ones? neighbors? distal points? If it is just the point itself then this is no different than an autoencoder with a reconstruction loss. This has to be clarified."}], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Brad"], "models": [""]}}
{"id": "Wiv9mUy4jsb", "review": "The authors present a new adaptive gradient method based on Nesterov acceleration. The results look very promising and the work is well presented. ", "rating": "0", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Brad"], "models": [""]}}
{"id": "2MOwojoQoyY", "review": "This paper develops a new, more efficient way to approximate the Fisher using a layer-wise local approximation, for use with natural gradient descent in deep learning. The manuscript is well written and easy to understand, and provides compelling results on the efficacy of the proposed method on certain problems when used in conjunction with the Shampoo optimizer.", "rating": "0", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Brad"], "models": [""]}}
{"id": "SyefBu5O3m", "review": "This paper proposes simple metrics for measuring the \"information density\" in learned representations. Overall, this is an interesting direction. However there are a few key weaknesses in my view, not least that the practical utility of these metrics is not obvious, since they require supervision in the target domain. And while there is an argument to be made for the inherent interestingness of exploring these questions, this angle would be more compelling if multiple encoder architectures were explored and compared. \n\n+ The overarching questions that the authors set out to answer: How task-specific information is stored and to what extent this transfers, is inherently interesting and important. \n\n+ The proposed metrics and simple and intuitive.\n\n+ It is interesting that a few units seem to capture most task specific information. \n\n- The envisioned scenario (and hence utility) of these metrics is a bit unclear to me here. As noted by the authors, transfer is most attractive in low-supervision regimes, w.r.t. the target task. Yet the metrics proposed depend on supervision in the target domain. If we already have this, then -- as the authors themselves note -- it is trivial to simply try out different source datasets empirically on a target dev set. It is argued that this is an issue because it requires training 2n networks, where n is the number of source tasks. I am unconvinced that one frequently enough has access to a sufficiently large set of candidate source tasks for this to be a real practical issue. \n\n- The metrics are tightly coupled to the encoder used, and no exploration of encoder architectures is performed. The LSTM architecture used is reasonable, but it would be nice to see how much results change (if at all) with alternative architectures.\n\n- The CFS metric depends on a hyperparameter (the \"retention ratio\"), which here is arbitrarily set to 80% without any justification.\n\n- What is the motivation for the restriction to linear models? In the referenced probing paper, for example, MLPs were also used to explore whether attributes were coded for 'non-linearly'. \n", "rating": "1", "sentences": [{"sentence_type": "ify", "sentence": "The CFS metric depends on a hyperparameter (the \"retention ratio\"), which here is arbitrarily set to 80% without any justification."}], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Brad"], "models": [""]}}
{"id": "ZuM4hIEX36", "review": "**Strengths**\n\n1. The empirical evaluations of this paper are comprehensive and solid, the results and findings are interesting. \n2. The paper is well-organized and clearly written. \n\n\n**Weaknesses**\n\n1. Despite good empirical efforts, the motivation of this paper seems somewhat unclear. Given that the encoder-decoder paradigm dominantly governs machine translation (also from the experiment part of this paper that LMs underperform EncDec most of the time.), for what reason should we need to consider a shift to a unified language model for such a seq2seq task? If the zero-shot transfer is the case, why not directly fix the off-target issue for EncDec and preserve the good of translation performance, instead of changing the paradigm? I feel like the authors didn't convey an incentive for this. \n2. The paper conducted extensive experiments to show how scaling affects LMs for MT, however, few suggestions based on the findings are given for future development of MT. \n3. The setting of CausalLM seems a bit weird that a unidirectional encoding behavior makes obviously no sense. \n\n**Questions**\n1. Section 5 seems to mainly examine p and L_{\\inf} In equation (7), whereas \\alpha remains undetermined. How was the value of \\alpha determined for diagrams in Figure 3? Get fitted from Figure 2 I guess? \n", "rating": "1", "sentences": [{"sentence_type": "2", "sentence": "The setting of CausalLM seems a bit weird that a unidirectional encoding behavior makes obviously no sense. "}, {"sentence_type": "ify", "sentence": "Get fitted from Figure 2 I guess?"}], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Brad"], "models": [""]}}
{"id": "4H1jTVEwu_K", "review": "In this paper, the authors investigate a risk-based ring vaccination strategy. Ring vaccination is a vaccine allocation strategy that vaccinates the contacts and contacts-of-contacts of an infected case. Here, the authors use an agent-based model to simulate an Ebola outbreak and test a variant of ring vaccination that prioritizes individuals within the contact-of-contact network with the highest risk (with risks estimated from the model). They show through their simulations that risk-based ring vaccination is significantly more effective than ring vaccination without prioritization, especially when more doses (100 or 200) of the vaccine are available.\n\nStrengths\n+ Risk-based ring vaccination is a nice idea and well-motivated\n+ The authors clearly demonstrate the effectiveness of this strategy through simulations\n+ The model is largely motivated by prior literature and uses parameters from prior work\n\nWeaknesses\n- The results feel almost like a foregone conclusion given the model, since they use risks from the model to decide which individuals to prioritize. It would be useful to establish, especially through mathematical analysis if possible, if we should be \"surprised\" by the results, or the settings that must hold true for risk-based to be significantly more effective.  \n- A lot of design decisions are made within the model, eg, levels of contact and types of contact within households/across households, disease parameters, etc. While it helps that parameters were mostly set based on prior literature, it would be useful to conduct sensitivity analyses to see how model results vary based on the decisions made.\n- Unclear if authors were the first to do risk-based ring vaccination. Also, unclear how realistic this model is in real life, since their simulation uses the individual's \"real\" risk from the model to determine prioritization. In reality, it seems hard already to get an infected person's contacts and contacts-of-contacts; would be even harder to know levels of contact/risk between all these people.", "rating": "0", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Brad"], "models": [""]}}
{"id": "ryx6YWDLo4", "review": "This paper addresses the problem of optimizing dynamic ridesharing in a setting that accounts for social preferences of requestors.  The primary technical contribution is a heuristic, real-time algorithm for matching requests to drivers that seeks to balance operational utility (vehicle operator's perspective) with user value.  The model of user value was derived from an extensive survey. A series of evaluations based on real-world taxi data shows that the approach improves user value without significantly affecting operational value for the service provider. \n\nThis is quite a nice paper:  interesting and timely topic area, technical solution well matched to the problem, solid experimental methodology, and good results.  The presentation and level of scholarship in the paper is high. \n\nA couple of comments:  \n-\tThe paper states that 5 minutes was set as an acceptable threshold for increased ride time by users.  But surely that threshold would depend on the length of the ride?   \n-\tThe increase in ride time is simplistic in that it accounts only for distance traveled.  More realistically, there would also be \"stopping time\" associated with pulling over, passengers entering/exiting the vehicle, retrieving luggage from the trunk, etc.  \n-\tThe population model was derived from a large-scale survey with \"demographic information ... drawn from the actual Chicago demographic distributions\".  Won't those demographics differ from the demographics of people who use ride-sharing services?\n-\tThe overall algorithm design separates Trip Formation from Trip Dispatch, leading to two separate heuristic planning algorithms.  Did you consider a uniform optimization algorithm that would combine both functions?   \n\n", "rating": "0", "sentences": [{"sentence_type": "ify", "sentence": "Won't those demographics differ from the demographics of people who use ride-sharing services?"}], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Brad"], "models": [""]}}
{"id": "Qm3pJMoqY4", "review": "##########################################################################\nSummary:\n\nThe paper develops a method to select a radar return region to be sampled at a higher rate based on a previous camera image and radar recording. Furthermore, the paper validates that an end2end transformer model trained on both camera and radar data outperforms an end2end transformer model only trained on camera data and hence, supports the argumentation to add a radar sensor to an automated vehicle.\n\n##########################################################################\nReasons:\n\nOverall, I vote for rejecting the paper. While it is generally a great idea to guide the selection of radar regions to be sampled at a higher rate the paper is very application-focused and lacks novelty in its method. The result that camera and radar data combined will outperform camera data only is expected. Using detections in images to guide the radar reverses its advantage to work well in adverse weather conditions compared to the camera.\n\n##########################################################################\nPros:\n\n* Interesting and relevant topic\n* Training results support claims\n\n##########################################################################\nCons:\n\n* Lack of algorithmic novelty\n* Use of Faster R-CNN (slow)\n* Using camera to select most important radar regions contradicts the stated advantage of radars to perform better in adverse weather conditions\n", "rating": "0", "sentences": [], "comments": "A good example of a critical yet academically productive review.", "annotation_info": {"type": "Manual", "humans": ["Brad"], "models": [""]}}
{"id": "mhHFcN_MZJ", "review": "The authors conduct empirical studies illustrating the phenomenon of slingshot mechanism as an inductive bias of adaptive optimizers, such as Adam, which promotes generalization. This encourages further work to better understand the dynamics of adaptive optimisers for training machine learning models. ", "rating": "1", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Brad"], "models": [""]}}
{"id": "RjBWM8Q4Cc", "review": "The authors present and evaluate five different methods for estimation of a motion vector from a series of 3D OCT volumes. The application is interesting and the proposed network architectures are intuitive and seem appropriate for the problem at hand. The use of 4D convolutions has not been explored extensively, and it is nice to see an application for them.\n\nSome minor remarks:\n\n- Please include the resolution (size in voxels as well as mm) of the input volumes.\n\n- What do the 12 outputs of the network represent exactly? If I understand correctly, the ouputs are 3D motion vectors (3 numbers), times 3 time points $\\Delta s_{t4}, \\Delta s_{t5}, \\Delta s_{t6}$. Would this not make 9 outputs?\n\n- In Figure one, it seems that one of the outputs is $\\Delta s_{t_0}$. Should this not be $\\Delta s_{t_n}$?\n\n- Data has been generated using smooth curved trajectories. It would be interesting to know how these were generated, and whether this resembles real data.\n\n- It took me a while to relate the 83Hz in the text to the 12ms in the Table, maybe make this more explicit.", "rating": "0", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Brad"], "models": [""]}}
{"id": "lhkV2ZAMP-", "review": "The key idea in the paper is to use functional prior that is completely uncertain about prediction of any class. To achieve this , the idea of introducing Dirichlet distribution after neural network is used from Evidential Deep Learning (EDL) paper. \nFrom table 1, it is clear that ECE is much lower for the proposed method. However, I have following concerns:\n1. It is not clear why calibration is reported and not simple measures of uncertainty like variance or entropy? Also, I would be convinced that the variance would increase for out of distribution test samples because you used a prior that enforced uncertainty of all labels. Now, it is difficult to connect  use of prior and improvement in ECE.\n\n2. What is the experimental setup? Did you train on some other dataset and test on skin lesion dataset?\n\n3. Last line of section 1: \"it can distinguish distributional versus data uncertainties\". How?\n\nOverall, the idea is fine.\n", "rating": "1", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Brad"], "models": [""]}}
{"id": "HJg4ZAZ0tH", "review": "The paper considers the problem of out-of-distribution (OOD) sample detection while solving a classification task. The authors tackle the problem of OOD detection with exploiting uncertainty while passing a test sample through the neural network. They treat outputs of (some) layers in a NN as random Gaussian-distributed variables and measure uncertainty as variance of these Gaussians. Then when uncertainty is high, OOD is detected.\n\nThe overall idea behind the paper could be interesting, but its realisation in the current form is questionable. \n\nThe paper seems totally misusing the reparametrisation trick and stochastic outputs of layers in NNs. Eq. (2) is not the objective of variational inference that seems to be required for stochastic outputs and the reparametrisation trick as presented before the equation. The objective misses the KL-divergence term! Without it what would stop a neural net to set sigmas to 0 and forget about the stochasticity altogether? Not to mention that the current objective is not mathematically justified.\nIf there is no mix and error in eq. (2) and the networks were trained using this loss (and based on provided code they were using this loss), my wild guess of explaining why this may give best results in the experiments is that the models were trained for surprisingly small number of epochs. Therefore, a hypothesis would be that this small number of epochs did not allow the networks to switch sigmas to 0. \n\nUntil the authors can clarify and justify the objective, I will vote for rejection only based on this ground.\n\nHowever, there are other issues in the paper as well. First of all, its clarity. It seems that the paper requires a lot of polishing.  The first paragraph of this review is based on my assumptions from the paper since I am not completely sure I understand it correctly. More about the clarity issues below\n\nFor strong evaluation, comparison with Malinin & Gales (2018) work seems to be important since it was the only work also using uncertainties for OOD detection in related work. Also related work section does not look like an exhaustive overview.\n\nSome of the detailed comments:\n1.\t\"In other words, in-distribution samples possess more features that convolutional filters react to than OOD samples\" \" first of all, this sentence is not easy to parse. Secondly, it is unclear, why this should be true. If OOD samples are still natural images, they would contain edges just the same as in-distribution samples. That is the power of deep learning enabling transfer learning, that low-level features are the transferable across different data and tasks. Therefore, the claim that \"Therefore, the uncertainties of the features will be larger when the inputs are in-distribution samples\" requires more elaboration and arguments\n2.\tThe arguments of the next paragraph regarding uncertainty of deeper layers should be larger for OOD samples are not very convincing either.  It is either requires a definition what the authors mean here as uncertainty, or it is not necessarily true that absence of fixed regions for embeddings leads to higher uncertainty. \n3.\t3rd and 4th paragraphs in Introduction have too many repetitions of phrases between each other. Compare, e.g. the first sentences of the paragraphs or the last sentences. \n4.\t\"One cause of the abovementioned problem is that their approaches\" and similarly the next paragraph: \"their approaches\" stylistically sound wrong. It is appropriate in the previous paragraph since there is a link to \"previous studies\". It seems that \"these approaches\" or \"the existing approaches\" would be a better choice for this and the next paragraph.\n5.\t\"Each uncertainty is easily estimated after training the discriminative model by computing the mean and the variance of their features using a reparameterization trick\" \" conventional discriminative models do not estimate mean and variances of the features. The issue of estimating uncertainty is addressed by several special methods such as Bayesian variational methods used in the referred papers. Therefore, in order to use a reparametrisation trick one need to firstly choose a special class of models, which is not obvious from the text.  \n6.\t\"Moreover, UFEL is robust to hyperparameters such as the number of in-distribution classes and the validation dataset.\" \" the size of the validation dataset? In any case neither size of the validation dataset nor the validation dataset itself are not hyperparameters (should not be hyperparameters for out-of-distribution detection). The number of classes can hardly be called a hyperparameter also. \n7.\t\"depends on the difference in the Dirichlet distribution of the categorical parameter <\"¦> In our work, the distribution of the logit of the categorical parameters\" \" what is/are this/these categorical parameter(s)?\n8.\t\"Further, they estimate the parameter of the Dirichlet distribution using a DNN and train the model with in-distribution and OOD datasets\" \" this sentence may mislead to impression that the proposed method does not need OOD dataset for training, which does not seem to be the case, since \\lambda and \\theta are trained based on OOD samples\n9.\t\"because they will not be relevant to the classification accuracy\" \" who are they?\n10.\t\"and \\epsilon is the Gaussian noise\" \"> the standard Gaussian noise\n11.\t\"where z^0 = x\" \" it seems this should be placed somewhere earlier when z^l is introduced since z^0 is not used in eq.(2) after which this text is placed\n12.\tIt is unclear how \\lambda^l and CNN \\theta are learnt\n13.\tIt is unclear how the values of features d(x) are used to detect OOD samples\n14.\t\"comparison methods, and models\" \" not clear what models mean here\n15.\tMissing references to datasets in the main text. At least reference to Appendix A.2 is required\n16.\t\"We used 5,000 validation images split from each training dataset and chose the parameter that can obtain\" \" which parameter? \n17.\t\"All the hyperparameters of ODIN\" \" a reader does not know yet that ODIN is used for comparison\n18.\t\"which consists of 100 OOD images from the test dataset and 1,000 images from the in-distribution validation set\" \" it is a bit confusing to call OOD dataset as a test dataset in this context\n19.\t\"We tuned the parameters of the CNN in Equation 4 using 50 validation training images taken from the 100 validation images. The best parameters were chosen by validating the performance using the rest of 50 validation images.\" \" this is confusing. What parameters do the authors talk about in the second sentence if not the parameters of the CNN?\n20.\t\"We used TNR at 95% TPR, AUROC, AUPR, and accuracy (ACC),\" \" Some elaboration is required, at least the reference to Appendix A.1. What is the changing threshold for AUROC and AUPR? Why AUPR-In and AUPR-Out are considered and only a single AUROC is considered. What is the positive class for AUROC?\n23.\t\"For LeNet5, we increased the number of channels of the original LeNet5 to improve accuracy\" \" do the authors mean that they allowed RGB images as input rather than greyscale? If yes, this explicit explanation would be preferable \n24.\t\"We inserted the reparameterization trick\" \" not the best word choice. Reparametrisation trick is a computational/implementation trick/method and it is hard to say that it can be inserted into a network. I believe what the authors mean is that they inserted mean/std outputs instead of point outputs. Conceptually, this means that the output of the corresponding layers is considered to be stochastic rather than deterministic. \nAlso, it is unclear when the authors say they insert it to the softmax layer. According to Section 3 the softmax layer is never considered to output means and stds.\n25.\tThe numbers of epochs for training NNs are very small for LeNet and WideResNet in the experiments. Did the models manage to converge during this short training?\n\nMinor:\n1.\t\"These data were also used\" -> \"this data\"", "rating": "2", "sentences": [{"sentence_type": "2", "sentence": "The paper seems totally misusing the reparametrisation trick and stochastic outputs of layers in NNs. Eq. (2) is not the objective of variational inference that seems to be required for stochastic outputs and the reparametrisation trick as presented before the equation. The objective misses the KL-divergence term! Without it what would stop a neural net to set sigmas to 0 and forget about the stochasticity altogether? Not to mention that the current objective is not mathematically justified."}, {"sentence_type": "ify", "sentence": "If there is no mix and error in eq. (2) and the networks were trained using this loss (and based on provided code they were using this loss), my wild guess of explaining why this may give best results in the experiments is that the models were trained for surprisingly small number of epochs. Therefore, a hypothesis would be that this small number of epochs did not allow the networks to switch sigmas to 0."}, {"sentence_type": "2", "sentence": "First of all, its clarity. It seems that the paper requires a lot of polishing."}, {"sentence_type": "ify", "sentence": "The first paragraph of this review is based on my assumptions from the paper since I am not completely sure I understand it correctly."}, {"sentence_type": "ify", "sentence": "More about the clarity issues below"}, {"sentence_type": "2", "sentence": "\"In other words, in-distribution samples possess more features that convolutional filters react to than OOD samples\" \" first of all, this sentence is not easy to parse. Secondly, it is unclear, why this should be true."}, {"sentence_type": "ify", "sentence": "The number of classes can hardly be called a hyperparameter also."}, {"sentence_type": "2", "sentence": "\"because they will not be relevant to the classification accuracy\" \" who are they?"}, {"sentence_type": "2", "sentence": "\"We tuned the parameters of the CNN in Equation 4 using 50 validation training images taken from the 100 validation images. The best parameters were chosen by validating the performance using the rest of 50 validation images.\" \" this is confusing. What parameters do the authors talk about in the second sentence if not the parameters of the CNN?"}], "comments": "Plenty here that could be *improved*... The decision to highlight sentences here seems completely dependent on \"toxicity\" threshold choice, though.", "annotation_info": {"type": "Manual", "humans": ["Brad"], "models": [""]}}
{"id": "HygQp_7b9r", "review": "# Summary\n\nThis paper trains a network to mimic simple known algorithms in a way that guarantees that they generalize to\nout-of-distribution test instances. The network mimics the algorithms by running repeatedly in a loop where each\niteration of the loop runs a Transformer and outputs a mask that tells the next iteration the inputs to process. The\nsetup is tested on sorting, adding, and graph algorithms, and found to learn regular number representations that\nsupposedly aid generalization.\n\n# Review\n\nThis paper has an admirable and useful goal, but the way it is currently implemented and presented is not ready for\npublication at ICLR.\n\nMy main issue is with the training/testing setup and its presentation. The authors assume a certain structure of an\nalgorithm (for instance, the iterative structure of recursive selection sort), delegate one or more modules inside this\nstructure to be implemented by neural networks, and train them only.\nMost of the \"strong generalization\" is coming from the fact that the iterative structure is fixed. The work abstracts\nout the most complex parts of each algorithm. In Figure 3, for instance, the NN must learn to find the smallest element\namong the non-masked-out ones on the input, return it, and mask it out. This is a much simpler task than the whole\nsorting algorithm. Training the network to solve \"find_min\" != claiming that the network solves and strongly generalizes\non \"sort\".\n\nImportant training details are left unspecified. How is the data for training NEEs generated? For instance, for training\nthe network in Figure 3, do you trace the whole selection sort on a randomly generated list, and collect the\nintermediate input/output pairs for \"find_min\"? If so, it's absolutely unsurprising that the process also works for\nlonger lists -- see above. Are composable NEEs, like the three networks in Figure 7, trained jointly or separately? Do\nthey observe their own outputs that are fed into subsequent NEE networks, or are the previous outputs teacher-forced,\nor are they pre-trained? Many of these details need to be clarified precisely to make the experimental setup\nverifiable.\nSome important details are presented factually without any motivation. For example, why does Figure 5 use\nSHIFT and XOR? Why, in general, the next mask produced by a NEE is XORed with a previous one instead of replacing it?\n\nI liked the embedding visualizations, which clearly demonstrate structure in the latent space driven by (a) the\nbinary number representations, and (b) the addition task objective. In addition to regular ordering structure (needed to\nimplement addition), the latent space also clearly exhibits regularities inherent to the binary representation, such as\nthe shift by 64 in Figure 8a. While interesting, this only confirms the findings of Shi et al., albeit in a more pure\nexperimental setting.\n\nIn summary, the scope of experiments and presentation of results would need to be significantly improved in order for\nthis work to reach the quality bar of ICLR.", "rating": "1", "sentences": [{"sentence_type": "ify", "sentence": "Some important details are presented factually without any motivation."}, {"sentence_type": "ify", "sentence": "In summary, the scope of experiments and presentation of results would need to be significantly improved in order for\nthis work to reach the quality bar of ICLR."}], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Brad"], "models": [""]}}
{"id": "chccuZzEknh", "review": "This paper proposes a method to train student policies when a teacher policy with extra information is available. The proposed method combines two objectives, the usual reward and minimizing the cross-entropy between student and teacher polices, i.e. push the student to imitate the teacher. The novelty of the method is to adaptively set the weight applied to the imitation objective, this is done by viewing the problem as a constrained optimization where the policy must be at least as good as some other target policy. The target policy is set to be another parameterized policy, trained without extra information nor teacher guidance to maximize reward, and the weight is adapted via the Lagrangian dual.\n\nThis seems like a nice straightforward workshop contribution. The writing is good and I was able to understand the paper fairly well, and this seems like a good use of constrained optimization. \n\nI do wonder if there is a simpler interpretation that exists that does not rely on access to a reference policy $\\pi_R$; in some deeper sense, what this lagrangian adjustment captures is a desire to unconstrain the policy improvement step when the policy improvement direction has a \"negative angle\" to the policy imitation direction. I recall some papers (e.g. [1]) constraining gradient steps in a multitask setting by projecting the gradients onto the right subspace so as to avoid interference. Perhaps something similar could be interesting to the authors. I also agree with the authors that per-state weights would be a very interesting contribution.\n\n[1] Gradient Surgery for Multi-Task Learning, Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, Chelsea Finn, 2020", "rating": "0", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Brad"], "models": [""]}}
{"id": "1m_YuhCMZ5Q", "review": "1. Originality: although the idea is not presented before, the k-reconstruction idea that views graph as a set of k-subgraphs has some connection to k-WL or k-GNN. As we know that each node in k-WL is a k-size super node which corresponds to a k-subgraph. And the k-WL algorithm over these k-size super nodes are permutation invariant which can be looked as implementation of the f_w function used in the paper. I wish the author provide more formal analysis to connect these two type of models, figuring out their difference and similarity. Nevertheless, from my end the presented k-reconstruction GNN is too complicated and is more like a toy for theoretical analysis instead of real-world useable model. \n\n2. Quality: the author provides a great literature review for expressiveness of GNN. The author also introduced the graph reconstruction theory in section 2. For the definition of reconstructible function, I wish the author can provide more insight of why introducing these defnitions in section 2. A trivial example of reconstructible function is a function that always output a constant representation for all graphs. In section 4 the author use these reconstructible function to characterize the expressiveness of k-reconstruction neural network, but before this I don't see any value of introducing these definitions. I suggest the author rewrite the section 2 part with giving some insight of why introducing these definitions from graph reconstruction theory. For section 3, the presented model looks like theoretical toy model to me. This is very similar to the relational pooling paper but I don't get the value of this model . The contribution of relational pooling is more like the universal approximation ability on graphs but this has been studied extensively. For section 4, the author provided several theorems to characterize the expressiveness of the designed k-reconstruction networks, however these theoretical results are not \"tight\" at all, most of these theorems just tell us that the k-reconstruction is not worse than GNNs. No amazing theoretical result shows that the k-reconstruction network achieves the best expressiveness (higher than k-WL/k-FWL). Also, the improvement on real world datasets are not much noticeable although it does show some ability in synthetic dataset. \n\n3. Clarity: In general the written is ok, but need some improvement to provide more insight in the flow to help reader understand the paper much easier. \n\n4. Significance: my impression is that the paper is more for theoretical analysis instead of real-world usage. I would like to see some more practical model that can make use of the author's analysis. The author did lots of approximations and it's really hard to know how much expressiveness left after these approximations. ", "rating": "1", "sentences": [{"sentence_type": "ify", "sentence": "This is very similar to the relational pooling paper but I don't get the value of this model ."}, {"sentence_type": "ify", "sentence": "No amazing theoretical result shows that the k-reconstruction network achieves the best expressiveness (higher than k-WL/k-FWL)."}], "comments": "An example of a review that could benefit from quite a bit of rewording in a more neutral sense.", "annotation_info": {"type": "Manual", "humans": ["Brad"], "models": [""]}}
{"id": "czzjj7Pbyw", "review": "Authors proposed a hierarchical fusion framework to integrate random classifiers. It is an interesting work and could have clinical impacts. Experiments show a large improvements from each individual classifier. \n\nMajor concerns:\n\n - Methods section is too short for readers to really understand HFRPC. It is necessary to summarize the two-level using equations. How to choose weights for each base classifier ? Also, Eq 1 is not clear due to the format issue.\n\n - Experiment section lacks lot of details. what value you are going to predict in survival prediction ? Did you predict if the patient can survival more than 2 years ? Also, how many radiomics features are extracted and did you use feature selection ?\nIn survival prediction, it usually reports C-index or AUC if the target survival year is specified.\n\n - Technical novelty is limited.", "rating": "1", "sentences": [{"sentence_type": "ify", "sentence": "Experiment section lacks lot of details. what value you are going to predict in survival prediction ? Did you predict if the patient can survival more than 2 years ? Also, how many radiomics features are extracted and did you use feature selection ?"}], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Brad"], "models": [""]}}
{"id": "r8QlT7O9wWc", "review": "Authors argue that combining iterative inference procedures (e.g. EM) with neural networks is challenging due to differentition through the inference process. They propose to make amoritzed iterative inference differentiable by implicit function theorem. This implicit differentiation decouples forward and backward pass and improves stability and tractability. They experiment with slot attention module in SLATE.\n\n### General comments:\nTypo in paragraph 2 of the introduction: \"e.g., n computes Î¸t+1 n â f (Î¸t n, xn )\", should it be f_w?\n\nTable 1 would've been more readable if it had 5 (1+2x2) columns, Data | FID_ours | FID_slate | MSE_ours | MSE_slate .\n\nI think the title 4.1 is wrong. It states \"does implicit differentiation stabilize the training of slot attention?\", but then talks about performance improvements. On the other hand, section 4.2 talks about training stabilization, namely it's first sentence is: \"To further understand the benefits of implicit differentiation, we then ask whether it stabilizes the training of slot attention without the need for optimization tricks like learning rate decay, gradient clipping, and learning warmup.\"\n\nFigure 4 is unclear what the shaded lines mean.\n\n### Pros:\nShows that implicit differentiation can stabilize the training and reduce the need for optimization \"tricks\". \nThe paper definitely fits the workshop theme, is technically correct, novel and clearly written. \n\n### Cons and room for improvement:\nI'm a bit skeptical that whether all the optimization tricks could be removed, as the authors only ablated by by removing individual tricks (figure 4). It would also be nice to see experiments with the original SlotAttention module, to see whether this improvement only applies to the SLATE model or is generally applicable for iterative inference in object-centric methods.\n\n### Final thoughts\nOverall I think this is a valid workshop paper, as it shows that a simple (in terms of lines of code) change can improve training of the Slot Attention module. Therefore I recommend acceptance.\n", "rating": "1", "sentences": [{"sentence_type": "ify", "sentence": "I think the title 4.1 is wrong. It states \"does implicit differentiation stabilize the training of slot attention?\", but then talks about performance improvements. On the other hand, section 4.2 talks about training stabilization, namely it's first sentence is: \"To further understand the benefits of implicit differentiation, we then ask whether it stabilizes the training of slot attention without the need for optimization tricks like learning rate decay, gradient clipping, and learning warmup.\""}, {"sentence_type": "ify", "sentence": "Figure 4 is unclear what the shaded lines mean."}], "comments": "Nothing screams \"toxic\" (or non-productive, discouraging, etc.) but it also very distinctly feels like it lacks the encouragement of the best reviews. Hence my '1'. Just adding a few suggestions or recognitions would make this so much better. Almost makes me want to highlight areas on that basis only.", "annotation_info": {"type": "Manual", "humans": ["Brad"], "models": [""]}}
{"id": "B1xnxSh4h4", "review": "The paper presents an approach for incrementally learning a probabilistic domain model in PPDDL. The motivation is that specifying complete domain models is challenging in one shot and that this problem is compounded in non-stationary environments. The paper introduces a novel framework, which is supported by a solid evaluation. Whereas the main issue with the current approach is learning overly conservative preconditions, it seems that this should be addressable within the current framework.\n\nThe approach to tackling the non-stationary environments is to only use the current example for generating training data for learning. Previous learning is communicated through a previous domain model, a reliability measure (to indicate the completeness of the previous model) and a list of state action pairs that led to failed action execution. This framework leads to a clean separation between past examples and derived knowledge and the current learning example. However, it does seem quite a specific scenario and I didn't find a convincing argument for why no previous data is retained. E.g., what happens if the framework is exposed to a small problem?\n\nThe factors involved in the reliability score seem reasonable, but little discussion is provided. This section as a whole would be improved with more motivation and better building of intuition. There are lots of parameters here and how any of these are set or how they should be set in a new environment is not clearly discussed. \n\nThe results indicate that the approach provides an effective learning framework. The analysis of the contribution of different factors is particularly informative. Some suggestion of how important using Gourmand for exploitation would be interesting, particularly in relation to Table 3. But overall, the current analysis of stationary environments is thorough and provides a solid base. It will be interesting to see how it acts in non-stationary environments and how the collection of parameters impacts on this.", "rating": "0", "sentences": [], "comments": "Clear, specific, constructive, and encouraging enough. Has (but could benefit from more) actionable suggestions.", "annotation_info": {"type": "Manual", "humans": ["Brad"], "models": [""]}}
{"id": "H1lHi3l_FE", "review": "Summary: This paper introduces three loss terms (L_cls, L_KL, L_sim), and shows performances each loss term. This work achieves reasonable performances without explicit class labels. I accept this paper.\n\nNotes:\n- The paper introduces the semantic similarity loss which is to learn similarity in the embedding space given similarity between targets. In the ablation study, the loss function plays an important role in improving results. \n- Also, they introduce the KL divergence term between the embedding distribution and binary target distribution. The paper also shows that the performances without the KL loss. With the KL loss, the proposed model shows substantial improvement.\n- The paper did experiments on different hash code lengths and showed better performances on the longer hash code\n\nI strongly accept this paper. This paper introduces novel loss functions and demonstrates that their method shows improvement in fully supervised and weakly supervised settings.", "rating": "1", "sentences": [], "comments": "Not much improvement recommended at all, but not toxic. Highly neutral tone overall, I feel.", "annotation_info": {"type": "Manual", "humans": ["Brad"], "models": [""]}}
{"id": "2zTspBhxFau", "review": "The paper is clearly written and well structured.\n\nI believe the question that the authors actually address, whether auxiliary tasks should be used separately or in conjunction with the main task, is important, and their results should be of interest to the community.\n\nHowever, I think the general framing of their paper in abstract / introduction is misleading. At no point do they train a model from scratch (i.e. without pre-training) with their proposed methods. They do justify this with the high cost of pre-training and the convenient availability of pre-trained models, which ironically would be my main criticisms of actually foregoing generic pre-training. So although they raise the question whether pre-training is necessary, they then don't actually compare against a model that is not pre-trained. Rather, they show that after pre-training it might not be necessary to further pre-train on large amounts of data just for domain adaptation.\n\nI think the paper would be much stronger if they did not defer their main question (\"Should we be pre-training?\") to future work, but rather tested their method with the typical MLM auxiliary task on a newly initialized Transformer model.\n", "rating": "0", "sentences": [], "comments": "A strong critique, but ultimately seems to genuinely engage with the work and provide actionable suggestions.", "annotation_info": {"type": "Manual", "humans": ["Brad"], "models": [""]}}
{"id": "B1eXpM0L6m", "review": "This paper suggests a source of slowness when training a two-layer neural networks: improperly trained output layer (classifier) may hamper learning of the hidden layer (feature). The authors call this \"inverse\" internal covariate shift (as opposed to the usual one where the feature distribution shifts and trips the classifier). They identify \"hard\" samples, those with large loss, as being the impediment. They then propose a curriculum, where such hard samples are identified at early epochs, their loss attenuated and replaced with a requirement that their features be close to neighboring (in feature space) samples that are similarly classified, but with a more comfortable margin (thus \"easy\".) The authors claim that this allows those samples to contribute through their features at first, without slowing the training down, then in later epochs fully contribute. Some experiments are offered as evidence that this indeed helps speedup.\n\nThe paper is extremely unclear and was hard to read. The narrative is too casual, a lot of handwaving is made. The notation is very informal and inconsistent. I had to second guess multiple times until deciphering what could have possibly been said. Based on this only, I do not deem this work ready for sharing. Furthermore, there are some general issues with the concepts. Here are some specific remarks.\n\n-\tThe intuition of the inverse internal covariate shift is perhaps the main merit of the paper, but I'm not sure if this was not mostly appreciated already.\n\n-\tThe paper offers some experimental poking and probing to find the source of the issue. But that part of the paper (section 3) is disconnected from what follows, mainly because hardness there is not a single point's notion, but rather that of regions of space with a heterogeneous presence of classes. This is quite intuitive in fact. Later, in section 4, hard simply means high loss. This isn't quite the same, since the former notion means rather being near the decision boundary, which is not captured by just having high loss. (Also, the loss is not specified.)\n\n-\tSome issues with Section 3: the notions of \"task\" needs a more formal definition, and then subtasks, and union of tasks, priors on tasks, etc. it's all too vague. The term \"non-computable\" has very specific meaning, best to avoid. Figure 2 is very badly explained (I believe the green curve is the number of classes represented by one element or more, while the red curve is the number of classes represented by 5 elements or more, but I had to figure it out on my own). The whole paragraph preceding Figure 3 is hard to follow. I sort of can make up what is going, especially with the hindsight of Section 4, since it's basically a variant of the proposed schedule (easy to hard making sure all clusters, as proxy to classes, are represented) without the feature loss, but it needs a rewriting.\n\n-\tIt is important to emphasize that the notion of \"easy\" and \"hard\" can change along the training, because they are relative to what the weights are at the hidden layer. Features of some samples may be not very separable at some stage, but they may become very separable later. The suggested algorithm does this reevaluation, but this is not made clear early on.\n\n-\tIn Section 4, the sentence where S_t(x) is mentioned is unclear. I assume \"surpass\" means achieving a better loss. Also later M_t (a margin) is used, when I think what is meant is S_t (a set). The whole notation (e.g. \"topk\", indexing that is not subscripted, non-math mode math) is bad.\n\n-\tIf L_t is indeed a loss (and not a \"performance\" like it's sometimes referred to, as in minus loss), then I assume larger losses means that the weight on the feature loss in equation (3) should be larger. So I think a minus sign is missing in the exponent of equation (2), and also in the algorithm.\n\n-\tI'm not sure if the experiments actually show a speedup, in the sense of what the authors started out motivating. A speedup, for me, would look like the training progress curves are basically compressed: everything happens sooner, in terms of epochs. Instead, what we have is basically the same shape curve but with a slight boost in performance (Figure 4.) It's totally disingenuous to say \"this is a great boost in speed\" (end of Section 5.2) by saying it took 30 epochs for the non-curriculum version to get to its performance, when within 4 epochs (just like the curriculum version) it was at its final performance basically.\n\n-\tSo the real conclusion here is that this curriculum may not have sped up the training in the way we expect it at all. However, the gradual introduction of badly classified samples in later epochs, while essentially replacing their features with similarly classified samples for earlier epochs, has somehow regularized the training. The authors do not discuss this at all, and I think draw the wrong conclusion from the results.\n", "rating": "2", "sentences": [{"sentence_type": "2", "sentence": "The paper is extremely unclear and was hard to read."}, {"sentence_type": "2", "sentence": "The narrative is too casual, a lot of handwaving is made. The notation is very informal and inconsistent. I had to second guess multiple times until deciphering what could have possibly been said."}, {"sentence_type": "ify", "sentence": "Based on this only, I do not deem this work ready for sharing."}, {"sentence_type": "ify", "sentence": "Some issues with Section 3: the notions of \"task\" needs a more formal definition, and then subtasks, and union of tasks, priors on tasks, etc. it's all too vague."}, {"sentence_type": "2", "sentence": "Figure 2 is very badly explained (I believe the green curve is the number of classes represented by one element or more, while the red curve is the number of classes represented by 5 elements or more, but I had to figure it out on my own)."}, {"sentence_type": "2", "sentence": "The whole notation (e.g. \"topk\", indexing that is not subscripted, non-math mode math) is bad."}, {"sentence_type": "2", "sentence": "It's totally disingenuous to say \"this is a great boost in speed\" (end of Section 5.2) by saying it took 30 epochs for the non-curriculum version to get to its performance, when within 4 epochs (just like the curriculum version) it was at its final performance basically."}, {"sentence_type": "2", "sentence": "However, the gradual introduction of badly classified samples in later epochs, while essentially replacing their features with similarly classified samples for earlier epochs, has somehow regularized the training. The authors do not discuss this at all, and I think draw the wrong conclusion from the results."}], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Brad"], "models": [""]}}
{"id": "NQzFKWwARw", "review": "quality: This is a well-written paper which tackles and interesting clinical problem, has a well-described framework and experiments and does a good evaluation.\n\nclarity: Of course more details would be nice, but considering the brevity of the submission framework for short papers, the paper is very clear.\n\noriginality: I am not aware of the background clinical literature in this area, but it seems a novel application.\n\nsignificance: The significance of the clinical solution is high and the presented algorithm seems to perform well enough to actually be a possible solution in the future.\n\npros:\n- the paper has a very nice twist of making the network robust, but excluding certain image modalities during training\n- the multi-task learning approach to learn the labels all at the same time is also very appropriate for this problem\ncons:\n- all abbreviations (OS, T1ce) should be introduced, not everyone has the same background in the clinic or in MRI to understand this\n- why are there so different balances in training, validation and testing data? why were they not all divided in the same way in terms of cases?\n\nCAVEAT: The authors state themselves in the abstract: \"This short paper only contains a brief summary and selection of results from a manuscript that will shortly be submitted to Neuro-Oncology.\" Is this allowed according to MIDL guidelines?", "rating": "1", "sentences": [{"sentence_type": "2", "sentence": "all abbreviations (OS, T1ce) should be introduced, not everyone has the same background in the clinic or in MRI to understand this"}, {"sentence_type": "2", "sentence": "why are there so different balances in training, validation and testing data? why were they not all divided in the same way in terms of cases?"}], "comments": "low 1/high 0; seemed so positive but clearly requires improvement in the wording of the 'cons' section.", "annotation_info": {"type": "Manual", "humans": ["Brad"], "models": [""]}}
{"id": "oeU5_5nU8tj", "review": "The authors propose another DARTS-based method for neural architecture search in AutoML with constrained model size. The general presentation of the work is comprehensive, but in many places unclear. \nThe reviewer recommends to check the work \"RC-DARTS: Resource Constrained Differentiable Architecture search\" by Jin et al. (2019), which also considers the number of parameters as a constraint. The algorithms in both works seem to be quite different, which is why I propose to accept this work.", "rating": "1", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Brad"], "models": [""]}}
{"id": "aFs-QMp8L0", "review": "**Strengths**\n* This paper is overall clearly written and easy to follow. The method is easy to understand and relatively straightforward. There are enough details to fully reproduce the training. \n* To my knowledge, evolving the optimal loss function for auxiliary RL tasks seems to be a novel approach. The central thesis of better representation learning in RL is a problem of good practical value.  \n\n**Weaknesses**\n\nMy major concern is the weak experimental results. To elaborate: \n\nFor pixel-based Deepmind Control Suite, the strongest baseline that the authors compare to is CURL (Laskin et al., 2020). However, this is quite an outdated baseline. The authors *ignore at least 2 recent strong baselines*:\n\n* Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels. Yarats et al. ICLR 2021 Spotlight. https://openreview.net/forum?id=GY6-6sTvGaf. The algorithm is known as \"DrQ\"\n* Reinforcement Learning with Augmented Data. Laskin et al. NeurIPS 2020. Also known as \"RAD\". \n\nBoth papers (neither cited in the paper) are published on top ML conferences before June 2021, so it is fair to request comparison with these prior SoTAs per the ICLR review guideline. Both papers above are about a very simple idea - vanilla reinforcement learning with simple data augmentation can be an exceptionally strong baseline. In fact, if we compare table 1 of the \"DrQ\" paper (Yarats et al.) with table 3 of this paper, we will see that DrQ *beats AARL-Pixel in 7 out of 12 tasks. For the other 5 tasks, none of the gains of AARL-Pixel is statistically significant.*\n\nThis indicates that even **simple RL with image augmentation can outperform the complicated bilevel optimization and auxiliary loss in AARL**, which greatly undermines this paper's contribution. \n\nFurthermore, in section 3.1, Fig. 4, the author does mention \"SAC with data augmentation\", which appears to use the same scheme as \"Reinforcement Learning with Augmented Data. Laskin et al.\" (RAD). However, SAC + augmentation (blue dashed line) consistently underperforms CURL (blue dotted line) in Fig. 4, which contradicts the results in RAD. In addition, the numerical results indicated by the lines also disagree with Table 1 in the RAD paper. I believe there are factual errors in these plots. \n\nFor state-based DMControl experiments, the paper claims in section 3.2 that \"there is no data augmentation in the state- based setting.\" This is not true. Section 5.4 in the RAD paper discusses simple ways to augment low-dimensional states, and shows that they are highly effective in boosting performance. \n\nMinor comment: table 1 has a misnomer. \"Inverse dynamics\" typically means action inference, instead of predicting the previous state. ", "rating": "1", "sentences": [{"sentence_type": "ify", "sentence": "However, this is quite an outdated baseline."}, {"sentence_type": "2", "sentence": "The authors *ignore at least 2 recent strong baselines*"}, {"sentence_type": "ify", "sentence": "This indicates that even **simple RL with image augmentation can outperform the complicated bilevel optimization and auxiliary loss in AARL**, which greatly undermines this paper's contribution."}], "comments": "Could absolutely be interpreted as overly critical, even if it is specific. It's not particularly encouraging.", "annotation_info": {"type": "Manual", "humans": ["Brad"], "models": [""]}}
{"id": "uw3oRgahlL", "review": "Summary:\nThe paper suggests an online learning algorithm to find the right amount of resources for serverless containers to reduce idle resources while meeting the user SLOs.\n\nPros: \n- Serverless functions are aimed in reducing developer effort in managing the resources and delegating that to the platform. The motivation of the paper is aligned with this design goal. The paper suggests delegating the right-sizing task to the platform so the developer does not need to explore this design space.\n- Serverless platforms are beneficial in the unpredictable environment, i.e., when the qps is not constant or pre-determined. A dynamic approach for right-sizing is aligned with this benefit of serverless functions\n- The resource right-sizing can bring dollar value to the vendors by increasing resource efficiency. \n- A white-box approach as presented in the paper can be insightful for the algorithm and can potentially results in better resource efficiency, compared to a block-box algorithm.\n\n\nCons:\n- On the other hand, a white-box approach might not be well received by customers. It could also allow developer to manipulate the learner algorithm.\n- There are currently some state-of-the-art serverless offerings from major vendors, e.g., Azure functions, that do not require user to specify the memory/cpu for the function. This is against the assumption/motivation of the paper.\n- The on-line right sizing will impact the debug-ability of the serverless function. That is, if the SLO is not met, how the developer is supposed to debug their service to tackle the issue?\n- A complicated algorithm such as on-line learner would bring computation and memory overhead to the serverless platform, hence negating the purpose. This overhead needs to be studied.", "rating": "1", "sentences": [], "comments": "Just feels blunt. Not particularly encouraging. If it were, easy '0'. Also, seems to be more of an evaluation of the practical value of the paper, rather than an evaluation of the paper itself.", "annotation_info": {"type": "Manual", "humans": ["Brad"], "models": [""]}}
{"id": "BfGxLM6BPW5", "review": "This work presents a novel dataset, called LOGICINFERENCE,  which was developed for evaluating the compositional generalisability of AI models, specifically for evaluating the performance of models to handle tasks of logical inference. Apart from this another goal of the dataset is to evaluate how well a model trained on this dataset can transfer learned knowledge to other tasks (although this is only mentioned as a step in future work). \nThe dataset consists of several interesting logic subtasks presented in semi-formal logical notation as well as natural language and contains fairly reduced number of samples, making it an interesting challenge, particularly for more data hungry deep learning models. The authors provide a small baseline evaluation, focusing on different versions of the T5.1.1 model, indicating the challenge of the dataset.\n\nIn summary the work is well written and the dataset seems interesting for initiating research in more structured models for logic inference. I believe it offers interesting points of discussion for the workshop.\n", "rating": "0", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Brad"], "models": [""]}}
{"id": "rJxIe2_Z5E", "review": "This paper introduces a new GAN model (LGGAN) for labeled graph generation. The output of LGGAN generator contains two parts: labels of each node (represented by one-hot vectors) and the adjacency matrix. The discriminator is a graph convolutional network that outputs graph-level scalar probability of the input being real data. Specifically, the author uses JK-Net to parameterize the discriminator. Empirical results show that LGGAN outperforms state-of-the-art baselines such as DeepGMG and GraphRNN in terms of the MMD evaluation metrics.\n\nThe proposed LGGAN model is very similar to MolGAN. In both model, the generator outputs the adjacency matrix and node label, and the discriminator is parameterized as GCN. The major difference is in the architectural choices of discriminator (JK-Net v.s. Relational GCN). Unfortunately the paper does not compare with MolGAN. It should be very easy to adapt MolGAN model for the datasets used in this paper. The reviewer is also concerned why LGGAN is not compared on molecule tasks. The \"specialized evaluation method\" has been established by previous work (e.g., MolGAN) and it's not hard to run at all.\n\nMoreover, I have several important questions that needs to be clarified:\n1) Since node labels and adjacency matrix are discrete values, how did the LGGAN propagates the gradient from the discriminator to the generator?\n2) The author mentioned in Section 3.1 that the node attributes are not included in the discriminator. If so, the discriminator only focus on the adjacency matrix and the node labels would be mostly random. How would LGGAN learns the distribution of labeled graphs like Protein?\n", "rating": "1", "sentences": [{"sentence_type": "ify", "sentence": "The \"specialized evaluation method\" has been established by previous work (e.g., MolGAN) and it's not hard to run at all."}], "comments": "Just not particularly encouraging.", "annotation_info": {"type": "Manual", "humans": ["Brad"], "models": [""]}}
{"id": "Hy7uO8PlG", "review": "*Quality*\n\nThe paper is easy to parse, with clear diagrams and derivations at the start. The problem context is clearly stated, as is the proposed model.\n\nThe improvements in terms of average log-likelihood are clear. The model does improve over state-of-the-art in some cases, but not all.\n\nBased on the presented findings, it is difficult to determine the quality of the learned models overall, since they are only evaluated in terms of average log likelihood. It is also difficult to determine whether the improvements are due to the model change, or some difference in how the models themselves were trained (particularly in the case of Z-Forcing, a closely related technique). I would like to see more exploration of this point, as the section titled \"ablation studies\" is short and does not sufficiently address the issue of what component of the model is contributing to the observed improvements in average log-likelihood.\n\nHence, I have assigned a score of \"4\" for the following reasons: the quality of the generated models is unclear; the paper does not clearly distinguish itself from the closely-related Z-Forcing concept (published at NIPS 2017); and the reasons for the improvements shown in average log-likelihood are not explored sufficiently, that is, the ablation studies don't eliminate key parts of the model that could provide this information.\n\nMore information on this decision is given in the remainder.\n\n*Clarity*\n\nA lack of generated samples in the Experimental Results section makes it difficult to evaluate the performance of the models; log-likelihood alone can be an inadequate measure of performance without some care in how it is calculated and interpreted (refer, e.g., to Theis et al. 2016, \"A Note on the Evaluation of Generative Models\").\n\nThere are some typos and organizational issues. For example, VAEs are reintroduced in the Related Works section, only to provide an explanation for an unrelated optimization challenge with the use of RNNs as encoders and decoders.\n\nI also find the motivations for the proposed model itself a little unclear. It seems unnatural to introduce a side-channel-cum-regularizer between a sequence moving forward in time and the same sequence moving backwards, through a variational distribution. In the introduction, improved regularization for LSTM models is cited as a primary motivation for introducing and learning two approximate distributions for latent variables between the forward and backward paths of a bi-LSTM. Is there a serious need for new regularization in such models? The need for this particular regularization choice is not particularly clear based on this explanation, nor are the improvements state-of-the-art in all cases. This weakens a possible theoretical contribution of the paper.\n\n*Originality*\n\nThe proposed modification appears to amount to a regularizer for bi-LSTMs which bears close similarity to Z-Forcing (cited in the paper). I recommend a more careful comparison between the two methods. Without such a comparison, they are a little hard to distinguish, and the originality of this paper is hard to evaluate. Both appear to employ the same core idea of regularizing an LSTM using a learned variational distributions. The differences *seem* to be in the small details, and these details appear to provide better performance in terms of average log-likelihood on all tasks compared to Z-Forcing--but, crucially, not compared to other models in all cases.", "rating": "0", "sentences": [], "comments": "Very specific and constructive. Encouraging? Close enough, maybe. I'd say borderline 0/1.", "annotation_info": {"type": "Manual", "humans": ["Brad"], "models": [""]}}
{"id": "r1xpLmpJK4", "review": "Summary: This paper conducted an excellent quantitative and qualitative review of the state of the reproducibility for ML healthcare applications.  I learned a great deal from reading it!  \n\nNotes: \n  -Reproducibility is especially important in health due to safety concerns.  \n  -Review of 100 ML4H research papers relating to reproducibility\n  -ML4H has more issues with data and code access.  \n  -Proposes recommendations to make research more reproducible.  \n  -In 2018, 12 healthcare tools using ML got FDA clearance.  (cool!)\n  -Quantitative and qualitative review showing ML4H has worse code availability data availability and dataset variety than other ML subfields.  \n  -The choice of evaluation metrics is reasonable but a bit limited.  \n  -Privacy issues make it difficult to release health data publicly.  \n  -ML4H papers are more likely to report mean/stdv than other fields in ML.  \n  -Only 19% of ML4H studies used multiple datasets.  \n  -The issue of preregistration in ML is interesting.  \n  \nComments / questions: \n  -Does failure to reproduce basic research papers in ML4H really lead to problems for production health systems?  Presumably there are many steps of validation beyond the basic research papers.  \n  -I really like the reproducibility taxonomy: technical, statistical, and conceptual reproducibility.  Technical reproducibility refers to getting the exact results, so includes things like implementation.  Statistical reproducibility is equivalence but only up to the statistical properties being the same (so the results follow the same distribution).  Conceptual reproducibility means that the idea works as long as the concept is preserved.  \n  -Figure 1 is quite nice\n", "rating": "0", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Brad"], "models": [""]}}
{"id": "LBBH0MoRM_v", "review": "The authors propose a unifying framework for reinforcement learning based on the dual form of seeing RL as a convex problem with linear constraints. They also use this framework to propose a new algorithm for imitation learning from arbitrary data that relaxes the assumption from previous work that requires expert coverage of the problem space. The authors are thorough in their framework and method. While the empirical results of their proposed method lacks analysis, the major contribution of this work is in their theoretical framework.\n\nHowever, the paper is only vaguely related to reusing prior computation for RL, in that the proposed method is for imitation learning with offline datasets. For this reason I rate the submission lower than the quality of the paper would suggest.", "rating": "1", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Brad"], "models": [""]}}
{"id": "klltXDz10r", "review": "This paper refines the existing finite-sample bound of double Q-learning. This paper considers the rescaled linear schedule of the learning rate and claims that the sample complexity bounds are improved in the sense the dependence on all main parameters (epsilon, 1-gamma, L, D) have been improved.\n\n\nOriginality: The paper follows the work in Xiong et.al 2020, but has made some non-trivial improvements. The nested SA representation of double Q-learning is interesting and the proving techniques seem new. \n\nQuality: The technical quality of this paper looks reasonably good to me although I have only roughly checked the proofs. The high level idea of the proof makes sense to me. I haven't found any flaws in the proof sketches.\n\nClarity: Overall the paper is well written. \n\nSignificance: I am not sure whether the significance level of this submission meets the standard of ICLR or not. The main reason is that the bounds in this paper are on the expectation of L1 norm of the iteration error. The bounds in the Xiong et.a. (2020) are high probability bounds. Is it fair to make comparisons between these two? The L1 bound is  weaker than the mean square bounds and I am not fully convinced how meaningful such results are. In addition, the implications of the proposed theory on algorithm design have not been fully verified on numerical examples.\n\nPros:\n1. The linear learning rate schedule is considered.\n2. The nested SA representation is interesting and the proof technique looks new.\n\n\nCons:\n1. The bounds on the iteration errors are in the L1 sense and it is unclear how meaningful these bounds are. Is the comparison with Xiong et.al (2020) really fair? \n2. Does the theory in this paper lead to any new insight for design and tuning of double Q-learning? Any numerical justifications for these new insights?\n \n\n", "rating": "0", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Brad"], "models": [""]}}
{"id": "-pKvzWAHsYf", "review": "### Pros\n- This work takes a problem-driven approach to improve the performance of a reinforcement learning agent on VizDoom. Although most of the RL literature starts from conceptual ideas to experimental results, I consider the present work to be valuable as it assembles and confronts some well-known techniques in RL to concrete problems.\n- The impact of the action wrapper and the rule-guided policy search is a great example that sometimes less is more. Using simple tricks on top of generic methods can sometimes be much more effective than complicated approaches.\n\n### Cons\n- The quality and clarity of the writing is problematic. Besides the numerous spelling mistakes, the imprecise language often confuses the reader. For instance:\n\t* \"...the state transition is not static\", we generally talk about stationary or non-stationary environments. I'm not sure what static means in this case?\n\t* \"..., while they are enabled to act diversely when the environment dynamics changes.\" The term `diverse` in the context of reinforcement learning is most often associated with Quality-Diversity that aims to generate a large collection of diverse solutions/policies. It is unclear whether the authors want to highlight the ability of their agent to adapt to changing dynamics (few-shot learning for instance) or if they want their agent to learn a policy performing well against opponents displaying a diverse set of strategies. For this reason, the method name, Diversified Strategic Control, is confusing as well.\n\t* The title mentions \"General Reinforcement Learning\". I'm not sure what the authors try to communicate? Besides the fact that general reinforcement learning feels undefined, the described approach uses tricks specific to FPS, like the statistical map provided as extra features to the agent's network or the rules defined in the 2 RGPS losses.\n- As highlighted by the authors, the proposed approach consist of a combination of existing techniques. Although replicating and using these techniques require significant engineering effort, these can't be considered novel contributions. Other works that took a similar work setting, often made novel and original contributions, e.g. AlphaStar and their league system.\n- The work is certainly complex from an engineering point of view: handcrafted curriculum learning, hindsight experience replay, distributed training strategy. As such, particular attention to providing all the required information to replicate the work has to be initiated. This aspect of the work is deeply lacking.\n\t- The values of all the hyper-parameters are not provided e.g. optimizers, losses coefficients, training parameters, etc.\n\t- Many terms and aspects are not defined. The authors don't explain: how they applied Policy Distillation, how the prioritized self-play has been implemented and put in action, what's the training dynamic (how long each stage lasts), where do the baselines come from (were they re-implemented or available as part of the competition?), etc.\n\tThese missing bits of information would make the replication of the results nearly impossible to achieve and don't help the reader understand how the approach fully works and how it was implemented. I would encourage the authors to make a few steps in this direction as the lack of transparency on their method is puzzling.\n\n### Questions\n- Figure 5 only provides the score for a few goals. Why is that, and could the performance of the agents for the other goals be provided in the appendix?\n- Regarding the training at stage 3, the authors mention that the selection of goals should be infrequent to avoid goal switching. Have the authors tried to constrain the switching of goals by restricting the goals to be adjacent to the current agent's goal, or by introducing a penalty in the reward?\n- How are the end-of-life events processed? When the agent dies, is it considered the end of the episode? What are the implications for the hidden state of the LSTM, does it persist over the 10 minutes of the Deathmatch?\n- What are the motivations behind having chosen PPO? The work requires applying Hindsight experience replay and importance sampling which increases the variance of the gradient. Have the authors tried using an off-policy algorithm that suits more easily HER?\n- I would like additional clarifications on the goal selection. If I understand correctly the third head can decide the goal. If, for instance, the agent detects an enemy in an area that is not its current goal. How can the agent decide to change its strategy given that the third head is only activated once the agent reaches its current goal?\n\n### Minor Comments\n- Hindsight Trust Region Policy Optimization appears twice in the bibliography.\n- Figure 2 is really hard to understand. It would benefit from a more extensive caption describing in greater detail each component and the transitions from one stage to the next.\n", "rating": "1", "sentences": [{"sentence_type": "2", "sentence": "The quality and clarity of the writing is problematic. Besides the numerous spelling mistakes, the imprecise language often confuses the reader."}, {"sentence_type": "ify", "sentence": "I'm not sure what static means in this case?"}, {"sentence_type": "ify", "sentence": "I'm not sure what the authors try to communicate? Besides the fact that general reinforcement learning feels undefined, the described approach uses tricks specific to FPS, like the statistical map provided as extra features to the agent's network or the rules defined in the 2 RGPS losses."}, {"sentence_type": "ify", "sentence": "As such, particular attention to providing all the required information to replicate the work has to be initiated. This aspect of the work is deeply lacking."}, {"sentence_type": "2", "sentence": "The authors don't explain: how they applied Policy Distillation, how the prioritized self-play has been implemented and put in action, what's the training dynamic (how long each stage lasts), where do the baselines come from (were they re-implemented or available as part of the competition?), etc.\n\tThese missing bits of information would make the replication of the results nearly impossible to achieve and don't help the reader understand how the approach fully works and how it was implemented."}, {"sentence_type": "ify", "sentence": "I would encourage the authors to make a few steps in this direction as the lack of transparency on their method is puzzling."}], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Brad"], "models": [""]}}
{"id": "SkeeJ9ku3Q", "review": "In this paper, the authors proposed to learn a stacked classifier on top of the outputs of well-known transfer learning models for transfer learning. The authors claimed that their proposed solution can avoid negative transfer.\n\nTechnically, there are no contributions. The proposed solution is a straight-forward A+B, where both A and B are well-known. Specifically, in the proposed solution, different well-known transfer learning models are used as the 1st level classifiers to generate intermediate outputs, then a stacked classifier is trained with the intermediate outputs as its inputs. Stacking techniques are also well-known in ensemble learning. Therefore, I do not see any new technical ideas. \n\nMoreover, the proposed solution cannot really avoid negative transfer. If two domains are indeed very different, the performance of the basic transfer learning models would be very bad, e.g., worse than random guess. In this case building a stacked classifier cannot help to boost the final performance. \n\nThe datasets used to conduct experiments are all of toy sizes.\n\nThere are a lot of typos and grammar errors. The format of citations in the main text are incorrect. \n\nIn summary, the quality of this paper is far below the standard of top conferences.", "rating": "1", "sentences": [], "comments": "Not remotely encouraging, but certainly specific and professional (even if very blunt). Hard to identify any specific sentences though! Maybe a textbook 1?", "annotation_info": {"type": "Manual", "humans": ["Brad"], "models": [""]}}
{"id": "SkgJtsr4YN", "review": "This is a nice submission. The authors show how a generalized entropy regularized policy formulation encodes a variety of approaches to seq2seq learning, from maximum likelihood estimation to reinforcement learning implemented using the RAML, SPG, and data noising methods. Besides the theoretical insight, the authors show how by implementing an easy-to-hard paradigm that resembles curriculum learning leads to improvements on two tasks: machine translation and summarization.\n\nThe paper is likely to generate good discussions, especially with respect to other approaches to sequence learning that are not yet encompassed by the proposed framework.\n", "rating": "0", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Brad"], "models": [""]}}
{"id": "cEdTt9ECmjL", "review": "The contribution of the paper is the use of Newton's method for solving the subproblem (5). The main part of the paper is focused on the computation of derivatives and results related to them, many of them very basic. I think this part is not relevant, and in any case might be included in the supplementary material. \n\nThe modification of the method seems to be of importance, given the complexity discussed (even though it might not work fast for some vectors), but I think the presentation should be dramatically improved.\n\nThe texts is repetitive, the toy example in Fig.1 is not explained or motivated at all, and there are some technical parts with details or omissions. For instance, in line 26 it should say that the vector to be projected is $y$; problem (1) should be actually argmin; on the line 30, the convexity of $\\Delta_k$ has to be used, otherwise the statement is false, Newton's algorithm doesn't have a stopping criteria, it runs for a fixed number $T$ of iterations, but $T$ is not an input, and is not discussed as it should.\n\nAlso, please proofread and check for typos, missing words, etc, and please be consistent with the notation, for instance in some parts $[]_+$ is used, and in other parts just $\\max(0,\\cdot)$, for the same argument.\n\nI tend to think that the result is important, but unfortunately I think the paper needs more work.\n", "rating": "1", "sentences": [{"sentence_type": "ify", "sentence": "I think this part is not relevant, and in any case might be included in the supplementary material. "}, {"sentence_type": "ify", "sentence": "The modification of the method seems to be of importance, given the complexity discussed (even though it might not work fast for some vectors), but I think the presentation should be dramatically improved."}, {"sentence_type": "2", "sentence": "The texts is repetitive, the toy example in Fig.1 is not explained or motivated at all, and there are some technical parts with details or omissions. For instance, in line 26 it should say that the vector to be projected is $y$; problem (1) should be actually argmin; on the line 30, the convexity of $\\Delta_k$ has to be used, otherwise the statement is false, Newton's algorithm doesn't have a stopping criteria, it runs for a fixed number $T$ of iterations, but $T$ is not an input, and is not discussed as it should."}], "comments": "1 borderline 2", "annotation_info": {"type": "Manual", "humans": ["Brad"], "models": [""]}}
{"id": "NoE09WESWRZ", "review": "Deep neural networks are known to be brittle, and can lead to dangerous consequences if left unverified. Forward reach set computation can be used as a basic primitive to verify properties of deep neural networks used in a robotic setting. There has been a rising interest in verifying larger neural networks used in safety critical setting. \n In this paper,  the authors propose a way to compute reachable sets for a neural network in a backward sense. Starting from the outputs of the neural network, and  then work it's way to the inputs. This is an interesting way to look at the problem itself,  but as the authors point out it is an intractable problem.\n\nMy concern about this paper is I don't see the use of a pre-image computation algorithm as being very useful. A forward reachability tool works pretty well for the size of neural networks considered in the paper. Pre-image computation does not provide any advantage in terms of scalability, as is apparent from the experiments. Moreover, almost any safety constraint that needs to be verified with system dynamics in the loop always should ideally work forward in time. Thus for the neural network controller from the inputs to the outputs. \n\nCartpole example : The authors come up with rules about, which output behaviors are correct for a few of the input regions. Then use this as a specification for the verification algorithm. But the very specifications, comes from reasoning about the forward behavior of the system dynamics itself. The idea of forward reach sets computation would generalize much better to a wide range of examples therefore.  Without the need to come up with such handcrafted rules. \n\nThe authors do make a convincing case for the ACASXu example. But this example is less interesting given the amount of attention it has received recently. ", "rating": "1", "sentences": [], "comments": "Harmless, professional, nothing obviously discouraging. Just don't quite feel like I can call it a 0.", "annotation_info": {"type": "Manual", "humans": ["Brad"], "models": [""]}}
{"id": "HXIS3TQQeDQ", "review": "The paper aims to make use of prior knowledge in RL, specifically in the case where it is known that certain sets of states are functionally equivalent and should have the same value. In addition to normal TD learning, it proposes updating states' value based on states that are known to be in the same functionally equivalent set, enabling faster learning. \n\nFirst, I'm not fully convinced about the novelty of this approach compared to the rich literature on works like bisimulation state abstractions, successor representations, etc. Second, my sense is that in practice, especially with realistic state/action spaces, it will be quite difficult for a human user to specify these invariances and know this set of equivalent states. Also, if the human does know this, I wonder if there are better ways of incorporating the prior knowledge, for instance, shouldn't the human then design a state abstraction that removes this ambiguity? It makes sense that this method works on Solitaire, but I'm not totally convinced there are many other domains where such a method would be practical to use. ", "rating": "1", "sentences": [], "comments": "Specific, respectful... Encouraging? Constructive with clear and supportive feedback? No. 1.", "annotation_info": {"type": "Manual", "humans": ["Brad"], "models": [""]}}
{"id": "fOBU1wV6enr", "review": "Strengths:\n+ Welcome empirical study of fine-tuning strategies specifically for source code applications.\n+ Good use of statistical analysis helps correct for small dataset.\n\nWeaknesses:\n- Empirical analysis is too narrow, focusing on a single task and dataset, as well as very small models.\n- Technical contributions, such as the fine-tuning of a single decoder layer, appear very ad-hoc.\n- Overall results provide no actionable new insight into the use of fine-tuning for software engineering.\n\nFine-tuning and other domain adaptation methods are rightfully becoming important subjects in machine learning driven software engineering research, so an empirical study of this process is relevant. However, the approach in this work is too narrow to provide much in the way of actionable insights. The analysis is based on a single paper and dataset, on which it considers mostly long-established fine-tuning methods. The results are exactly as expected; complete-model fine-tuning is somewhat slower, but effective; fine-tuning just one or two layers is a bit faster and slightly less accurate (depending on compute), etc. Indeed, much of the motivation is very generic to the motivation for, and problems with, fine-tuning in general. The use of statistics to avoid drawing unwarranted conclusions is a welcome addition and further reinforces that no new results are found.\n\nIn terms of the original motivation, which identifies key factors such as privacy and memory/compute footprint, the results do not provide a particularly satisfying conclusion. The analysis entirely considers other open-source projects, so it shines little light on fine-tuning behavior within corporate code-bases, which may well have much more different characteristics to the training data than this test set. And the compute results in Figure 4 suggests that simply fine-tuning the full model is already the best solution at around 1 PF-second, which translates to much less than a minute on virtually every commonly used GPU. That seems like a _very_ small cost for adapting to a new project, which somewhat undercuts the need for any other analysis here. On the other hand, arguable more product-relevant issues such as working memory usage and inference time/cost are not addressed by this work.\n\nThe main technical novelty in this work stems from the addition of fine-tuning a single decoder layer. This is based on an analysis that shows that higher layers are updated (rather) slightly more in terms of parameter change when fine-tuning the full model. This feels very ad-hoc; it falls well short of more grounded analyses of fine-tuning different layers such as in Yosinski et al. (NeurIPS 2014), and its motivation suffers from the clear problem that parameter changes in full-model fine-tuning are not necessarily indicative of which layer \"needs\" to be fine-tuned the most. Updates naturally cascade across layers, so the behavior in Figure 2 seems natural when updating an entire model. The results indeed do not show a substantial difference between this and updating another layer (embedding/output), except perhaps at very specific compute values, for which no significance test was applied.\n\nDetailed Comments:\n- Introduction: please add references to substantiate the claims in the first two paragraphs\n- P2, top paragraph: \"performances\" -> \"performance\"\n- P2: the first two bullet points in the introduction paint a very similar picture, mostly because of the second half of the first, which also essentially talks about privacy.\n- The use of the word \"Custom\" throughout the paper is rather odd. For one, it is often used interchangably with \"customized\", but those evoke rather different ideas -- \"custom\" suggests a specially designed model for a new task, which does not resonate the contributions here. \"Customized\" should really just read \"fine-tuned\"; I see no reason for another term. The variant called \"Custom fine-tuning\" (2.2) would be better called something like \"Full model fine-tuning\".\n- P3, last paragraph: \"finetuining\" -> \"finetuning\" (twice)\n- Table 2/3: why use such long IDs, instead of simply numbering the projects for this paper?\n- What motivated fine-tuning both embeddings and the output layer, besides following Lu et al. -- are these weights tied in this model?\n- Figure 2 is rather hard to read, primarily in terms of both axes' labels. Please also explain \"(Other, 1)\"\n- Comparing matching rates on abstracted code mostly makes sense in terms of ignoring identifiers, but not distinguishing between different objects of the same type means this metric does not capturing semantic correctness. Please clarify to what extent this impacts the results; ideally ablate this latter decision to better isolate the effect of just naming conventions.", "rating": "1", "sentences": [], "comments": "blunt. specific.", "annotation_info": {"type": "Manual", "humans": ["Brad"], "models": [""]}}
{"id": "v0wwNgiqS", "review": "In the introduction the authors state that none of the existing deep learning approaches to lobe segmentation use explicit knowledge from pulmonary fissures. This is not true, the Gerard and Reinhardt 2019 reference uses pulmonary fissures as an input channel. This method is currently the leader in the LOLA11 challenge which should also be mentioned.\n\nIt seems the proposed method requires a lung segmentation as a precursor which distinguishes left and right lungs (for cropping input). This needs to be explicitly mentioned. It should also be explained in the methods how this was obtained in this work.\n\nDuring training patches of size 60 are used, however, it is unclear what is done during inference. Are the same patch sizes used? Are non-overlapping patches used? If not, how are patch results merged?\n\nFigure 2 shows the \"mean distance to visible fissure\". It is unclear how this is calculated. For this to be calculated the evaluation data ground truth would need to include annotations of just visible fissures, however, based on the description it seems only complete lobe segmentations are available, i.e., all extracted fissures would be extrapolated and include both visible and non-visible fissures.", "rating": "1", "sentences": [], "comments": "Undoubtedly respectful, specific, and constructive. Encouraging? Borderline 0/1.", "annotation_info": {"type": "Manual", "humans": ["Brad"], "models": [""]}}
{"id": "BJx-6PRtnQ", "review": "This paper presents a new approach to an active learning problem where the idea is to train a classifier to distinguish labeled and unlabeled datapoints and select those that look the most like unlabeled.\n\nThe paper is clearly written and easy to follow. The idea is quite novel and evokes interesting thoughts. I appreciated that the authors provide links and connections to other problems. Another positive aspect is that evaluation methodology is quite sound and includes comparison to many recent algorithms for AL with neural networks. The analysis of Section 5.5 is quite interesting.\nHowever, I have a few concerns regarding the methodology. First of all, I am not completely convinced by the fact that selecting the samples that resemble the most unlabeled data is beneficial for the classifier. It seem that in this case just the data from under-explored regions will be selected at every new iteration. If this is the purpose, some simpler methods, for example, relying on density sampling, can be used. Could you elaborate how you method would compare to them? I can see this method as a way to measure the representativeness of datapoints, but I would see it as a component of AL, not an AL alone. What would happen it is combined with Uncertainty and you use it to labeled the points that are both uncertain and resemble unlabeled data? \nBesides, the proposed approach does not take the advantage of all the information that is available to AL, in particular, it does not use at the information about labels. I believe that labels contain a lot of useful information for making an informed selection decision and ignoring it when it is available is not rational.  \nNext, I have conceptual difficulties understanding what would happen to a classifier at next iteration when it is trained on the data that was determined by the previous classifier. Seems that the training data is non-iid and might cause some strange bias. In addition to this, it sounds a bit strange to use classification where overfitting is acceptable.\nFinally, the results of the experimental evaluation do not demonstrate a significant advantage of the proposed method and thus it is unclear is there is a benefit of using this method in practice. \n\nQuestions:\n- Could you elaborate why DAL strategy does not end up doing just random sampling?\n- Nothing restrict DAL from being applied with classifiers other than neural networks and smaller problems. How do you think DAL would work on simpler datasets and classifiers?\n- How does the classifier (that distinguished between labeled and unlabeled data) deal with very unbalanced classes? I suppose that normally unlabeled set is much bigger than labeled. What does 98% accuracy mean in this case?\n- How many experiments were run to produce each figure? Are error bars of most experiments so small that are almost invisible?\n\nSmall comments:\n- I think in many cases citep command should be used instead of cite. \n- Can you explain more about the paragraph 3 of related work where you say that uncertainty-based approach would be different from margin-based approach if the classifier is neural network?\n- Last sentence before 3.1: how do you guarantee in this case that the selected examples are not similar to each other (that was mentioned as a limitation for batch uncertainty selection, last paragraph on page 1)?\n- It was hard to understand the beginning of 5.5, at first it sounds like the ranking of methods is going to be analysed.\n- I am not sure \"discriminative\" is a good name for this algorithm. It suggested that is it opposite to \"generative\" (query synthesis?), but then all AL that rank datapoints with some scoring function are \"discriminative\".", "rating": "0", "sentences": [{"sentence_type": "ideal", "sentence": "Another positive aspect is that evaluation methodology is quite sound and includes comparison to many recent algorithms for AL with neural networks."}, {"sentence_type": "ideal", "sentence": "However, I have a few concerns regarding the methodology."}, {"sentence_type": "ify", "sentence": "I believe that labels contain a lot of useful information for making an informed selection decision and ignoring it when it is available is not rational."}], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Brad"], "models": [""]}}
{"id": "r42eqqUWkzc", "review": "Summary: the authors propose a novel learning setup, Cascade, that tasks the agent to intervene on the initial conditions of the scene to achieve a counterfactual outcome, given an observed scene. The agent is provided with an instruction that gives the agent a counterfactual goal to achieve, provides a hint on which object to intervene on, and provides constraints the agent must satisfy. This is challenging problem and the authors provide a method for solving this task.\n\nStrengths:\n- A novel data structure, the Event Tree, that make searching through possible futures efficient\n- A value function for focusing the search\n- Leveraging observed data to inform the search\n\nWeaknesses:\n- the proposed approach requires much domain knowledge of the underlying system in order to build the event tree. Such domain knowledge may not necessarily be available, or expensive to get, in applications.\n\nOverall, however, I see this as a novel and interesting benchmark and very much relevant to the workshop theme.\n", "rating": "0", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Brad"], "models": [""]}}
{"id": "SklcjZbP5r", "review": "This paper propose a modified U-net architecture to segment the stagnant zone during silo discharging process. It lacks novelty and the improvement is marginal.  More importantly than all of that, this paper violates the double blind review rule and is same with [1]. So I think this paper is not suitable for acception.\n\n[1]Waktola S, Grudzien K, Babout L. Stagnant zone segmentation with U-net[C]//2019 IEEE Second International Conference on Artificial Intelligence and Knowledge Engineering (AIKE). IEEE, 2019: 277-280.\n", "rating": "1", "sentences": [{"sentence_type": "ify", "sentence": "It lacks novelty and the improvement is marginal."}], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Brad"], "models": [""]}}
{"id": "BO_gDxO-kMq", "review": "Summary: the authors apply the Invariant Causal Representation Learning (iCaRL) framework to imitation and reinforcement learning.\n\nStrengths:\n- provides results on the conditions under which it is possible to expect generalization in representation learning, dynamics learning, and policy learning\n\nWeaknesses:\n- the method builds upon the non-linear ICA framework to identify latent variables. However, it is not clear in what sense these latent variables can be considered \"causal\" to the observed variables. As far as I know, from Elements of Causal Inference by Peters, Janzing, and Scholkopf, what makes a DAG \"causal\" is that it is the functions (i.e. mechanisms) that relate the variables are independent, and the paper was not clear how the non-linear ICA framework enforces the independence of mechanisms.\n\nThis paper was quite dense and had most of the results in the appendix. For example, the paper set up the imitation learning problem but did not present its proposed solution to this problem in the main text.\n", "rating": "1", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Brad"], "models": [""]}}
{"id": "BjKUyY7YDbn", "review": "Paper strengths:\n+ The reparameterization of hyperparameters to make them derivable seems good. The idea is a good idea, even if already existing in the literature.\n\nPaper weaknesses:\n- This paper is not novel, a lot of existing work already tested this idea and showed that it already works. See the following references:\n\nFranceschi, L., Donini, M., Frasconi, P. & Pontil, M.. (2017). Forward and Reverse Gradient-Based Hyperparameter Optimization. Proceedings of the 34th International Conference on Machine Learning, in PMLR 70:1165-1173\n\nStamoulis, D., Ding, R., Wang, D., Lymberopoulos, D., Priyantha, B., Liu, J., & Marculescu, D. (2019). Single-path nas: Designing hardware-efficient convnets in less than 4 hours. arXiv preprint arXiv:1904.02877.\n\nZhang, C., Ren, M., & Urtasun, R. (2018). Graph hypernetworks for neural architecture search. arXiv preprint arXiv:1810.05749.\n\nTwo of those references involve hypernetworks, with the same idea as proposed in this paper, which is define a larger network containing all subnetworks embedded within the hyperparameter search space.\n\n- The selected benchmarks are outdated.\n- Going back to the first point, the methods selected for comparison with the proposed approach are also outdated.\n- The paper is poorly written, hard to read and the ideas are not clearly stated.\n", "rating": "2", "sentences": [{"sentence_type": "2", "sentence": "This paper is not novel, a lot of existing work already tested this idea and showed that it already works."}, {"sentence_type": "2", "sentence": "The selected benchmarks are outdated."}, {"sentence_type": "2", "sentence": "Going back to the first point, the methods selected for comparison with the proposed approach are also outdated."}, {"sentence_type": "2", "sentence": "The paper is poorly written, hard to read and the ideas are not clearly stated."}], "comments": "It feels like it's trying to be helpful, but is just too blunt for me.", "annotation_info": {"type": "Manual", "humans": ["Brad"], "models": [""]}}
{"id": "SJlBb0_QYB", "review": "This paper proposes an estimator to quantify the difference in distributions between real and generated text based on a classifier that discriminates between real vs generated text.  The methodology is however not particularly well motivated and the experiments do not convince me that this proposed measure is superior to other reasonable choices.  Overall, the writing also contains many grammatical errors and confusing at places.\n\nMajor Comments:\n\n- There are tons of other existing measures of distributional discrepancy that could be applied to this same problem.  Some would be classical approaches (eg. Kullback-Leibler or other f-divergence based on estimated densities, Maximum Mean Discrepancy based on a specific text kernel, etc) while others would be highly related to this work through their use of a classifier.  Here's just a few examples: \n\ni) Lopez-Paz & Oquab (2018). \"Revisiting Classifier Two-Sample Tests\n\": https://arxiv.org/abs/1610.06545 \nii) the Wasserstein critic in Wasserstein-GAN\niii) Sugiyama et al (2012). \"Density Ratio Estimation in Machine Learning\"\n\nGiven all these existing methods (I am sure there are many more), it is unclear to me why the estimator proposed in this paper should be better. The authors need to clarify this both intuitively and empirically via comparison experiments (theoretical comparisons would be nice to see as well).\n\n- The authors are proposing a measure of discrepancy, which is essentially useful as a two-sample statistical test.  As such, the authors should demonstrate a power analysis of their test to detect differences between real vs generated text and show this new test is better than tests based on existing discrepancy measures.\n\n- The authors claim training a generator to minimize their proposed divergence is superior to a standard language GAN. However, the method to achieve this is quite convoluted, and straightforward generator training to minimize D_phi does not appear to work (the authors do not say why either).\n\n\nMinor Comments:\n\n- x needs to be defined before equation (1). \n\n- It is mathematically incorrect to talk about probability density functions when dealing with discrete text. Rather these should be referred to as probability mass functions, likelihoods, or distributions (not \"distributional function\" either). \n\n", "rating": "2", "sentences": [{"sentence_type": "2", "sentence": "Overall, the writing also contains many grammatical errors and confusing at places."}, {"sentence_type": "ify", "sentence": "Here's just a few examples"}, {"sentence_type": "ify", "sentence": "Given all these existing methods (I am sure there are many more), it is unclear to me why the estimator proposed in this paper should be better. The authors need to clarify this both intuitively and empirically via comparison experiments (theoretical comparisons would be nice to see as well)."}, {"sentence_type": "2", "sentence": "However, the method to achieve this is quite convoluted, and straightforward generator training to minimize D_phi does not appear to work (the authors do not say why either)."}], "comments": "'Toxic' would be the wrong word to describe this review, but it nonetheless could be improved a bit to promote a healthier peer-review environment.", "annotation_info": {"type": "Manual", "humans": ["Brad"], "models": [""]}}
{"id": "sSa7o2B8zSM", "review": "Pros:\n1. Standard training schemes and architectures cannot guarantee accuracy and certified robustness at the same time. The work focuses on addressing this issue and therefore the problem considered here is an important and challenging one.\n\n2. The construction of IBP-MonDEQ is novel and is among the first attempts to create certified implicit networks. While the interval analysis is relatively simple, the authors provide non-trivial theoretical analysis and guarantees.\n\nCons:\n1. Some of the details about theoretical formalism and empirical evaluation was not clear to me (see my comments below).\n\n2. The practical relevance of the certified robustness obtained on the more challenging CIFAR10 dataset presented here is not clear as the results do not advance the state-of-the-art. For example, the best-certified robustness for CIFAR10 for epsilon=2/255 achieved here is 51% with the corresponding standard accuracy as 64%. This is significantly worse than that obtained by the state-of-the-art which is 60.4% robustness and 78.4% accuracy as reported by COLT (https://openreview.net/pdf?id=SJxSDxrKDr). Similarly for epsilon=8/255, the best-certified robustness and accuracy achieved here are 30% and 44% respectively. These are again worse than the state-of-the-art bounds of 35% robustness and 50% accuracy from L_oo nets (https://arxiv.org/pdf/2102.05363.pdf).\n\nI have a few other questions:\n\n1. Since the interval abstraction is a complete lattice. Therefore, fixed points exist for any monotone (wrt interval inclusion) neural network. Is the class of networks that you identify in Theorem 3.1 and 3.3 a subset of those provided by the classical Knaster-Tarski theorem (https://en.wikipedia.org/wiki/Knaster%E2%80%93Tarski_theorem)?\n\n2. Is it possible to define a class of DEQs that have multiple fixed points? \n\n3. Can one equivalently identify a class of fixed-point obtaining implicit networks for other analysis types? e.g., CROWN, DeepPoly, or Zonotopes?\n\n4. The authors should consider providing an intuitive meaning of symbols (e.g., M, D) used in equations (3.4) and (3.7). The text in the evaluation says that M is a learnable parameter which makes things clearer but it comes too late. An example showing instances of W that satisfy these equations and those that do not would also help in improving the readability of the paper.\n\n5. Is it possible to train IBP-MonDEQs for perturbations that cannot be exactly captured by intervals?\n\n6. Is the certified robustness in Table 2 for explicit networks computed using IBP analysis? If yes, will the numbers improve with a complete verifier or with a more precise analysis like Crown or DeepPoly? This is important as it seems that the IBP-MonDEQ cannot be analyzed with anything else besides the IBP analysis while the explicit networks support other analyses.\n\n7. What are the implications of computing a post fixed-point by replacing the equality constraint in eq. (3.6) with interval inclusion, i.e., compute a fixed-point such that the interval of RHS is included inside that for LHS? ", "rating": "0", "sentences": [], "comments": "Specific, constructive, acknowledged the positives and was respectful. Not the best review I've ever seen, but I think it best fits into '0'.", "annotation_info": {"type": "Manual", "humans": ["Brad"], "models": [""]}}
{"id": "R074PlNeOBO", "review": "Review Summary\n--------------\n\nRecommendation: oral or poster\n\n__Relevance for workshop: 10/10__\n\nInvestigates the factors on how pre-trained visual representations can be helpful for downstream embodied AI / RL tasks.\nFits very well with the workshop topic.\n\n__Scientific quality: 9/10__\n\nLarge-scale empirical study that investigates different conditions (source dataset size, diversity, model size).\nProcedures are sound.\n\n__Paper quality: 9/10__\n\nWell written.\n\n(Points:  lowest: 0/10 means, highest: 10/10, >=5 means a recommendation to be included to the workshop)\n\n\nPaper Summary\n-------------\n\nThe paper is a large-scale study about pre-trained visual representations (PVRs) for embodied RL tasks.\nIt investigates the influence of different conditions on the performance of PVRs: the size of source datasets, diversity of source datasets, the scale of trained models, and type of models.\nIt evaluates the performance of different trained PVRs on several target RL tasks.\nKey findings show that larger source datasets, higher diversity, and larger scale of model result generally in a better PVR.\nNonetheless, the paper shows exceptions for specific tasks/conditions.\nThey also trained and provide the current SOA of PVRs in the embodied AI domain.\n\n\nMajor Points\n------------\n\nI didn't find major points in your current paper and I think it could be published as is.\nThe points here are meant for potential future research or to enrich the paper.\n\n1) You currently analyzing the influence of the source datasets on the resulting downstream performance of the PVRs.\nIt would be interesting to analyze the PVRs themself, such as: how they represent certain features or how the representation change depending on the different source dataset conditions.\nAnd how might different representation types aid the downstream tasks.\nThis could help to understand why you found exceptions where larger, more diverse source datasets do not improve performance over smaller, less diverse datasets.\n\n2) Your goal is to study the influence of different conditions, such as dataset size and dataset diversity on the trained PVRs.\nYou do this by using \"real\" datasets and target tasks where conditions such as diversity are difficult to measure and to control.\nYou might consider constructing an \"artificial\" dataset where you have more control over diversity and how well the data fits the downstream task.\nThen you could directly manipulate these factors and see their effect on the downstream performance.\n\n3) Influence of training time.\nIt would be great to see the influence of the training time on the PVRs.\nThe current paper shows their performance after a set amount of training epochs.\nIt would be interesting to see how well PVRs do that are trained after a different amount of epochs.\nMaybe also, if it is necessary to train them to \"completion\" on their source dataset.\n\n\nMinor Points\n------------\n1) I can not find how many (seeds/repetitions) you ran per downstream task to compute their mean performance and the std.\n2) Tables 2 and 4: The light grey text (line 1) is a bit too light. Could be darker.\n3) Sometimes you write \"ViT\" and other times \"VIT\", but I guess you mean the same thing?\n4) Chapter 5.1: All your datasets include the \"Ego4D\", thus you could avoid this name in your abbreviations to make them look less complex.\n   You could use \"Man+Nav\" to make the M and N easier to recognize for what they stand for.\n", "rating": "0", "sentences": [{"sentence_type": "ify", "sentence": "Sometimes you write \"ViT\" and other times \"VIT\", but I guess you mean the same thing?"}], "comments": "Not powerfully encouraging, but specific and constructive and clearly supportive in some way. Hard to call it a 1, so 0.", "annotation_info": {"type": "Manual", "humans": ["Brad"], "models": [""]}}
{"id": "HJXv1p8awc","review": "This paper presents a method to denoise low-dose CT images. In contrast to previously proposed methods, no proprietary projection data is required. Instead, the method operates on the image domain as well as on the spatial frequency domain. The authors show that the combination of networks operating in the image and spatial frequency domain leads to quantitatively better denoising results.\n \n Strengths \n - It's an interesting idea to apply a U-Net not only in the image domain but also in the Fourier domain. It's good that the method does not require sinogram data.\n - Experiments are well-structured and results are compared with statistical analysis. \n - The results show that operating in the spatial frequency domain has added value over operating only in the image domain.\n \n Weaknesses\n - There has already been a lot of work on deep learning-based CT image denoising. E.g. using wavelet transforms instead of Fourier transforms https://aapm.onlinelibrary.wiley.com/doi/full/10.1002/mp.12344 or using generative adversarial networks https://ieeexplore.ieee.org/document/7934380. In this context, the use of a perceptual loss is also not novel https://ieeexplore.ieee.org/document/8340157. None of these works are mentioned in the paper.\n - The data set used is quite small and the denoising results are only evaluated using quantitative measures that don't take into account for which clinical application images are made. It would be good to add evaluation using a clinical task, e.g. nodule detection.\n - The authors write that networks 'demonstrated exceptional contrast between [..] vessels and liver tissue' but this is not quantified in any way. In fact, all results in Fig. 1 look nearly identical, I'm not convinced that adding a spatial frequency domain network has much practical value.\n - Networks now operate in sequence, but it may be more interesting to operate them in parallel so that errors are not propagated.\n \n Detailed comments\n - A method to denoise CT images in a non-image domain has previously been proposed: \n - Please explain how image intensities were normalized, was this by linear scaling between two HU values? \n - How was a value of 0.84 selected for alpha?","rating": "1","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Hari"],"models": []}}
{"id": "rVggpjxRTZq","review": "This work studies the problem of transfer learning/adaptation/generalization in offline RL algorithms. The main idea is to train the offline RL policy (instantiated as a Decision Transformer) on a more diverse set of policy rollouts generated from counterfactual environments. Further, they also use the concept of average treatment effect (ATE), here defined as the difference in cumulative returns of a policy rollout on the source environment and counterfactual (CF) one as a weighting factor used to proportionately sample trajectories (akin to Prioritized Experience Replay) for learning policy updates. \n \n Pros: \n \n 1) The main ideas are simple and well-motivated. The writing and elucidation of the main ideas is clear and concise. \n 2) The research question being investigated is topical and very relevant to the workshop. \n 3) Ablation experiments showing the effect of both components i.e. counterfactual trajectories and non-uniform sampling based on ATE to allow the policy to generalize on unseen environments. \n  \n Cons: \n \n 1) Experimental results only shown on 1 newly created \"RandomObstacles\" environment from the Minigrid suite. \n \n 2) Crucially, this method assumes that the underlying generative process of an environment is accessible. How can this dependence be overcome? \n \n 3) Source and target environments in this work are only assumed to differ in the transition function but have the same reward function, which partially allows us to continue to use the source policy to generate CF rollouts from the CF environments. Despite this the authors still use an exploration \"fail-safe\" strategy when the source policy is stuck. How can the counterfactual data collection process be extended to a more natural setting where the transition model remains the same but the reward function differs across source and target environments? For example, in the physical world, Newtonian mechanics remains consistent but the tasks we would like to achieve varies.","rating": "0","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Hari"],"models": []}}
{"id": "tmNLCXiWNwV","review": "Overall, the paper is well-written but it does not contain significant contributions.\n The two main contributions are the use of a surrogate to create a surrogate benchmark and the creation of an interesting new search space for NAS.\n \n I see little contribution of demonstrating how to create a surrogate benchmark for NAS. This has been done earlier for hyperparameter optimization as pointed out by the authors themselves and NAS is nothing but a specific type of hyperparameter optimization. Additionally, Yan et al. presented with their paper \"NAS-Bench-x11 and the Power of Learning Curves\" already one way to create surrogate benchmarks for NAS.\n While the authors claim that a larger search space might be of huge interest for the community, no evidence for this claim was provided. There is no reason to believe that a larger search space is more interesting than a smaller one if not chosen correctly. Furthermore, the search space considered (among others) is the DARTS search space which is well-known and well-explored by the community. Datasets such as CIFAR-10 and CIFAR-100 are used in many NAS papers and the community is already overfitting to these tasks such that they started looking for more challenging problems. Concluding, I do not see this benchmark to be particularly useful.\n \n The authors don't discuss all the shortcomings of their benchmark. It is important to point out that the most efficient NAS methods do not evaluate an architecture completely. Methods such as DARTS or Hyperband search differently and it seems like that these methods cannot be evaluated on this benchmark. While methods exist that evaluate architectures completely, they have no practical purpose since they are simply too expensive to run.\n \n A comment on societal impact: The authors claim that millions of GPU hours will be saved and carbon emissions will be reduced. This is a bold statement and not supported by any evidence. What we have learned from the very first NAS benchmark is that it led to more and more NAS benchmarks, only spending more GPU hours. If the authors want to take the societal impact section seriously, they should consider the risk that they have potentially wasted energy and hardware for no reason.","rating": "2","sentences": [{"sentence_type": "2","sentence": "Overall, the paper is well-written but it does not contain significant contributions. "},{"sentence_type": "2","sentence": "  I see little contribution of demonstrating how to create a surrogate benchmark for NAS. "},{"sentence_type": "2","sentence": "  If the authors want to take the societal impact section seriously, they should consider the risk that they have potentially wasted energy and hardware for no reason."}],"comments": null,"annotation_info": {"type": "Manual","humans": ["Hari"],"models": []}}
{"id": "Hyxp2iQZcE","review": "The paper presents a tool to analyse various aspects of model trained using the VAE (amortised VI) framework for discrete data. VAE framework is known to be prone to several learning related issues such as slow convergence, posterior collapse, etc. therefore such a tool could provide a significant insight in tuning the model as well as selecting a model that best suits the needs.","rating": "0","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Hari"],"models": []}}
{"id": "SygPO9VuFN","review": "Summary of the paper:\n This paper proposes a technique to tackle the case where the labels of a supervised problem come from different sources and have varying quality. To this end, a three steps approach called FWL (Fidelity Weighted Learning) is proposed. In particular, FWL consists in constructing a \"student\" and a \"teacher\" interacting to refine the quality of the prediction.\n \n Some numerical experiments supporting the approach are proposed. In particular, the FWL algorithm is tested on the problem of document ranking. \n An investigation of the sensitivity of the approach to the quality of the weak annotator is also proposed.\n \n \n A few comments and questions:\n -the fact that the algorithm uses a Neural Net should be emphasized more in the introduction\n -Can the proposed approach be extended to other learning algorithm (i.e. not only a NN)?\n -Why specifically use a Gaussian process? Can another approach be used?\n -concerning the investigation on the \"sensitivity of the FWL to the quality of the weak annotator\" (sic),\n does a \"phase transition\" occur? i.e. is there a threshold on the quality of the weak annotator below which FWL does not yield any improvement?\n \n Reviewer's assessment:\n I found the paper to be fairly well written. The ideas are exposed clearly and the numerical results support the approach. Since the topic of this work clearly falls within the scope of the workshop, I recommend to accept this paper.","rating": "1","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Hari"],"models": []}}
{"id": "jQg09q4nWo","review": "This paper studied compositional problems where the task requires multiple ordered steps to complete. The authors proposed a novel algorithm that involves an iterative approach between using the learnt skills to generate new tasks and learning skills from those tasks. The process proposed in this paper forms an automatic curriculum for the agent to learn from. This paper aligns with the theme of the workshop in that it reuses learned skills or policies to generate new datasets during each iteration.\n \n In general, I find the idea of this paper interesting. The proposed method demonstrates good performance on a simplified setting. However, the presentation of the paper needs to be considerably improved for ease of comprehension from the reader's perspective. More details and clarification needs to be made before being published. Here are some suggestions that I think the authors can work on or make clarifications:\n \n 1. line 130: The task considered in this paper is not really long-horizon. From Table 1, the mean length of the trajectories is only 7.6.\n 2. line 140: From my understanding, the velocities of the objects are not needed since you are using teleports as actions. \n 3. I am very confused about how the goal is represented as inputs to the network since their size can vary during the course of learning. \n 4. How the $g^*$ being optimized from eq. (1) should be clarified. I think the choice of optimization algorithm here that deals with both discrete (the goal object) and continuous variables (the position of the goal) is important to the performance of the algorithm. \n 5. line 219: HER [1] should be cited here for the relabeling trick. \n 6. line 253: \"by assigning .... certain non-goal objects\". I do not fully understand what the author means from here. Does that mean you set object B's current position to be the target of object A so that the agent needs to move B somewhere else first, then they can move A to the target?\n 7. The success rate for the \"Y-shape\" in the first line of the Table 1 should be the same as in the fourth line of Table 3 from my understanding. Maybe there is a typo?\n 8. The differences between this work and the existing methods need to be discussed, for example [2].\n \n [1] Andrychowicz et. al., Hindsight Experience Replay, NIPS 2017\n \n [2] Li et. al., Solving Compositional Reinforcement Learning Problems via Task Reduction, ICLR 2021","rating": "0","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Hari"],"models": []}}
{"id": "BJeL9xKqtB","review": "This paper study the lottery ticket hypothesis by observing the properties of lottery tickets. In particular, the authors tested several different pruning techniques by varying evaluation criteria (L_1, L_2, L_-\\infty and random) and pruning structures (structured, unstructured and hybrid). The authors perform experiments mainly on LeNet with the MNIST dataset and analyze the observations.\n \n Overall, I think that the observations presented in the paper are not significant due to the following reasons.\n \n First, the paper consists of the list of observations but how the observations extend to is not clearly described. There are no guidelines how to utilize the observations in future research (e.g., how they can be used for verifying the lottery ticket hypothesis or how they affect to existing pruning techniques) while some observations might be trivial or not very interesting (e.g., contribution 1 and contribution 2) for me.\n \n Second, the observations are only presented for LeNet and MNIST and it is non-trivial whether they extend to large scale models. The authors present VGG11 and AlexNet results in Appendix but they are not large enough to verify their hypothesis for practice. The authors mentioned that larger models are not their subject, but this significantly reduces the confidence of the observations.\n \n Other comments:\n I think that Figure 5 is not well described. Explicitly noting the meaning of color in the figure would be better.\n \n Texts in Figure 7 are too small to read.","rating": "1","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Hari"],"models": []}}
{"id": "HkljfJVUKE","review": "This paper provides an unsupervised representation learning algorithm for performing classification and regression in multivariate time series. It relies on a combination of cutting-edge techniques: triplet loss, stacked causal dilated convolutions (à la WaveNet), weight normalization, and residual connections. Although these techniques had been published before in isolation, they had never been implemented in combination up to this paper. Therefore, the contributions of this paper are novel enough for the ICLR LLD workshop. \n \n The discussion of prior literature is solid. However, i will point out that the claim\n \"this works is the first in the time series literature to propose a triplet loss for feature learning\"\n is wrong. The paper of Jansen et al. ICASSP 2017 \"Unsupervised learning of semantic audio representations\" (https://arxiv.org/abs/1711.02209) is one counterexample.\n \n The rest of the paper is very clear and eloquent. I recommend this paper for acceptance.","rating": "0","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Hari"],"models": []}}
{"id": "Nu0maTYCh","review": "The idea is valid. And random projections are one method for stratified bootstrap sampling of datasets. Technically correct bu this paper falls significantly below the threshold of this conference. It is a passable workshop paper. The significance is rather weak. \n \n The evaluations reported in the table are the same as in figure. Putting both is redundant. \n The presentation can be improved a lot. \n There are meager significant differences of proposal with existing baseline ensembles.","rating": "2","sentences": [{"sentence_type": "2","sentence": "Technically correct bu this paper falls significantly below the threshold of this conference. "},{"sentence_type": "2","sentence": " The significance is rather weak. "},{"sentence_type": "2","sentence": " The presentation can be improved a lot. "},{"sentence_type": "2","sentence": " There are meager significant differences of proposal with existing baseline ensembles."}],"comments": null,"annotation_info": {"type": "Manual","humans": ["Hari"],"models": []}}
{"id": "HkeV9kcD2V","review": "The paper provides an interesting perspective on explanations between two knowledge bases, and runs parallel to the work on model reconciliation in the planning literature. \n \n 1) I am slightly disappointed that the authors did not make an effort to make connections to planning concepts. After all, its an XAI\"P\" workshop. The example in BlocksWorld is nice, but really comes at the end as an afterthought. \n \n It would be very helpful if both model- and proof- theoretic explanations could be scoped in planning terms in the beginning. Like \"no plan of lengths 1 to h-1 exists\" and \"a plan of length h exists\". What kind of questions is model-theoretic explanation solving? What about proof-theoretic?\n \n 2) Many of the concepts here have parallels to model reconciliation in planning. It would be great to see a discussion on those parallels. For example,\n \n -- minimality of explanations: Minimally complete explanations (MCEs) in [Chakraborti et al. IJCAI 2017] seem to be equivalent to subset-minimality in Definition 1.\n -- monotonicity of explanations: similarly, minimally monotonic explanations (MMEs) talk about the shortest explanation that can be given such that no further model updates invalidate it. This is somewhat complimentary to the general-support scene: except that these are explanations such that all subsuming eplisons are also valid supports. On the other hand, model patch explanations (MPEs) are trivial explanations as indicated by the fact that \"KB1 is itself an m-explanation for c from KB1 for KB 2\".\n -- negative clauses + monotonicity of logic (Proposition 1) and empty models: This is an interesting artifact, the implication being that explanation can address missing or lack of understanding of the user, but not mistaken expectations (in monotonic L). From the model reconciliation point of view, there is no such restriction. However, if the mental model is not known, and the we take the starting point as an \"empty model\" then one can see a similar property that explanations can only add to the user's understanding but not fix mistaken ones. \n -- preferences: Explanations in planning under the model reconciliation framework are also non-unique and may have arbitrary preferences from the user's point of view. The use of non-monotonic cost functions is interesting. I wonder whether there might be ways of mapping them to revealed preferences of users among otherwise \"logically\" equivalent explanations. This might be of interest: https://ieeexplore.ieee.org/document/8673097\n \n I had a lot of fun making these connections to the explanation literature in planning. I would request that the authors try to make a similar XAIP argument to the paper -- especially Question 1 above.\n \n Other:\n -- Definition 3: K --> L?\n -- ?s in final section","rating": "1","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Hari"],"models": []}}
{"id": "rygmm14EYV","review": "The authors propose using conditional entropy regularization during the training in semi-supervised settings to mitigate the bias of imbalanced data. The idea is elegant and effectively communicated, and the authors demonstrate its effectiveness empirically on MNIST and CIFAR-10/100.\n \n I suggest the committee consider this submission for best paper.\n \n I'm curious how the weight on the regularization term affects the bias mitigation.\n \n Minor grammatical errors: \"care should be taken *into* account\", \"a a substantially more diverse ranking\".","rating": "0","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Hari"],"models": []}}
{"id": "Ka8dgAhR3eM","review": "**1. Presentation and clarity**\n \n I believe the paper is very poorly structured, does not introduce related work properly, contains many unclear points in the presentation, making it almost impossible for the reader to grasp key ideas without reading at least three related works on which this paper is heavily based. Until one consults (Schonfeld et al., 2020) it is not clear what the per-pixel feedback means, it's also not clear what is inpainting regularizations. If we look at section 3.3 we might get confused what is the disciminator encoder and decoder, and how the losses are used to train them. These details are fleshed out in the appendix and in the prior work. \n \n I believe the flaw in presentation is due to weak originality of the paper. It borrows heavily from (Schonfeld et al., 2020) and (Siarohin et al., 2019b) and hence does not have much of original content. \n \n **2. Originality**\n \n The paper replaces the discriminator of (Siarohin et al., 2019b) with the one presented in (Schonfeld et al., 2020) supervising it with the images generated with the priority-cut scheme. The latter uses the occlusion mask of (Siarohin et al., 2019b) to generate a new image by combining the generated image with the driving image, attempting to show the discriminator which pixels require further attention. \n \n In my opinion, these ideas are quite marginal and do not contain any interest for community. The paper re-uses already existing and well working techniques, such as the whole framework of (Siarohin et al., 2019b).\n \n **3. Results**\n \n By looking at the results I cannot see significant differences compared with (Siarohin et al., 2019b). The artifacts present in the first order motion model are present here, so no noticeable improvement overall. The reader will be able to notice something only if they zoom into the figures and compare very small details. To facilitate such behavior, the authors increased the regions in which such details might be noticeable. Even with such visualization, I have to admit, the differences are insignificant. This observation is supported by the numerical results as well. The improvement over FOMM is 0.0012 in terms of L1 on VoxCeleb and 0.002 on Tai-Chi-HD and 0.0016 on BAIR. \n \n **4. Rating**\n \n I believe, poor presentation alone is a sufficient reason to recommend rejection. If we set aside it for a moment, we notice that the presented ideas are simple adaptations from previously published papers and results do not show necessary improvement.","rating": "2","sentences": [{"sentence_type": "2","sentence": " I believe the paper is very poorly structured, does not introduce related work properly, contains many unclear points in the presentation, making it almost impossible for the reader to grasp key ideas without reading at least three related works on which this paper is heavily based "},{"sentence_type": "2","sentence": " In my opinion, these ideas are quite marginal and do not contain any interest for community. The paper re-uses already existing and well working techniques, such as the whole framework of (Siarohin et al., 2019b)."}],"comments": null,"annotation_info": {"type": "Manual","humans": ["Hari"],"models": []}}
{"id": "KBgKj6rMXG","review": "In this paper, a fundamental problem in training deep learning networks is explored: How to automatically select a good batch size. In contrast to previous studies, the entire trajectory of the stochastic training process is considered here to determine an optimal batch size.\n \n \n The paper is clearly written and understandable. \n The topic might not fit fully to this workshop, since no practically applicable results are given. \n The paper is written in a clear and understandable way. \n The topic may not be entirely appropriate for this workshop, as no practically applicable results are given. \n \n Unfortunately, I am not familiar with Optimal Control and could not review chapters 3 and 4 in detail.\n \n \n Points of criticism are:\n - Please mention and compare to the approach of [1].\n - Please try to find practically useful experiments in addition to your rather simple toy example. Ideally, find a way to estimate the eigenvalues of the Hessian and show how your estimate works on CIFAR-10.\n  \n \n Questions:\n - Equation (3): What is the dimension of $\\Sigma$? If it is $n \\times n$, the dimensions do not match. \n - Equation (3): What kind of root operator is this exactly? (element-wise?)\n - Figure (1b): It seems to me to be coincidental that the divergence at the beginning leads to better losses. It could also be that the divergence is slightly different and then leads to worse loss as you increase the loss size. Please comment on this.\n - Appendix (A) line 187: Your results for the covariance matrix contradict the result of [1]. Please explain why?\n \n \n Formatting:\n - Figure (1) Headings and labels are cut off. \n  \n \n \n [1] Smith, S. L. and Le, Q. V. (2018). A Bayesian Perspective on Generalization\n and Stochastic Gradient Descent. In 6th International Conference on Learning\n Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018,\n Conference Track Proceedings. OpenReview.net.","rating": "0","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Hari"],"models": []}}
{"id": "FteLvJtLPU_","review": ".== Pros ==\n The paper involves extensive empirical study, which may be useful for some readers.\n \n == Cons ==\n While reading the paper, I really felt that each of the experiments are poorly motivated, without sufficient explanation of why each experiment should appear at a certain point and why each experiment and the corresponding message is important. It seems that the whole paper is simply enumerating many experimental results without proper structures. Taking a few examples,\n 1. Why is the observation of section 3.1 and Figure 1 important? I see that sparsity levels can be different for different layers and for different initial sparsity ratio, then so what? Such a discussion has not been provided with any proper context, so it is really confusing what we should do next and what will appear next in the paper.\n 2. Same for the section 3.2. Personally, this section was a bit informative, such as in Figure 2 and 3, sparsity allows much larger learning rates and longer gradient steps, which is not available for standard MAML. However, I'm still confused why we should investigate this phenomena at this point. What is the context? The paper simply repeats \"We next study ...\" without properly motivating the readers.\n 3. Same for the section 3.3. Why do we need to compare with gradient modulation methods at this point? I roughly see that the role of gradient modulation can be overlapping with gradient sparsity, but I'm not sure because there is no such discussion. Furthermore, in L158, why suddenly discuss about stochastic gradient masks? Is the Section 3.3 about stochasticity vs. performance? \n 4. Same for the section 3.4. I understand that sparsity is beneficial for cross-domain adaptation by looking at the table, but again, what is the context of this experiments? Why the performance improves?\n \n Same for all the other sections. I strongly think that the paper is poorly structured and should be rewritten with clear motivation. I understand that the paper is analysis-styled one, but it should be structured with clear motivation everywhere.","rating": "2","sentences": [{"sentence_type": "2","sentence": " While reading the paper, I really felt that each of the experiments are poorly motivated, without sufficient explanation of why each experiment should appear at a certain point and why each experiment and the corresponding message is important "},{"sentence_type": "2","sentence": " It seems that the whole paper is simply enumerating many experimental results without proper structures. "}],"comments": null,"annotation_info": {"type": "Manual","humans": ["Hari"],"models": []}}
{"id": "MFfc9iJQzs","review": "This manuscript proposes a new method for (differentiable) neural architecture search that takes into account model size constraints. The paper is clearly written and the results are compelling, demonstrating gains over previous (e.g. unconstrained) approaches.","rating": "0","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Hari"],"models": []}}
{"id": "Bkl4MaqOnm","review": "This paper addresses the problem of adversarial transferability, i.e. the ability that an adversarial example generated by one model can successfully fool another model. There are numerous papers on this topic recently, such as Fawzi'15, Liu'17, Dong'18, Athalye'18...\n The authors propose tot study two types of factors that might influence transferability: model-specific parameters and smoothness of loss surface for constructing adversarial examples. Two experimental studies are made for each influence factor from existing architectures. Another attack strategy aiming at smoothing the loss surface is proposed, an experimental evaluation shows the effectiveness of the proposed method.\n \n Pros\n -the proposed experimental studies can be interesting to the community\n -many interesting illustrations are provided.\n \n Cons\n -The conclusions of the study were suggested by previous papers or are rather expected: adversarial transfer is not symmetric: Deep models less transferable than shallow ones, averaging gradient is better\n -I find the experimental studies a bit limited and I would expect larger studies which would have improve the interest of the paper.\n -Only two influence factors are studied, again the paper would be more interesting with a more general study\n \n The paper has an interesting potential but seems a bit limited in its present form.","rating": "1","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Hari"],"models": []}}
{"id": "wHRx2y2IWJ","review": "The authors present a method to compute the sample size needed for estimating the bounds of PNS (probability of necessity and sufficiency) with a given confidence interval. The result of the paper is technically simple and straightforward. It leverages the facts that X and Y are binary variables and that the bounds for $PNS = P(y_x, {y'}_{x'})$ are linear combinations of the observational and experimental distributions. The idea simply relies on an indicator (binary) variable R for the events $y_x$ (R=1) and $y'_x$ (R=0), for both $X=x$ (true) and $X=x'$ (false). Despite the simplicity of the result, it is relevant for practical applications.\n \n Simulations are conducted to illustrate the estimation of the bounds of PNS with a confidence of 95% in two different SCMs. The simulations were conducted 1000 times. The results show that the number of samples is adequate, even though it considers the worst-case scenario, and, therefore, smaller sample sizes could be sufficient for simpler models. \n \n \n Minors:\n \n 1) 'The assumption is that one is in possession of a large enough sample to permit an accurate estimation of the experimental and observational distributions.' - I wouldn't say that this is an assumption of the theoretical bounds by Tian and Pearl. They proposed an estimator in terms of the real probability distributions, not of their estimates. Of course, if one needs to estimate these distributions in practical settings, large samples will lead to better estimates. \n \n 2) In Figure 2, the label for the X-axis is a bit confusing, since samples could be the sample size. I suggest using another label for it, for example, 'number of simulation trials'. \n  \n 3) In the appendix, line 231, there is a typo: occrus -> occurs","rating": "1","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Hari"],"models": []}}
{"id": "SJgQ2_I_YN","review": "This paper deals with the problem of image generation from scene graphs, building on Johnson et al ( Image generation from scene graphs, 2018). There are three main contributions in the paper: \n \n 1. a data augmentation scheme that employs heuristics to add fine-grained annotations of spatial relationships between pairs of objects in the scene, e.g., \"on top of\", \"left of\", \"behind\", etc. \n \n 2. A graph neural network that adds context on top of object segmentation masks, to maintain information about the relationships between objects. \n \n 3. A new evaluation metric that measures the compliance of the generated images to the (augmented) ground truth scene graph, as a fraction of the satisfied spatial relationships between objects in the ground truth. \n \n The experiments are expensive, including even a perceptual study using amazon turkers, and the results show noticeably improved performance, compared to the baseline, both in terms of IOU and the new proposed metric (MORS). I have a question/remark though: when the authors describe the heuristics they used to augment the data, they claim that \" A is 'in front of' B if the bottom boundary of A's bounding box is closer to the image's bottom edge\". I don't think this is true in the case where the bounding box of B is fully contained in the bounding box of A, in which case B is in front of A.","rating": "1","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Hari"],"models": []}}
{"id": "aq6_ayFN6_","review": "1. The paper is well written but I have a concern that the author's contribution is very low. There are many other papers that have used the same data and the same algorithms and reported the same results. \n 2. There are other criteria like AUC and sensitivity that have not been reported in this short paper.","rating": "2","sentences": [{"sentence_type": "2","sentence": "The paper is well written but I have a concern that the author's contribution is very low."}],"comments": null,"annotation_info": {"type": "Manual","humans": ["Hari"],"models": []}}
{"id": "BN2xsFsPYWq","review": "Summary\n This paper considers several relational tasks over simple, 2D shapes. The authors make a neuroscience argument for separating sensory inputs from relational reasoning, and extend an existing model, demonstrating better results on several tasks.\n \n Pros\n -Strong neuroscience background\n -Interesting insight, different from prevailing GNN methods of relational reasoning\n \n Cons\n -Not really clear how inductive bias of separating perception from relations is implemented--CoRelNet still derives its similarity matrix from percepts\n -Hard to understand CoRelNet well without understanding seperate paper, ESBN. Consider including more network description in paper.\n -Would be nice to see wider varity of tasks/visual inputs. Consider adding tasks from https://arxiv.org/abs/1911.01547\n \n Other feedback\n -Figure 2, plots missing (a) and (b) labels\n -Please add error bars and confidence intervals to all results","rating": "1","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Hari"],"models": []}}
{"id": "jafny_CBEOw","review": "The paper proposes a method for an improvement of generative adversarial models via post-processing its latent variable distribution. To be more precise, the method proposes to train an additional neural network that outputs an important weight for each point of the latent space, thus reweighting the final distribution in the space of images. For the optimization of this network, the authors use the dual form of the Wasserstein distance, where they multiply the initial latent density by the output of the network. To fix the ill-behaved objective, the authors add two regularization terms to it. The proposed objective is then validated on 3 MNIST-like datasets quantitatively and on CelebA qualitatively.\n \n Review:\n My major concern is the limited theoretical novelty together with modest empirical study. Let me clarify. I think the idea to put the filtering stage into the latent space is indeed worthy. However, the straightforward amortization of the discriminator network via a fully connected network is challenging due to the described computational problems and usually high dimensionality of the latent space. Furthermore, the verification of the method on MNIST-like data does not seem convincing, especially when the relevant works provide a comparison on ImageNet (Azadi 2018, Neklyudov 2019).\n \n Additional comments:\n 1. perhaps, I'm missing something, but for me, it is not clear why the objective in equation (3) corresponds to the optimization of Wasserstein distance in the space of images w.r.t. the parameters alpha and phi. I mean that there are even no guarantees that \\widehat{\\gamma} is a distribution.\n 2. \"since the rejection sampling scheme is now tractable, we do not need to implement the MH algorithm or the importance sampling\". Firstly, I do not understand why the rejection sampling is tractable. The regularization term does not provide any guarantees for the maximum value of the density ratio. Secondly, even if the rejection sampling is tractable, I still find the MH algorithm more efficient: it does not require the evaluation of the constant; given the same proposal, MH's acceptance rate is greater or equal to the acceptance rate of the rejection sampling.\n 3. the authors claim that reweighting in the latent space allows for better support coverage than the methods operating on the image space. Although I believe that such an effect occurs, I wouldn't expect the quality of images to be high. Indeed, this additional coverage could be produced by sampling from the low-density regions of the latent distribution. It is clear that such regions are underrepresented during the training. Moreover, there is empirical evidence of the deteriorating quality of images for latent distributions with higher variance (see Brock 2018).\n 4. the bottom of page 3. DRS does not assure sampling from the target distribution since it adjusts the constant and uses an approximation of density ratio. In contrast, the MH algorithm provides some guarantees by upper bounding the total variation distance between the stationary distribution and the target (see Neklyudov 2019).\n \n minor comments:\n 1. abstract. I would suggest finding a better analog for the phrase \"inject disconnectedness\". It does not sound like a desirable feature of your model when we speak about GANs, especially at the beginning of the paper, where few context is given. I would propose something like \"postselection\" or \"filtering\".\n 2. eq. 4, the signs of regularization terms are incorrect\n 3. typo on page 5, item 2). every methods -> every method\n \n References:\n 1. (Azadi 2018) Azadi, Samaneh, Catherine Olsson, Trevor Darrell, Ian Goodfellow, and Augustus Odena. \"Discriminator rejection sampling.\" arXiv preprint arXiv:1810.06758 (2018).\n 2. (Brock 2018) Brock, Andrew, Jeff Donahue, and Karen Simonyan. \"Large scale gan training for high fidelity natural image synthesis.\" arXiv preprint arXiv:1809.11096 (2018).\n 3. (Neklyudov 2019) Neklyudov, Kirill, Evgenii Egorov, and Dmitry P. Vetrov. \"The Implicit Metropolis-Hastings Algorithm.\" In Advances in Neural Information Processing Systems, pp. 13954-13964. 2019.","rating": "1","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Hari"],"models": []}}
{"id": "zcZ8axtzxfq","review": "This paper describes some experiments to simulate language acquisition in which a speaker talks to a listener over a continuous voice channel. The speaker uses a GRU conditioned by one or more concepts to generate a sequence of sounds selected from an inventory of 160 English phonemes released by eSpeak. The listener applies a convolutional net to the spectrogram of received sounds, then decodes sequential frames to generate a distribution over phonemes from which an argmax generates the decoded concept. Learning uses a simple 1-step DQN. The channel between speaker and listener has additive noise and time/pitch shifts applied.\n \n The experiments include the unconstrained transmission of 1 and 2 concepts and the same but constrained so that either the speaker knows English words or the listener knows how to recognise English words. Claims made include the ability to use composition and generalisation when moving from 1 to 2 concept transmission.\n \n There are many things that I do not understand about this paper.\n \n Firstly, when the speaker is generating a sequence of M phonemes, each phone is regarded as an action but the optimisation treats this an M-armed bandit problem and simply averages the Q values over all actions. Why is this not treated as an M-step trajectory and solved using the standard DQN optimisation?\n \n Secondly, it appears that all phoneme sequences are exactly M long, rather than a maximum of M long (see eg Table 1). Is this true? Wouldn't it be better to allow variable length and perhaps use the reward to encourage brevity.\n \n Thirdly, how are the input concepts coded? The examples suggest that each concept is coded as a 1-hot vector. For multiple concepts, the natural assumption would be that the concepts are simply concatenated. If this is the case, how does the listener work since it is only capable of forming a distribution over 1 set of concepts. How does the listener distinguish multiple component concepts?\n \n The claim that the results in Table 1 demonstrate the ability to learn a compositional language protocol are unconvincing. Quoting 0.25 as the chance mean reward ignores the fact that the agents could simply recognise one of the concepts whenever the combination was unseen.\n \n Adding noise will often improve the robustness of a classifier, why is the generalisation implied by Table 2 any different? What is the statistical significance of the numbers in this table?\n \n On page 9, it is claimed that using DTW for the 2 word listener case is impractical. Why is this? If the Listener only has to consider a vocabulary of 50 words, there are only 2500 two word combinations which should be tractable for a DTW style recogniser with pruning.","rating": "1","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Hari"],"models": []}}
{"id": "997dEeb_37E","review": "Summary:\n In this paper, the authors provide a series of experimens where they show that when dealing with a very small dataset, a single very deep network is outperformed by an ensemble of multiple more shallow networks. More specifically, the authors artificially create training sets from CIFAR10 and CIFAR100 datasets where the number of images per category is limited to 10-250 samples. Then, they compare the test performance of ResNet101, an ensemble of 5 ResNet 20 and an ensemble of 20 ResNet8, trained for classification with different loss functions, i.e. cross-entropy and cosine distance. The bottom line is that the ensembles work better and have a comparable computational complexity in FLOPs.\n \n Strengths:\n - The topic of the paper fits well in the paradigm of representation learning.\n - The work demonstrates that the community does not have a good understanding of what kind of models must be used when little data is available for training and brings attention to classical techniques for variance reduction.\n \n Weaknesses:\n - The paper is basically a compilation of experiments with no explanations of the observed phenomena. The authors perform a set of experiments with already known methods and merey propose the reader to look at the results. I would like to know not only that we need to do ensembles of small networks but also why these ensembles are more efficient than a single deep network in the low data scenario. Why do we observe the difference between using cross-entropy and cosine losses, depending on the network, dataset and its size?\n - The novelty of the paper is limited. It is already known from [1] that using ensemble methods in few-shot problems helps the performance a lot. Even if the authors propose a different evaluation strategy, referencing existing work in this field is still required.\n - Abblation studies are missing. To be more convinced by the experiments I would like to see how the performance differes if you vary the ensemble size and the single network's capacity. That may improve our understanding of the phenomena too. Experimenting with more datasets may help to answer the question of why the behavior of different loss functions is so different between the two used datasets.\n - A question of wheather a single ResNet101 with vanilla training is a fair baseline. It's been known [2] that to achieve better results on a small-data task it is beneficial to train deeper networks with proper regularization rather than shallow networks. Using an ensemble of N networks is identical to using a single network where each layer is N times wider; each convolutional layer will have N times more filters (that could be obtained by concatenating the weights of the original network), however the convolution operation now changes from a standard to a grouped one (has N groups). The resulting output of the fused network must be averaged across the groups to match the ensemble definition exactly. The group-separated convolutions restrict representational power of the network and introduce stronger regularization, which is most likely the reason for the ensemble to perform better. If we speak about regularizing ResNet101 what kind of regularization did you introduce to adapt it to the small size dataset? It is possible that vanilla training with higher weight decay and more data augmentation is not actually efficient in the case of ResNet101 on the small datasets. Instead, it may require introducing more aggressive data aufmentation [3,4] or some structural changes must be introduced, as for example in [5].\n \n \n Even though the direction of reserch is interesting and deffinitely useful for the community the work still needs development to be recommended for acceptance.\n \n \n [1] - Dvornik et.al \"Diversity with Cooperation: Ensemble Methods for Few-Shot Classification\"\n [2] - Geiger at.al \"The jamming transition as a paradigm to understand the loss landscape of deep neural networks\"\n [3] - DeVries et.al \"Improved Regularization of Convolutional Neural Networks with Cutout\"\n [4] - Zhang et.al \"Mixup: beyond empirical risk minimization\"\n [5] - Gastaldi \"Shake-Shake regularization\"\n \n \n -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n Update after the author's comment:\n \n I appreciate the effort of the authors to add more experiments that all suggest that the ensembles tend to perform better in the small data regime. This makes the case stronger and the story more compelling, hence I raise my score. However, the paper is still missing the core explanations or a hint of why this may be happening, hence I still can not recommend the paper for acceptance.","rating": "2","sentences": [{"sentence_type": "2","sentence": "The paper is basically a compilation of experiments with no explanations of the observed phenomena."}],"comments": null,"annotation_info": {"type": "Manual","humans": ["Hari"],"models": []}}
{"id": "RPOjACdR9qv","review": "Summary:\n \n This paper primarily deals with cross-domain few-shot learning. Under this setting, there is a large shift in domain going from the meta-train dataset to the few-shot datasets. Inspired by previous work, the authors argue that high-level concepts might not be useful in this setting but low-level concepts like edges, textures and shapes can be utilized. They propose a Cross-domain Hebbian Ensemble Few-shot (CHEF) learner, that learns an ensemble of classifiers at multiple levels of a deep neural network, thus making use of both low and high level concepts. Experimental results show that CHEF does better, in most cases, than learning a separate classifier at a given level. They show results under the cross-domain and the standard few-shot setting.\n \n Pros:\n 1. Utilizing low and high level concepts is a simple technique to boost few-shot learning performance.\n 2. CHEF does not require any updates to the weights of the model backbone. It learns additional weights for classification.\n \n Cons:\n 1. The paper is missing details. The authors talk about Hebbian learning, FID, etc. but do not give details about it. The experimental set-up is missing information about how the models are trained and tested.\n 2. Is the proposed algorithm a Hebbian learner? The update in Equations 1 and 2 is a standard gradient descent update.\n 3. Using 2 fully-connected layers changes the model backbone from ResNet-x to a ResNet-(x+1). This should be clearly noted in Table 2 and 3.\n \n Clarifications:\n 1. For the experiments, the softmax output layer has as many units as the number of classes in the meta-train and meta-validation sets combined. Does this mean that the pre-training is done on the sets combined? If so, this is not an apples-to-apples comparison. If not, the model does not know the difference between the classes in the meta-validation set. How does having these classes in the validation set help while pre-training?\n \n Notes:\n 1. Mini-ImageNet and Tiered-ImageNet do involve general domain shifts. Even though p(x) does not change much going from meta-train dataset to the few-shot datasets, the samples seen in the two scenarios are disjoint.\n 2. The Appendix should be cleaned up.","rating": "1","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Hari"],"models": []}}
{"id": "r1gSl4pJFV","review": "Summary: This paper is a reproduction attempt of the Bertinetto et al. 2019 paper \"Meta-learning with differentiable closed-form solvers\", an improvement over Finn's 2018 MAML (Model-Agnostic Meta-Learning) paper. It is a competently-executed reproduction attempt. It has successfully reproduced Bertinetto et al, modulo small, insignificant differences that do not affect the conclusions, provides a TensorFlow implementation of the original authors' proposed algorithm, and additionally resulted in the original authors releasing a PyTorch implementation.\n \n The reproduction focuses on Bertinetto's proposed Ridge-Regression Differentiable Discriminator (R2D2), leaving aside their Logistic Regression Differentiable Discriminator (LRD2) because its solution is not truly closed-form. The reproduction provides a service to readers of Bertinetto's paper by providing the R2D2 algorithm in pseudo-code, rather than textual form.\n \n The reproduction provides some background context for Bertinetto's paper, explaining the N-way K-shot (few-shot) meta-learning problem. Figures 1 & 2, however, do not succeed in explaining the problem well; They are as helpful as they are confusing.\n \n Whereas Bertinetto's paper fails to provide certain parameters of the neural network's convolutions layers, among others, the reproduction attempt correctly guesses and publishes them with their reproduction source code. Correspondence with Bertinetto et al via OpenReview has led to improvements to their paper and release of their code.\n \n The reproduction attempt strengthens in some places Bertinetto's paper (and the older MAML paper), and is a worthy contribution to the workshop and to science in general. My only comment is that Figures 1 & 2 are not clear, and distract from the background explanation.","rating": "1","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Hari"],"models": []}}
{"id": "pDVuYvR6rV","review": "This paper combines recent methods in improving NNs' generalization performance: SAM and self-distillation. The initial experiments results look good and the method is interesting. Thus, I vote for acceptance.","rating": "0","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Hari"],"models": []}}
{"id": "NhnjMmIHNvj","review": "Strength:\n 1) CoDEx proposed in this paper provides a more comprehensive understanding of the video feature by extracting concepts from the explanation of the labels and constructing a higher-dimensional concept space. \n 2) The pipeline can be easily applied to any supervised video classification architecture.\n \n Weakness: \n 1) One of the motivations of the CoDEx is to get concepts without domain experts, but only simple cases are reported in the results. In a simpler domain, the label will be highly correlative to the concepts, so the requirement of experts is low. However, in a more complicated domain, the requirement of experts will become higher. Whether the explanation will still be reasonable is not reported. \n 2) This work is more like a \"label decomposition\" based on natural language explanations and has low relevance to video modeling. \n 3) The annotation quality of explanations is important since the extracted concepts are from them. However, only two datasets with 5 and 10 classes are reported in this paper. It seems that the annotation cost of this pipeline is high in even normal-size datasets.\n 4) In the extraction phase of CoDEx, a fixed set of rules is used to extract \"raw concepts\". The motivation for using sentences as the concept is not clear. Will word-level concepts work? \n \n Minor: Both \"black-box\" and \"blackbox\" are shown in the paper, which is inconsistent.","rating": "1","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Hari"],"models": []}}
{"id": "Hyg__4H0KH","review": "This paper proposed a framework based on a mathematical tool of tropical geometry to characterize the decision boundary of neural networks. The analysis is applied to network pruning, lottery ticket hypothesis and adversarial attacks.\n \n I have some questions:\n \n Q1: What benefit does introducing tropical geometry brings in terms of theoretical analysis? Does using tropical geometry give us the theoretical results that traditional analysis can not give us? If so, what is it? I am trying to understand why the authors use this tool. The authors should be explicit in their motivation so that the readers are clear about the contribution of this paper. More specifically, from my perspective, tropical semiring, tropical polynomials and tropical rational functions all can be represented with the standard mathematical tools. Here they are just redefining several concepts.\n \n Q2: In \"Experiments on Tropical Pruning\", the authors mentioned \"we compare our tropical pruning approach against Class Blind (CB), Class Uniform (CU), and Class Distribution (CD) methods Han et al. (2015)\". What is Class Blind, Class Uniform and Class Distribution? There seems to be an error here \"Figure 5 shows the pruning comparison between our tropical approach ...\", i think Figure 5 should be Figure 4. \n \n Q3: In the adversarial attack part, is the authors proposing a new attack method? If so, then the authors should report the test accuracy under attack. Also, the experimental results should not be restricted to MNIST dataset. I am also not sure about the attack settings here, the authors said \"Instead of designing a sample noise Î· such that (x0 + Î·) belongs to a new decision region, one can instead fix x0 and perturb the network parameters to move the decision boundaries in a way that x0 appears in a new classification region.\". Why use this setting? Are there any intuitions? Since this is different from traditional adversarial attack terminology, the authors should stop using adversarial attacks as in \"tropical adversarial attacks\" because it is really misleading.\n \n \n \n ===================================================================\n Thanks the authors for the response. I still have two questions:\n \n Q1: The authors say that this theory provides a deeper understanding to Lottery Ticket Hypothesis (LTH). Then another paper \"Rethinking the Value of Network Pruning\" [1] suggests something different than LTH. [1] suggests that we do not need the initialization of large networks to train the pruned network from scratch to achieve high accuracy. Since the authors claim that their theory is related to LTH, then what would the proposed theory say about [1]?\n \n Q2: Since you redesign the task of adversarial attacks, I am still not convinced why this setting is interesting? The reason why people are interested in adversarial attacks is because it could happen during test time. What is the application of this setting? Why this new setting is important and worth studying? They are not clear to me. \n Also, as i wrote in my initial review, \"the authors should stop using \"adversarial attacks\" as in \"tropical adversarial attacks\" because it is really misleading.\". I hope the authors can address this concern, or otherwise new readers may also find this part difficult to understand.\n \n [1] Rethinking the Value of Network Pruning. Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, Trevor Darrell. ICLR 2019.","rating": "1","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Hari"],"models": []}}
{"id": "rJx0u1p19B","review": "This paper provides generalization bounds for permutation invariant neural networks where the learning problem is invariant to the permutation of input data. \n \n Unfortunately, the technical value of the content and its novelty is very limited since the proof reduces to a very basic argument that counts invariances (which is simply n! where n is the number of invariant dimensions) and uses a standard approach to give a generalization bound. Therefore, I don't think the results does not help us with better understanding of permutation invariant neural networks. \n \n Unfortunately, the paper has several typos and mistakes as well. Another non-technical issue is that apparently authors have removed the ICLR format and reduced margin to fit the paper in 10 pages which is against the spirit of page limit.\n \n ***********************************\n \n After author rebuttals:\n \n After reading authors' response and reading the proofs, I realize that the formal proof is not trivial and requires more work that I assumed. However, I do not understand how this work can improve our understanding of permutation invariant networks. Therefore, I think the contributions are not significant enough for publication and my evaluation remains the same.","rating": "2","sentences": [{"sentence_type": "2","sentence": " Unfortunately, the paper has several typos and mistakes as well. "},{"sentence_type": "2","sentence": "  Unfortunately, the technical value of the content and its novelty is very limited since the proof reduces to a very basic argument that counts invariances (which is simply n! where n is the number of invariant dimensions) and uses a standard approach to give a generalization bound. "}],"comments": null,"annotation_info": {"type": "Manual","humans": ["Hari"],"models": []}}
{"id": "4ylX6a3vTs","review": "Language needs harder tasks/benchmarks and this paper attempts to introduce one. The most important question to answer in the paper is is the problem difficult enough and unsolved to be a new challenge benchmark. I do not think this is answered by the experiments. The fact that a T5 base model doesn't do well is a good sign, and intuitively seems like a very hard problem. However, this particular baseline may not be well-suited to the problem since it tokenizes text into SentencePiece tokens, which makes it difficult to interpret character-length constraints given as input. To do this task properly, the model must learn the number of characters in each token, which presumably requires a lot of data, and may partially explain the poor performance. This limitation is not discussed in the paper. I'd like to see a baseline (preferably a pre-trained LM similar to T5) that takes in characters as input. It'd also be useful to see whether larger T5 models do better.\n \n The \"Curricular\" model is really just supervised pre-training on related datasets (american crosswords) with some data-augmentation. It is not curricular learning, nor is it particularly novel.\n \n In summary, there is not much in terms of new methodology, but the dataset is potentially interesting as new language benchmark. However, the experiments do not convince me that it is indeed hard enough to be one.\n \n \n Questions/clarifications:\n \n T5 prefix-lm is not the standard T5 model, which is encoder-decoder.\n Why did you use the T5 prefix-lm model instead of encoder-decoder?\n \n \n # Update after author response \n Again I think this dataset is potentially interesting, but I'd like to see character-based model baselines as this task involves counting characters in the output which SentencePiece tokenization severely obstructs. It'd be great to see results in a future paper using something like ByT5. If a good character-based model cannot solve this task, I would be convinced that this is a great new benchmark for NLP; in the absence of that it is hard to tell. At the very least, some of the points in the author response should be added to the discussion in the paper. I've updated my score assuming this is added.\n \n \"This suggests that T5 both can \"understand\" the length-enumeration part of the input and also that it learns the interaction between lengths of different tokens in a word with the length enumeration.\"\n - I'm sure it can but it must learn the length of tokens to properly do the task, which a model that decodes characters would have a much easier job doing.\n \n Thanks for clarifying that the model is in fact an encoder-decoder.","rating": "2","sentences": [{"sentence_type": "2","sentence": " \n The \"Curricular\" model is really just supervised pre-training on related datasets (american crosswords) with some data-augmentation. It is not curricular learning, nor is it particularly novel."}],"comments": null,"annotation_info": {"type": "Manual","humans": ["Hari"],"models": []}}
{"id": "r1gpwXPsYB","review": "The work involves modifiying gaussian conditional random fields to work for classification problems instead of regression problems. The main idea is to apply a bernoulli distribution on top of the regression values to convert them to work with binary classification problems. Two variations are discussed along with the inference and learning methodology. The inference can be done using numerical approximation and learning using variational methods and is still untracktable. Comparisons with other modeling strategies is done using experiments.\n \n The paper is incremental and doesn't really provide improvements to learning parameters (or at least there is no theory showing this in the paper). The experiments do not seem satisfactory as discussed below.\n a) Applying a bernoulli distribution on the output of the GCRF seems trivial. It is not very clear when the GCRFBCb model would be better than the GCRFBCnb. The learning procedure is untracktable and hard to follow on why this might provide better results.\n b) The datasets (music classification and gene classification) don't seem to be good datasets for structured predictions i.e. the interaction needed between the nodes is not clear. Since they are multilabel problems, one could have just modeled the system with N independent nodes or design a multinomial distribution instead of only for binary classification.\n c) There should be more thorough fine-tuning of other models, for e.g. in the ski lifts experiment, the CRF does much worse than logistic regression in the results. This is most likely because the parameters were not initialized properly using normal tricks like using logistic regression. Typically for truly structured problems, CRFs do better than their logistic regression counter parts. It is also not clear how the other models (CRF and SSVM) pairwise potentials were modeled.\n \n It would really help to make this paper stronger by showing the new modeling technique does better than CRFs (that are tuned properly) on better structured datasets. It would be good to have a discussion on when this model would do worse than the other structured models and why.","rating": "1","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Hari"],"models": []}}
{"id": "PzB74SHx7eK","review": "The authors define a quantum version of a binary neural network (all weights and activations are 0 or 1) by first defining a classical stochastic generalization and then upgrading the stochastic part to a quantum process. The calculations all appear to be correct, and to my knowledge this type of quantum neural network is novel. The quantum network is simulated on MNIST and Fashion MNIST. The quantum network shows improvements in test accuracy, but those improvements are attributed to increased parameterization. Thus the jury is still out on whether this is actually an interesting use-case for quantum computers, and it's not clear that this work will have any real impact on the field. For that reason, I cannot recommend this paper for acceptance.\n \n Pros:\n 1) Clearly explains the quantum theory in a way that should be understandable to a novice.\n 2) A novel design of quantum neural networks, to the best of my knowledge.\n 3) Makes an attempt to test the performance in simulation.\n \n Cons:\n 1) The authors make a number of claims about possible quantum advantage, but there is no reason to suspect quantum advantage based on the evidence presented. (Generally, one should expect that there is no quantum advantage for classical data processing unless clear evidence is given to the contrary.) It would be good to attempt to distinguish between improvements in performance due to overparameterization from improvements due to quantum properties, e.g., entanglement.\n 2) By the same token, is there any benefit from using the quantum deformed model over the classical stochastic model? More information about the stochastic model would be helpful.\n 3) The paper would benefit from a discussion of near-term vs long-term uses of a quantum computer (i.e., noisy vs error-corrected). The authors use phase estimation in their construction, which is typically an operation that is reserved for error-corrected quantum computers. The authors mention noisy intermediate-scale computers at one point, but it's not clear at all that this algorithm is applicable in that case. If it is, the authors should discuss the effects of noise on their results.\n \n \n Miscellaneous comments:\n 1) The authors cite Hooft 2016, but this should be 't Hooft. The 't part is included with the surname.\n 2) In the 4th paragraph of the introduction, \"restrict\" should be \"restricted\".\n 3) The authors state that Farhi and Neven 2018 restricted themselves to 4x4 images because of exponential time of simulations. I believe it's more likely that it was memory constraints of doing full quantum simulations that was important there.\n 4) in the second paragraph of section 2 there is a malformed citation that just says \"staines2012variational\".\n 5) In section 3.1, the authors state that a qubit is a normalized vector. A qubit more properly refers to the vector space, or to the set of unit vectors in that space, rather than an individual vector. The wording is a bit sloppy, since N qubits are properly associated with a vector space in the following line.\n 6) In the last paragraph of 3.2, the authors refer to the V_h subspace. V_h is not a subspace, but a factor (or subfactor). The distinction is important as it often confuses people.\n 7) In the first paragraph of 3.3, \"it's' should be \"its\".","rating": "1","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Hari"],"models": []}}
{"id": "BkSq8vBxG","review": "The authors report a number of experiments using off-the-shelf sentence embedding methods for performing extractive summarisation, using a number of simple methods for choosing the extracted sentences. Unfortunately the contribution is too minor, and the work too incremental, to be worthy of a place at a top-tier international conference such as ICLR. The overall presentation is also below the required standard. The work would be better suited for a focused summarisation workshop, where there would be more interest from the participants.\n \n Some of the statements motivating the work are questionable. I don't know if sentence vectors *in particular* have been especially successful in recent NLP (unless we count neural MT with attention as using \"sentence vectors\"). It's also not the case that the sentence reordering and text simplification problems have been solved, as is suggested on p.2. \n \n The most effective method is a simple greedy technique. I'm not sure I'd describe this as being \"based on fundamental principles of vector semantics\" (p.4).\n \n The citations often have the authors mentioned twice.\n \n The reference to \"making or breaking applications\" in the conclusion strikes me as premature to say the least.","rating": "2","sentences": [{"sentence_type": "2","sentence": " Unfortunately the contribution is too minor, and the work too incremental, to be worthy of a place at a top-tier international conference such as ICLR. "}],"comments": null,"annotation_info": {"type": "Manual","humans": ["Hari"],"models": []}}
{"id": "QnqwFETJPw","review": "Summary\n -------\n By combining a task-success classifier with the latent space of a tool-shape generative model, this paper shows that an activation-maximization approach can generate tool shapes which can succeed at particular tasks. \n \n Positives\n ---------\n The paper addresses the interesting topic of affordances, and how latent representations of tool shapes might be structured.\n \n The methods and experiments are clean and well motivated, and the writing is clear.\n \n Negatives\n ---------\n The specific contribution of the paper is difficult to see at first glance. The architecture is taken from previous work, as well as the method for maximizing activations.\n \n Although the task-unaware baseline is a reasonable one to compare against, I would like to see other comparisons to approaches present in the literature. For example, how do the supervised affordance learning approaches perform in the tool imagination task when combined with task-success prediction? Additionally, are other methods for latent disentangling sufficent to observe similar behavior?\n \n In the tool imagination task, it would be useful to see a couple of things from the task-unaware approach to make sure the conclusions are sound. First, how well does the activation-maximization step work? Is the model reaching the same level of feasibility for task aware and unaware versions? Second, what do the imagined tool trajectories look like for the task unaware approach? If they are also smooth and encode length, width, etc., it would be hard to claim that task-aware approaches are required for that type of encoding.\n \n Reasons for score\n -----------------\n The topic explored is interesting, and the experiments are simple and illustrative, but there are remaining questions about the baseline comparisons required to make strong conclusions.\n \n Post-rebuttal response\n -----------\n Thank you for the detailed rebuttal responses. The rebuttal suggests that there are not many other baselines against which to compare. If that is true, I would want to see much more detailed analysis of the comparisons offered in the paper. However, I am still missing details of the latent space generated by the task-unaware approach. I will leave my score as it is.","rating": "1","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Hari"],"models": []}}
{"id": "rJggDirgiN","review": "This paper analyzes three approaches to tackle the (re)scheduling problem of typical plans to be executed onboard a planetary rover. The main challenge to be solved in this work is the limited amount of computational resources to execute the scheduling process, as such process is supposed to occur on board. The presented analysis is therefore a study tailored on the actual computational capabilities of a rover-embedded scheduler, whose performance is necessarily limited.\n \n This work is a crisp example of scheduling technology applied to a real-world applicative context even though, as far as the reviewer understands, the study revolves around possible techniques tailored for the real Mars 2020 onboard scheduler but implemented on only a surrogate of the same scheduler. The analysis is therefore intended to provide just a validation of the proposed scheduling techniques. \n \n This work is certainly of great interest for the SPARK audience, and may generate a lively discussion ay the workshop. Also, the model of the applicative domain may potentially represent a very close approximation of the real-world scheduling instances that could be met by the real rover.\n \n Some perplexities arise only in the fact that the experimental conditions used in this work are still relatively simple, as the authors adopt some simplifying hypotheses (for instance, all guards are computed disregarding the data volume and considering time and energy only; moreover, the energy issues are reduced to temporal considerations only, as the consumed energy is directly proportional to the activity durations, therefore any constraint on energy is directly translated to a temporal constraint). The reviewer understands that this is an ongoing work, and that the obtained results are still preliminary. Many points in the analysis are still open, and the impression is that a significant amount of work is necessary to fill all the gaps mentioned by the authors. But the level on maturity of the presented material is suitable for a workshop. \n \n Below, some comments that may help the authors improve the contents/clarity of the paper.\n \n - Some of the subsections describing the various cases (e.g., Fixed Point Vs. Sol Wide) are awkwardly ordered, see for instance the \"Guarding for Energy\" section.\n \n - Section \"Sol Wide Minimum State of Charge Guard\": the authors claim that they are working on a better heuristic w.r.t. the one currently implemented. Truly, the current heuristic seems very costly to the reviewer, and hardly suitable for hard real-time utilization. Any comment?\n \n - Hoe would the approach fit when deadlines on activity end times are added to the scenario? (e.g., plans having to end before martian sunset). The reviewer understands that at the time being, unexpectedly longer durations are cut to the nominal durations, hence only \"shorter than expected\" durations are taken into account. \n \n - In the current scenario, are minimum battery requirements to keep all instrument \"warm\" in order to survive the freezing martian, satisfied?\n \n - To the best of the reviewer's understanding, one of the most important issues in this work is the decision about how often to reschedule, as well as the issue of managing reschedulings in the presence of committed activities. Especially related to the second point, this raises the general issue of system's latency. What are the rescheduling time constants associated to the proposed approach, relatively to the problem instances selected for the empirical analysis? \n \n - One last comment on the empirical analysis. The section is not very clear. It is sometimes hard to follow the textual description of the results based on the figures, and to check their correspondence to what is depicted in the same figures. For instance, some numbers are mentioned in the textual description that the figures do not show. \n \n Despite the approach analyzed in this work raises some perplexities on the real hard real-time applicability of the current implementation, the reviewer believes that the presented material is sufficiently interested to be accepted.","rating": "1","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Hari"],"models": []}}
{"id": "Pann-l2fWF","review": "Summary: Authors propose to replace the matrix-multiplication part of a convolutional layer with a differentiable random fern (as defined in à–zuysal et al. IEEE TPAMI 2009). It is shown that this method reduces by two the number of parameters, with respect to a standard CNN, and preserves almost the same performance in terms of classification accuracy.\n \n Remarks:\n 1- the paper is quite difficult to read and understand. Many concepts such as ferns, IM2COL, UFM and EmbeddingBag-Layer are not sufficiently explained in the paper. \n \n 2- A fern, as introduced by à–zuysal and colleagues, is a small set of binary tests that is used with a Semi-Naive Bayesian approach in a problem of classification. It's not clear how exactly ferns can replace matrix-multiplication. Authors should better explain this point.\n \n 3- Many choices are not well motivated or explained such as: the use of tanh, the offset s^k, the definition of w_u^k\n \n 4- Results seem interesting and it's a pity that the paper is not so clear. Authors probably need more space to better explain their algorithm and all related concepts.","rating": "2","sentences": [{"sentence_type": "2","sentence": "Results seem interesting and it's a pity that the paper is not so clear. "}],"comments": null,"annotation_info": {"type": "Manual","humans": ["Hari"],"models": []}}
{"id": "FiTVSZX4_v3","review": "Summary:\n \n - This paper focuses on compositional goal-conditioned MDPs. When we don't have high-quality data or prior information about the task of interest, the authors argue that current offline RL or curriculum learning methods fail to work. Instead, this paper proposes a novel approach to generate datasets demonstrating more complex behaviours in the environment based on the universal value function through iterative data expansion and policy-distillation steps.\n The paper empirically evaluates the quality of learned policy in a multi-objective block-stacking task. It shows this method learns policies that can solve more complex tasks and generalize to unseen tasks.\n \n Strength:\n - The paper proposes a novel progressive compositional data generation method using the universal value function. \n - It systematically evaluates the quality of the proposed method in a compositional goal-conditioned task and studies the contribution of different components of the proposed method in a set of ablation studies.\n - The paper is well-written and easy to follow.\n \n Weaknesses: \n - The paper discusses a broad range of problems initially, but as early as section 3, it becomes too focused on a specific environment. This restricts the applicability of the main contributions to only that particular task. For instance, section 4.1 should discuss the proper definition of primitive skills for a general environment, while section 4.2 should address how to define the restriction on newly sampled tasks in such an environment. The sections before section 5 should be task-independent to increase the paper's contribution beyond the limited scope of the specific task.\n - The ablation study conducted to verify if online RL algorithms can solve the problem should be more extensive. Firstly, starting from primitive skills before switching to the main task seems might induce a negative bias in the policy, did you try to train it on the evaluation task from scratch? Secondly, PPO was only trained with 137M timesteps while the proposed method is trained for much more time steps. The fine-tuning of PPO was done with a budget of 150M steps in each iteration and as vaguely mentioned in the paper the results were obtained after \"several iterations\". I would suggest reporting the performance of vanilla-PPO trained with the same number of environment transitions, at least as a baseline.\n - In Equation 1, for goal g* to have the right difficulty level (solvable $s_T \\rightarrow g*$ but not trivial $s_0 \\rightarrow g*$), the universal value function $v_{\\varphi}$ should be a close approximation of the true universal value function of current policy, and it needs to be approximated at every iteration of expansion-distillation. How is it learned efficiently? Do you have an approach to using collected data to learn it or is it based on online interaction with the environment? \n \n Suggestions for improvement:\n - In section 3, an example of what is T in the definition of MDP and how it changes through different rounds of task expansion can help to improve the clarity of the paper.\n - Although the paper aims to address the lack of a dataset containing rich behaviours, it still appears to be an issue in this method. As described in line 250-forward, having multiple-goal initial tasks is important for the success of this method. It would be helpful to show the sensitivity of this method to the quality/diversity of initial tasks.\n (on a related note, I would suggest adding the data of initial goals to Figure 2 as well.)\n - Figure 3 lacks a clear explanation of each image in its respective section.","rating": "1","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Hari"],"models": []}}
{"id": "Sh942wQpsy9","review": "The paper proposes methods to address churn in deep neural networks for classification, defined as the extent of disagreements in predictions of two models trained on the same data with the same algorithm. In addition to an existing measure of churn that is based on exact match of predicted classes, the paper introduces a soft measure of churn that measures disagreement by comparing the two models' class probability distributions. The paper proposes three regularization terms that can be added to the primary loss function used during training to reduce churn: two single-model regularization terms (based on cross entropy and KL divergence respectively) that encourage the model to output a more uneven probability distribution for an example, and a divergence-based term that is used by training two models simultaneously and that imposes a KL-divergence-based penalty that encourages the two models to output probability distributions that are as similar as possible. Experiments with ResNet architectures on CIFAR-10/100 and ImageNet indicate that the proposed approaches and their combination indeed reduce churn and do so to a larger extent than the two-model divergence-based approach applied in conjunction with cross-entropy that was proposed in work by Anil et al. in 2018 (although the improvement seems quite minor on ImageNet).\n \n My primary concern is that the paper does not evaluate the effect of the regularization terms on calibration of the probability estimates. This issues is briefly discussed in Section 5. In practice, it seems vastly more important to have well-calibrated probability estimates than no churn. The single-model regularization terms proposed in the paper seem to encourage the model to become overconfident. This will reduce churn but is clearly not desirable in most practical applications.\n \n Another concern is that the hyperparameters \\alpha and \\beta appear to be tuned by maximizing performance on the *test* data. This inflates the performance estimates on this data obtained for the proposed methods compared to the baseline, which does not have these hyperparameters to play with.\n \n I would also like to see results for churn when standard forms of regularization are included, such as L_2 or L_1 regularization. As these should also increase the stability of the learning process, it stands to reason that they will also reduce churn (particularly when the hyperparameters are tuned on the test data).\n \n The paper appears to claim that the ordering of the mini batches (over multiple runs!) has an effect on accuracy. This does not appear to make sense (assuming only random orders are considered).\n \n The paper eliminates churn due to data augmentation by removing data augmentation. Instead, the random number generator used for augmentation should be initialized in a deterministic manner.\n \n Given that the best-performing model is based on training two networks, the paper should also include results for two-member ensembles that are obtained by averaging the probability distributions across the two networks. If memory consumption and inference time are not critical, and this two-member ensemble turns out to reduce churn substantially, it would be a useful solution.\n \n Some small issues:\n \n \"disagreements between predictions of the two models independently trained by the same algorithm\" --- on the same training data???\n \n Table 2 is discussed before Table 1.\n \n Why is SChurn_1 denoted a percentage in Table 1?","rating": "1","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Hari"],"models": []}}
{"id": "S1luM8aHtN","review": "Summary\n The paper proposes a technique to transfer knowledge from trained models to task/domains/languages where such models cannot be trained due to the lack of training data. This is done in contrast to using only pretrained embeddings such as GloVe or ELMO which are trained in an unsupervised manner, and independently from any downstream task. \n The proposed approach assumes the existence of pre-trained models either for different tasks to the targeted one, or the same task but on different domains or languages. All these models are trained in a supervised manner where the last layer in the model is an output layer (before applying a Softmax). The authors project the feature representations that are generated from the pre-trained models using the Convex Combination method to unify and combine the features from all the models. Finally, they concatenate the resulting representation with either GloVe or ELMO. \n In their experiments on different tasks, domains, and languages they showed that the proposed approach achieves an increase in the F1-Score ranging between 5%-7% compared to only using the pretrained embeddings. \n \n Pros.\n 1. The experimental setting fits the workshop. Specifically, training a model on limited data where the results are on par with models trained on all the available data. \n 2. The proposed approach is sound. Mainly, where it does not require retraining models that were used for other tasks/domains/languages. \n 3. The experiments are designed to cover the different knowledge transfer settings: different tasks, the same task but different domains, and the same task but different languages. \n 4. Overall, the paper is clear and well-written, and the authors provided a good discussion on the results. Furthermore, the results are well-presented and easy to interpret. \n 5. From a reproducibility perspective, the authors provided all the experimental details in their paper. \n \n Cons.\n 1. In the Related Work section, it is not clear how the work under Supervised Transfer Learning is different from this work. \n 2. It is not clear how the GloVe representation was combined from the word level to the document/sentence level. If it was averaged for all the words, please mention that in the Experiments Setup section. \n \n Further comments.\n In addition to the proposed future work provided by the authors, I suggest the following:\n 1. In addition to only using pretrained embedding as a baseline, compare the performance of your model to the performance of SOTA of each task but with limited data. \n 2. Run the experiments on multiple subsets of the data (1k each) and report the average performance with SD. \n 3. For the cross-task setting: would adding more (models trained on different) tasks always have a positive effect? Would having a large number of tasks eventually lead to a generic representation which is similar to simply using ELMO again? \n 4. Minor formatting-related issues\n - Citation. Please note the difference between having the author name inside or outside the brackets: XX (2018) discussed ... / the model is provided in (XX 2019 \n - In Sec.3 Approach, the acronym SRL was used without providing the full name (Semantic Role Labelling is mentioned in the first line of the introduction).","rating": "0","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Hari"],"models": []}}
{"id": "YXLZnKRH9Hm","review": "3342 Identifying Treatment Effects Under Unobserved Confounding\n # Summary\n \n The authors propose a representation learning method for estimating causal effects in the presence of unobserved confounding when covariates that act as proxies for a latent confounder are available. The authors connect the problem to some recent results on the partial identifiability of VAE and non-linear ICA models. The authors lay out some conditions under which the causal effect may be identifiable and propose a VAE-based model, CFVAE, that enforces some of these conditions on the estimated model. They compare CFVAE to a previously proposed VAE algorithm, CEVAE, and other methods that are designed to work under unconfoundedness.\n \n # Feedback\n \n Identification of causal effects in the presence of proxy variables for unobserved confounders is an important but subtle problem. However, this means that the bar for making contributions in this area needs to be high. Because the conditions for identification can't be falsified in a particular application, making unclear statements about when the effects of interest are and are not identifiable is very important. Readers who misunderstand will only experience silent failures and make poor decisions as a result.\n \n Unfortunately, I don't believe this paper meets this bar of clarity. I think that the authors explore some interesting connections to recent work on identifiability in latent variable models, and understanding what these results imply for causal inference is important. In particular, the observation that one might estimate a decoder up to a _different_ affine transformation in the treated and control arms seems like a useful insight. But the results in the paper are incomplete, disorganized, and in some cases wrong. I'll list out a few general points here, then discuss some substantive issues with the paper's specific argument.\n \n ## General points about identifiability arguments\n \n ### Identifiability is not a property of the method\n \n Identifiability is a property of the _data generating process_ and not a property of the model being used to do estimation. If the causal effect of interest is not identifiable in the process that generated the data, then using an \"identifiable model\" to estimate the causal effect will not solve the problem.\n \n The exposition in the paper seems to argue that using the right model will make the causal effect identifiable. This may just be a matter of unclear writing, but this is a broader point of confusion in the ML community, so it's important that the authors be clear on this point.\n \n Here, I think it would make sense for the authors to state what their assumptions are about the data generating process, separately from the parameterization of their model. Reading the paper, the distinction between these two layers of assumptions was unclear.\n \n ### Relaxing identifying assumptions is not an option\n \n In the same vein, if there are assumptions that the authors need to make to eliminate identification failure modes, then showing that the model \"works\" when those assumptions are relaxed does not inspire confidence. Unlike standard model misspecification which can be detected and debugged based on observables, making the wrong identifying assumptions in a model can result in silent failure, where the model can fit the observed data perfectly, but return a causal effect estimate that converges to the wrong place, or doesn't converge at all.\n \n If one is able to relax the identifying assumptions and still see success in experiments, this means either (a) the assumptions were unnecessary, or (b) the experiments did not probe the method well enough.\n \n ### Assumptions needs to be stated clearly, with implications clearly highlighted\n \n When making identification arguments, assumptions play the role of eliminating equally plausible causal explanations of the observed data, until only the true one can remain (if the assumptions are true). These are not the kinds of assumptions that eliminate exotic corner cases; instead, they eliminate cases like the most obvious explanation for the observed data like the absence of unobserved confounding. Bounding arguments like the Manski and Kallus et al papers that are cited in the introduction construct the full set of causal explanations that identifying assumptions must narrow down to a point.\n \n All of this is to say that clearly stating assumptions, and the cases they eliminate, is essential for any identification argument. In the paper as it is written now, many assumptions are made implicitly or in passing, and it is unclear which assumptions are made for illustration (e.g., the noise on the outcome going to zero) and which assumptions are essential for the argument in general. The assumptions are always framed as \"mild\" and do not highlight situations that the assumptions eliminate (i.e., in which cases they would fail to hold).\n \n ## Specific Concerns\n \n ### Balancing covariate implies that the naive estimator \"just works\"\n \n The primary identifiability result of the paper involves the assumption that the observed covariates are \"balancing covariates\", satisfying t \\indep z | x. The authors argue that this is a weaker condition than requiring that x satisfy unconfoundedness. This may be true, but the gap between the two assumptions is, at most, a set of knife-edge violations of faithfulness. In terms of estimating causal effects, simply doing the standard covariate with x and ignoring z would give the right answer.\n \n This can be shown in two ways. First, graphically, t \\indep z | x implies that there is no backdoor path through z from t to y when you condition on z. So z doesn't induce any non-causal association between y and t. Secondly, using the standard adjustment formula:\n \n \\mu_t(x) = E[ E[Y | X = x, T = t, Z = z] ]\n = \\int_z E[Y | X = x, T = t, Z = z] p(z | x) dz\n = \\int_z E[Y | X = x, T = t, Z = z] p(z | x, t) dz (using the balancing covariate property)\n = E[Y | X = x, T = t]\n \n In particular, the naive regression function E[Y | X = x, T = t] only fails in cases where p(z | x, t) \\neq p(z | x); i.e., when the distribution of the latent variable is different in the two observed treatment arms even after conditioning on x. The balancing covariate property eliminates this possibility.\n \n This also means that the VAE model specified can only generate data where the latent variable z does not introduce confounding.\n \n ## Other Concerns\n \n  * The adjustment formula in equation (2) is wrong. The last integral should be with respect to p(z | x), not p(z | x, t) (see the argument above).\n \n  * There is a substantive difference between estimating individual level causal effects given the observed outcome (a counterfactual query) versus estimating the CATE. The authors do divide this into \"pre-treatment\" and \"post-treatment prediction\", but the counterfactual query presents additional identification questions. In particular, whether the reported expectation is correct depends on Cov(y(1), y(0) | z, t), which is never observable. None of the identifying assumptions in the paper make any arguments about this quantity, so the models in the paper are making implicit strong assumptions here.\n \n  * The f^{-1}(y) notation in the paper is very unclear in the case that there is actually outcome noise for y. The function f relates the latent z to the expectation of y, not y itself. When y includes independent noise, the distribution of f^{-1}(y) does not yield the marginal distribution of z; you need to deconvolve the independent noise in y, which is non-trivial.","rating": "1","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Hari"],"models": []}}
{"id": "S1gt14y3YS","review": "I have read the author response. Thank you for responding to my questions.\n \n This paper aims to predict typical \"common sense\" values of quantities using word embeddings. It includes the construction of a data set and some experiments with regression models. The general direction of this work is worthy of study, but the paper needs additional justification for its task, better discussion of recent related work, and more development of its regression models.\n \n The work starts by describing the construction of interesting crowdsourced data sets that include people's estimates of typical quantities, what they would consider to be low or high values for a given object in given units e.g. the temperature of a hot spring or the height of a giraffe. Overall, the data sets are interesting but are not especially large (2300 total [low, high] pairs of 230 different quantities). Further, the particular task formulation here needs more justification. I think most of us would agree that common sense is a critical AI challenge, and that the question of whether embeddings reflect typical quantities is important. But, in a paper where the data set is considered a primary contribution, I would expect more justification for exactly how this task is formulated, and which objects and units were selected. As one example, why ask about \"large\" and \"small\" values rather than something with more precise semantics (like the 10th and 90th percentile, for example)? I also felt that the introduction could be improved to provide more convincing motivation. E.g., the first paragraph only says that humans apply different adjectives (like \"hefty\" and \"cheap\") to different things depending on their numerical attributes (weight, cost), but does not argue why teaching AI systems to use those adjectives is a priority.\n \n Regarding related work, the paper is missing a discussion of several relevant papers that use embeddings to obtain relative comparisons or estimates of commonsense properties of objects, including:\n \n Forbes, Maxwell, and Yejin Choi. \"Verb physics: Relative physical knowledge of actions and objects.\" ACL 2017\n \n Yang, Yiben, et al. \"Extracting commonsense properties from embeddings with limited human guidance.\" ACL 2018\n \n Elazar, Yanai, et al. \"How Large Are Lions? Inducing Distributions over Quantitative Attributes.\" ACL 2019\n \n The paper then presents the performance of some regression models. These models are standard existing techniques, and given the relatively low performance I would have liked more development of the models and more analysis of the performance. For a conference like ICLR I would expect to see a more thorough exploration and analysis of possible models for the task. Looking at more powerful neural regressors (perhaps using contextual embeddings rather than just fixed word embeddings) might be one option. Offering an explanation for why ARD seems to work better than the other approaches would be helpful.\n \n Minor: In Table 3, the way that small and large are interleaved makes it hard to compare systems, I think presenting all the small results together, and large results together may help. In Figure 2, it would be helpful to see the histogram for size-large within the same plots here, so we could see how far apart they are.\n \n \"Because Skip-gram has to handle more words to predict words, we assume Skip-gram will obtain more information about numerical values.\"\n -- I didn't understand what you meant about skip-gram having to \"handle more words to predict words.\" Also, I did not understand how this entails that skip-gram would obtain more info about numerical values.","rating": "1","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Hari"],"models": []}}
{"id": "BJgTv4QbtV","review": "This paper addresses the problem of limited data in rumor detection. They augment data by identifying unlabeled data as paraphrases of labeled rumors based on semantic similarity. They build a rumor detection model by fine-tuning a pretrained language model.\n \n I recognize space is limited, but a brief explanation of the rumor detection task and specifics about the class imbalance would help.\n \n Preprocessing as described removes critical meta information about whether the tweet is citing a particular source (url/rt). I'm skeptical that removing this information is necessary to build a model.\n \n I would like to see some exploration of whether sentence cosine similarity is actually a good metric for semantic similarity. What properties are captured by cosine similarity?\n \n Does this manner of augmenting data create bias towards detecting the same sort of rumors as are in the corpus? That is, will topic be relied on more than other markers of credibility? Perhaps holding out specific events from augmented training data would be a good way to test.\n \n The models that serve as a test bed show a sound methodology, and this paper strikes me as a solid work in progress.","rating": "1","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Hari"],"models": []}}
{"id": "rkxclIjPjE","review": "This paper introduces a very interesting and compelling scheduling problem where privacy is the center theme where information from involved entities need to be hidden and should not be revealed through the final scheduling solution. The authors demonstrated the concept through concrete application of scheduling coalition operations of delivering aid from multiple aid providers to a single aid recipient by ships.\n \n The paper is clearly written with good examples to illustrate different concepts. The scheduling problem setup is novel with lots of challenges. The detailed discussion on the solution approach through MPC and how to use QIF framework to reason about the level of private data that can be inferred, with clear examples, is interesting. Given the novelty of the problem setup and the concrete application scenario, just introducing this type of scheduling problem to the planning and scheduling community to me is a strong enough reason to accept the paper.\n \n The current weak point of the paper is the technical contribution from the planning and scheduling algorithm and evaluation. The actual scheduling algorithm proposed at the moment is a somewhat simple greedy algorithm targeting the main objective function of optimizing for load-balancing (btw, are other popular scheduling objective functions such as makespan etc have been considered by the users?). While there are mentioning here and there in the paper about different concrete problem setup for different tests of different components, there is currently no systematic evaluation on the set of instances of this domain/application. If possible, I would like to see description of the instances that this approach has been implemented on, some analysis of the results to see how the greedy algorithm performed (easy/hard, if backtrackings are required during greedy search, solution quality etc) based on different requirements on QIF; also, the user feedback on the solutions provided by this approach.","rating": "0","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Hari"],"models": []}}
{"id": "NQRePxFH5Ow","review": "This paper presents a new type of CBM, called GlanceNets, that addresses two limitations of existing models: interpretability and concept leakage. GlanceNets build on existing CBMs by leveraging (1) concept-level supervision (cf. CBNMs) to achieve alignment (disentanglement?) between the generative and learned representations, thereby increasing interpretability; and (2) open-set recognition to deal with domain shift, thereby reducing concept leakage. These are implemented on top of a VAE architecture and outperform CBNMs on several datasets. \n \n The ideas presented in this paper are very interesting and make novel steps towards addressing significant challenges with CBMs. The experiments and results are also well presented. I think this would be a great paper for the workshop but feel that some of the notions used could be described more clearly in the text to improve the work:\n - How does alignment relate to disentanglement? Why/how is it better at preserving semantics?\n - How is alignment measured, i.e. what are the y-axes in Figure 1?\n - How does open-set recognition address the problem of concept leakage \"failing to provide well-defined semantics\", i.e. where does it relate to semantics?","rating": "0","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Hari"],"models": []}}
{"id": "452_6iuYOUs","review": "The paper proposes an algorithm called LAWA (LAtest Weight Averaging) which keeps a separate model that is an average of the parameters of the latest $k$ model checkpoints. This LAWA model provides a significantly better predictive performance in the middle of the training process.\n \n The experiments are presented clearly and show that LAWA provides better predictions in the middle of training compared to the baselines.\n \n Feedback:\n - Perhaps it would be better to not talk about training speed improvements or that LAWA \"speeds up convergence\" (caption of Figure 1). LAWA doesn't reduce the time it takes these models to converge. However, LAWA improves the any-time performance of a model (while still in training). This shouldn't diminish the contributions of the papers, but I think would be more precise.","rating": "0","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Hari"],"models": []}}
{"id": "Skx-6E86tr","review": "Authors introduce a novel algorithm based on multi-objective optimization to jointly position embedded documents and graph nodes in a 2-d landscape. \n \n First, the multi-objective optimization used in this paper is quite confusing. Since the overall error defined in (5) is a weighted summation of three objectives, it is not a real multi-objective optimization where multiple objectives are optimized simultaneously. \n \n Authors define a hypergraph to jointly model documents and entities. However, there lacks the novelty on the objectives defined in this paper. The optimization using gradient descent is also trivial. \n \n It is hard to understand why equations (2) and (3) share the same objectives but the two sets of samples have totally different meanings. Moreover, both objectives are summed uniformly as shown in the experimental setting. Please explain it in details.\n \n For the quantitative evaluation, Table 1 shows the percentage of papers for each community per cluster. There are many numbers for a single clustering problem. It might be preferred to use a single measurement such as normalized mutual information or accuracy for clustering evaluation. In this way, it is easy to judge the compared methods.","rating": "1","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Hari"],"models": []}}
{"id": "HyxkvnHPnN","review": "This short paper provides a rather interesting analysis and discussion over insights after the organization of several ICKEPS editions. \n \n The paper identify pros and cons of past editions and propose some possible options for the future with the aim of increasing the number of participants in the competition and, hopefully, leverage its results for advancing the state of the art.\n \n The paper seems to me very well suited for KEPS as it can really foster an highly interesting discussion among its potential attendees.\n \n \n KE (applied to P&S) is a rather wide field and it seems to me hard to define \"bounds\" within which a tool/solution/methodology can be evaluated in a quantitative way. As discussed by authors, going in that direction entails several issues that may lead to not significant challenges (e.g., using toy problems) or very difficult assessments.\n Also restricting the competition to PDDL does not seem an exciting perspective to me.\n \n My personal opinion is that the organizers of future ICKEPS editions should look back at the first editions considering sort of open challenges tracks in which researchers can provide wider kind of solutions and be more free to investigate actual challenges. I would then encourage to look for something really interesting (and coming from real applications) rather than building artificial problems. Indeed, the risk is to build wonderful KE tools capable of dealing artificial problems/situations but that nobody cares about...or having something similar to IPC, i.e., planners very good/effective in solving (toy) planning problems but with (many) difficulties in addressing real world scenarios.","rating": "1","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Hari"],"models": []}}
{"id": "BkgFFnt-h4","review": "The paper addresses an important issue - accuracy of domain models. In particular, the paper utilizes Regression trees for adjusting values of numerical fluents such that they accurately capture real-world situation. The approach is evaluated in an Urban Traffic Control domain, where flows of vehicles have to be accurately represented so the planner can better reason about them.\n \n I like the direction of the research as accuracy of domain models is an important factor determining usefulness and reliability of planning approaches, i.e., plans are likely to be executable and better in terms of quality (when executed). The decrease of error is demonstrated in experimental evaluation.\n \n I do not have much to criticize regarding paper content, its presentation and technical quality. Perhaps, the notion of the process could be formally introduced to make the paper more accessible for researchers outside the planning community. Also, a paper \"Distributed Planning and Model Learning for Urban Traffic Control\" by Pozanco, Fernandez and Borrajo, presented on KEPS 2018, should be mentioned as it has similar aims (combining learning and planning in Urban Traffic Control).","rating": "0","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Hari"],"models": []}}
{"id": "-rL79PmmuYP","review": "The paper proposes a transformer-based model, which is light-weight and much faster to train. The results look promising and the text is reasonably clear.","rating": "1","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Hari"],"models": []}}
{"id": "iNrCAsOsQdE","review": "# Summary\nThis paper aims to investigate the use of generated data in reinforcement learning for allowing RL to be more data efficient: using smaller datasets in the offline setting, or with more sample efficiency in the online setting. The paper proposes using diffusion models for modeling existing data, which are then used to generate synthetic data, adding, or replacing the training data for any off-policy RL algorithm, without making any algorithmic changes.  In the offline setting, on the D4RL benchmark, model-free offline RL algorithms can retain performance and sometimes benefit from training on synthetic data,  or allow for reducing the volume of real data. In the online setting, augmenting RL algorithm(s) (SAC) with offline data is shown to increase sample efficiency. They also show their method can outperform explicit data augmentation or algorithms specifically designed for data efficiency (REDQ).  \n\n#  Strengths and Weaknesses\n## Strengths\n- The paper is clear to follow, and overall goals are well-motivated. Accelerating RL through upsampling prior data is also relevant.   \n- While diffusion models have begun to be explored in RL in offline reward-conditioned settings, this seems to be the first work that has used diffusion generative models for increasing efficiency of existing model-free RL algorithms, and in the online setting. \n- Experiments illustrating the quality of synthetic distributions, in comparison to augmentation, and state marginal distributions are also convincing towards diffusion model’s ability to capture dataset statistical. \n\n## Weaknesses\n- Minor: while the reported wall-clock times of SynthER and REDQ are appreciated, it would also be useful to have the wall-clock time of your implementation of SAC. This would be useful to help better understand the wall-clock versus sample-efficiency tradeoff.   \n## Questions\n- In the online RL experiment, I am confused by the choice to upsample every 10K real samples to 200K (20x scaling), but only use an UTD of 8? Is this ratio of 8, in reference to the original 10K samples, or after upsampling (200k)?","rating": "0","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Navya"],"models": []}}
{"id": "HyldHxls2X","review": "This paper presents a method for training neural networks where an efficient sparse/compressed representation is enforced throughout the training process, as opposed to starting with are large model and pruning down to a smaller size.  For this purpose a dynamic sparse reparameterization heuristic is proposed and validated using data from MNIST, CIFAR-10, and ImageNet.\n\nMy concerns with this work in its present form are two-fold.  First, from a novelty standpoint, the proposed pipeline can largely be viewed as introducing a couple heuristic modifications to the SET procedure from reference (Mocanu, et al., 2018), e.g., substituting an approximate threshold instead of sorting for removing weights, changing how new weights are redistributed, etc.  The considerable similarity was pointed out by anonymous commenters and I believe somewhat understated by the submission.  Regardless, even if practically effective, these changes seem more like reasonable engineering decisions to improve the speed/performance rather than research contributions that provide any real insights.  Moreover, there is no attendant analysis regarding convergence and/or stability of what is otherwise a sequence of iterates untethered to a specific energy function being minimized.\n\nOf course all of this could potentially be overcome with a compelling series of experiments demonstrating the unequivocal utility of the proposed modifications.  But it is here that unfortunately the paper falls well short.  Despite its close kinship with SET, there are surprisingly no comparisons presented whatsoever.  Likewise only a single footnote mentions comparative results with DeepR (Bellec et al., 2017), which represents another related dynamic reparameterization method.  In a follow up response to anonymous public comments, some new tests using CIFAR-10 data are presented, but to me, proper evaluation requires full experimental details/settings and another round of review.\n\nMoreover, the improvement over SET in these new results, e.g., from a 93.42 to 93.68 accuracy rate at 0.9 sparsity level, seems quite modest.  Note that the proposed pipeline has a wide range of tuning hyperparameters (occupying a nearly page-sized Table 3 in the Appendix), and depending on these settings relative to SET, one could easily envision this sort of minor difference evaporating completely.  But again, this is why I strongly believe that another round of review with detailed comparisons to SET and DeepR is needed.\n\nBeyond this, the paper repeatedly mentions significant improvement over \"start-of-the-art sparse compression methods.\" But this claim is completely unsupported, because all the tables and figures only report results from a single existing compression baseline, namely, the pruning method from (Zhu and Gupta, 2017) which is ultimately based on (Han et al., 2015).  But just in the last year alone there have been countless compression papers published in the top ML and CV conferences, and it is by no means established that the pruning heuristic from (Zhu and Gupta, 2017) is state-of-the-art.\n\nNote also that reported results can be quite deceiving on the surface, because unless the network structure, data augmentation, and other experimental design details are exactly the same, specific numbers cannot be directly transferred across papers.  Additionally, numerous published results involve pruning at the activation level rather than specific weights.  This definitively sacrifices the overall compression rate/model size to achieve structured pruning that is more naturally advantageous to implementation in practical hardware (e.g., reducing FLOPs, run-time memory, etc.).  One quick example is Luo et al., \"ThiNet: A Filter Level Pruning Method for Deep Neural Network Compression,\" ICCV 2017, but there are many many others.\n\nAnd as a final critique of the empirical section, why not report the full computational cost of training the proposed model relative to others?  For an engineered algorithmic proposal emphasizing training efficiency, this seems like an essential component.\n\n\nIn aggregate then, my feeling is that while the proposed pipeline may eventually prove to be practically useful, presently this paper does not contain a sufficient aggregation of novel research contribution and empirical validation.\n\nOther comments:\n\n- In Table 2, what is the baseline accuracy with no pruning?\n\n- Can this method be easily extended to prune entire filters/activations?","rating": "2","sentences": [{"sentence_type": "2","sentence": "Regardless, even if practically effective, these changes seem more like reasonable engineering decisions to improve the speed/performance rather than research contributions that provide any real insights."},{"sentence_type": "2","sentence": "Despite its close kinship with SET, there are surprisingly no comparisons presented whatsoever."}],"comments": null,"annotation_info": {"type": "Manual","humans": ["Navya"],"models": []}}
{"id": "Bkx1TgsDKN","review": "This paper proposes a structured convolution operator to model deformations of local regions of an image. The deformation field is parameterized by a Gaussian function. The advantage of this approach is that it significantly reduced the number of parameters. The result on image segmentation shows that it can achieve good accuracy and is more efficient. \n\nThe reviewer is unable to justify the advantage of the idea in the regime of small data samples.  \nThe paper is somewhere hard to follow,  due to the imprecise expression like “learned end-to-end”, “Static Composition”, etc. Is it to learn both parameters theta and Sigma, or just Sigma, or just theta? The result in Table 1 would be more clear if these were made precise. The formula (1), is it correct or not? Since convolution with g_Sigma involves bi-linear interpolation, the convolution does not seem to be done on the grid of I and f_theta. \n\nOverall, the idea is interesting, but the advantage of the approach to address small-sample problem is not conclusive enough. The experiments would be more convincing if it could be tested on other problems such as classification, detection.","rating": "1","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Navya"],"models": []}}
{"id": "BJxOUSBxnE","review": "The paper presents an approach to generating explanations online through model reconciliation. The smallest reconciliation that explains the plan prefix is given. This allows the user to understand the completed behavior, without becoming overloaded by a large explanation.\n\nThe approach is straight-forwardly explained and interesting. The main weakness of the paper is the motivation, which is not very clear from the motivating example alone. The evaluation setup is not fully explained, but could serve to better explain how the approach is intended to aid the user. Such an example, showing how the user interacts with the plan and the system, highlighting the problems with offline explanation, would definitely strengthen the paper.\n\nAbout the motivating example: rather than explain the plan, and increase the Emma's understanding about what is going to happen, the approach instead seems to hide information from Emma until it is impossible for her to backtrack and alter the plan. I feel that in this example, the lack of clarity on what is planned should lead to more confusion and chance for failure, rather than less. For example, why did Emma not buy food on the way to the study session without telling Mark? Although she did not have an explicit question about the plan, it was clear (to Mark) that she had an assumption that he knew to be false. The motivations for the approach is stated to be that the explanation is more easily understood if broken into multiple parts. The reason for bringing explanations online in the example is instead that the plan becomes acceptable (rather than understandable).\n\nIt is not clear to me from this example why online explanations reduce the frustration and mental demand of Emma. I'm not convinced (by this example) that offline explanations are worse. In fact, the same approach described in this paper could be applied as offline explanation, in which the explanation is given in parts to the user in order to reduce mental demand, without introducing any temporal demand or early commitment. Can you provide some examples in which online explanation is necessary for the user to understand a plan (before it is already executed)?\n\nThe algorithm is not completely clear to me. My understanding from the text is a set of changes reconciling M^R and M^H is found (\\lambda). Then, the maximum set of f\\in\\lambda is found such that the plan prefixes of M^R and M^R minus the maximum set remain the same. Then, the complement of this set is the minimum set of changes required to explain the prefix.\n- The set is found by breadth-first search, however algorithm 1 appears to add the changes to lambda in order and inside a single loop until the prefix remains unchanged. Which is it?\n- Algorithm 1 also returns the set \\lamdba_max as e_k, where instead should it not return the complement of this set?\n\nWith only 11/14 responses, it is hard to tell if there is a significant difference in any of the results shown in figure 3. The authors do state that no significant difference was observed due to insufficient data points collected. However, despite this, I would be interested to see more extensive results from this same experiment.\nGiven this:\n- How was the experiment carried out? Where the users controlling the rover, or only observing the rover? If they had some control, how was their plan and the AI plan used in the simulation?\n- Was there any temporal element to the simulation? If so, how long did actions last? Where there any time limits?\n- With MCE, how long (offline) were users given to understand the explanation?\n\nExpanding more on the last two points. It seems to me that the drawback to online explanation that is not mentioned in the paper is the additional time it takes to complete online. In contrast, a large offline explanation might be difficult to parse completely, but time can be devoted to understanding the explanation before confirming any plan.\n\n- Taking the explanations online much reduces the time available to parse the explanation. A fair test would be to compare online explanations against MCE, in which the user is given offline time to understand the explanation. The amount of time allowed offline depends upon the domain. Perhaps a motivating example would be a domain/scenario in which there is little time for a human user to understand and adjust plans before beginning execution?\n\n- It appears that the user does not have a chance to confirm the plan and the purpose of the explanation is to justify the robot's already completed behavior, not to confer understanding the long-term effects of current choices. It would be more interesting to see a domain in which there is the possibility of dead-ends and failure, or plan cost. If the user is working with the robot, then an effective explanation should allow the user to obtain a better cost, or complete the task more reliably.\n\n- The discussion of the evaluation focuses on the mental demand column, but does not mention the additional temporal demand or additional effort. I would say that the difference is also not significant, but that the summary in the abstract and conclusions are a bit one-sided.","rating": "0","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Navya"],"models": []}}
{"id": "DxyFZyrUVVK","review": "Overall I find the paper good. They have multiple tasks to learn about the implicit and explicit object properties. They show the gap that exists currently between learning to do descriptive tasks from visual features and to do counterfactual or planning tasks using implicit properties. They have done a great work to create scenarios to learn these properties. However, I believe, the range of values for these implicit features should have been more which could have made this dataset even more useful.\n\n# Pros\n- it's a nice benchmarking dataset with a varied range of scenarios for counterfactual reasoning. \n- the collection of tasks is quite fascinating. the way they re-simulate and do iterative performance evaluation for planning tasks was quite good.\n- one very interesting is that the only way to estimate mass is through subsequent collisions since object are initialised with fixed velocity. this way they make sure the model doesn't cheat and learn about mass through momentums.\n\n\n# Cons\n- it was a bit hard to differentiate colours in the video. that could have been made better. I believe this might be a problem during model training since most same-shaped objects looked too dark and of similar colour. \n- there are only two mass values for the objects. 2 and 14. I understand this might have been done to avoid the exponential increase in the possible state spaces. However, having more values would have been nicer.\n- similar opinion for friction coefficients","rating": "0","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Navya"],"models": []}}
{"id": "EhpG9Hiqor","review": "The authors implement use ENAS to classify benign and malignant breast lesions. It is an interesting approach that reaches an accuracy of 89.3%. The authors compare the ENAS results to AlexNet and CNN3 and show that their ENAS models give the highest performance. From the results it seems that the AlexNet just does not converge at all. The CNN3 network shows an accuracy of 78.1%, but it is unclear how these networks were trained. \nTwo small points:\n(1) The authors mention that the input size is rescaled, but the original image size (in pixels and mm) is not mentioned and it is unclear if the acquired images were made with a preset, or that the sonographer was able to adjust zoom, gain etc.\n(2) The authors augment the images using rotation of 90, 180 and 270 degrees, but this seems invalid since ultrasound images contain shadows which are always directed away from the transducer (so downwards).","rating": "1","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Navya"],"models": []}}
{"id": "YM69dKGw_Xv","review": "### Summary\nThis paper introduces EinStein VI: a lightweight composable library for Stein Variational Inference (Stein VI). The library is built on top of NumPyro and can take advantage of many of NumPyro's capabilities. It supports recent techniques associated with Stein VI, as well as novel features. The paper provides examples of using the EinStein VI library on different probabilistic models. \n\n### Strengths\nI'm not aware of other PPLs that support Stein Variational Inference. EinStein VI can provide an easier way to compare different Stein VI algorithms, and make research in the area easily reproducible.  \n\n### Concerns \nThe paper states that it provides examples that demonstrate that EinStein VI's interface is easy-to-use and performs well on pathological examples and realistic models. While it is true that there are several examples described, in my opinion there are not enough details to support the claims that EinStein VI is easy to use and performs well. A concrete comparison between EinStein VI and other methods is missing. It would have been helpful to have, for example, some concrete numbers (e.g. time taken to do inference, posterior predictive checks, posterior mean convergence plots, etc) that showcase why it is useful to use Stein VI for those examples, as opposed to other, already existing methods. \n\nAnother concern is that it is difficult to judge from the paper what the difference to (standard) NumPyro is. There is only a high-level explanation of the examples in the paper, so it's hard to imagine what the actual code looks like. Most importantly, I would have liked to see a comparison between EinStein VI code and what the code would have looked like without EinStein VI. \n\n### Reasons for score\nUnfortunately, there is not enough to go on in this paper, which is why I recommend reject. There is no strong evidence to support either the usability of the system (through elaborate examples and contrasting EinStein VI to other systems) or its performance (through experiments). This paper will be much stronger, and will have a better chance of reaching more people, if it includes either 1) more elaborate code examples that demonstrate that using EinStein is indeed better and easier than vanilla NumPyro, or 2) experiments comparing different Stein VI techniques to other inference algorithms, as evidence that a dedicated Stein VI library is indeed empowering our inference toolkit.\n\nHowever, I do appreciate that writing a paper about tools / libraries is difficult, as the contribution of tools is typically a longer-term improvement in the workflow of developing new methods and techniques. I am open to increasing my score during rebuttal, depending on the answers of the questions listed below.\n\n### Questions for the authors\nWhy has Stein VI not been implemented in PPL systems previously? Is it a matter of timing, or is there something particularly challenging about integrating Stein VI into a PPL? \n\nThe paper mentions \"compositionality\" several times. I was a little confused about what you mean by that: can you explain, perhaps with an example? \n\nThe paper mentions novel features (second to last paragraph page 8): can you elaborate?\n\nThe paper shows an example of using NeuTra in combination with Stein VI. Can you elaborate on the kind of problems that NeuTra won't be able to handle on its own? What about more lightweight approaches that can be applied in the context of probabilistic programming, such as \"Automatic Reparameterisation of Probabilistic Programs\" (Gorinova, Maria I., Dave Moore, and Matthew D. Hoffman. ICML 2020)? When will we see benefits of *both* applying a reparameterization that improves the posterior geometry, *and* using a more sophisticated inference algorithm like Stein VI?\n\n### Suggestions for improvement and typos that have not affected the score I gave to the paper\nPerhaps the most important change that would improve the paper is adding more concrete examples that would showcase the importance of using EinStein VI as opposed to simply NumPyro / other libraries. It would be nice to see a model where Stein VI gives us better inference results than a range of other algorithms / techniques and compare the code to what the user would have to write otherwise to achieve the same results. The examples of composing Stein VI with reparameterization / marginalization in NumPyro can be improved by comparing the results to Stein VI without reparameterization / marginalization and to other inference algorithms with reparameterization / marginalization. \n\nTypos:\n* last line of the abstract should be 500 000 as opposed to 500.000.\n* URL in footnote 3 does not lead to the correct page","rating": "1","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Navya"],"models": []}}
{"id": "Skx2F789YH","review": "Review: This paper considers the problem of dropping neurons from a neural network.  In the case where this is done randomly, this corresponds to the widely studied dropout algorithm.  If the goal is to become robust to randomly dropped neurons during evaluation, then it seems sufficient to just train with dropout (there is also a gaussian approximation to dropout using the central limit theorem called \"fast dropout\").  \n\nI think there are two directions I find interesting that are touched on by this paper.  One is the idea of dropping neurons as an adversarial attack, which I think has been studied empirically but not theoretically (to my knowledge).  However then it would be important to specify the budget of the attack - how many neurons can they remove and how precisely can they pick which neuron to remove?  Another would be studying the conditions for dropout to be useful as a regularizer (and not underfit), which are again somewhat understood experimentally but could deserve a more theoretical treatment.  \n\nHowever I don't think this paper solved a sufficiently clear problem and the motivation is somewhat confusing to me, especially when it seems like an analysis of dropout, and that isn't even mentioned until the 7th page.  \n\nNotes: \n  -Paper considers loss of function from loss of a few neurons.  \n\n  -Idea is to study more rigorously the fault tolerance of neural networks to losing a subset of the neurons.  \n\n  -In terms of impact of the work, one thing to consider is that even if a normally or arbitrarily trained network doesn't have perfect fault tolerance to dropping neurons, a neural network *trained* with dropping networks could learn hidden states which become more fault tolerant.  \n\n\nMinor Comments: \n  -I'm a bit unhappy with the argument about the brain losing neurons unless it has better referencing from neuroscience.  I imagine it's true in general but I wouldn't be surprised if some neurons were really essential.  For example squid have a few giant neurons that control propulsion.  It's just the first sentence so maybe I'm nitpicking.  \n\n  -  It also seems weird that the opening of the paper doesn't give more attention to dropout, since it's a well known regularizer and seems rather closely related conceptually.  \n\n  - In the intro it says neuromorphic hardware would pass information at the speed of light.  Is this really true?  My understanding is neuromorphic hardware would still generally use electrical or chemical signals but not pass things at the speed of light.  \n\n  - The citation format is not valid for ICLR.","rating": "2","sentences": [{"sentence_type": "2","sentence": "Is this really true?"}],"comments": null,"annotation_info": {"type": "Manual","humans": ["Navya"],"models": []}}
{"id": "JPOa379RtJC","review": "This work proposed a multi-modal relational neural process to provide a predictive distribution. Meanwhile, MoG is introduced to integrate information from different modalities. Generally, this paper is easy to read.\n\nI have the following concerns about the paper:\n\n\n   1. A literature review is lacking, especially for Gaussian processes-based multi-modal / multi-view classification methods. The discussion of the multi-view Gaussian process method should be conducted. The following papers are very relevant to this paper.\n\nMultiview learning with variational mixtures of Gaussian processes. Knowl. Based Syst. 2020\n\nMulti-view representation learning with deep gaussian processes.\n\nMultimodal similarity gaussian process latent variable model.\n\n\n2. Some statements in the article are biased. For example, the introduction states that “existing generative models for multi-modal learning focus on latent representation, but do not fully incorporate the label information.”  Generative models are only one of the branches of multimodal learning, and many multimodal supervised models have been developed, e.g., DeepIMV in the paper.\nThe proposed MoG is not convincing and novel, which just averages the graphs from different modalities. In my opinion, this contribution is weak. \n\n3. The experiment of the paper is not sufficient. The paper only compared the two baseline methods on two real-world datasets. At the same time, the experiment did not use large-scale datasets to illustrate the author’s claim \"makes mRNP scalable to large datasets through mini-batch optimization\". \n\nIn addition, I found some problems in the writing. \nIncorrect use of semicolon “;” and colon “:” in the introduction.\nThe meaning of the symbol is not introduced when it first appears (e.g., φ$_{sim}$ in Eq 2).\nThe title of the paper is multi-modal, but multi-view is used many times in the paper.","rating": "2","sentences": [{"sentence_type": "2","sentence": "The proposed MoG is not convincing and novel, which just averages the graphs from different modalities."},{"sentence_type": "2","sentence": "In my opinion, this contribution is weak."}],"comments": null,"annotation_info": {"type": "Manual","humans": ["Navya"],"models": []}}
{"id": "eVpJ-MvuURV","review": "Summary:\nThe paper presents a method that learns from few demonstrations by computing a reward function coming from an alignement score between agent trajectories and expert demonstrations. The alignement score is inspired by methods used in bioinformatics called profile models.\n\nComments:\nI think the paper could be better organise and not focus so much on the RUDDER framework. You could simply look at this problem from the angle of training an RL agent to produce aligned trajectories with the trajectories given by the expert where the local reward at time t is given by the profile models: g((s,a)_{0:t}) - g((s,a)_{0:t-1}).  This reward simply represents how much I deviated or not from the expert trajectories between time t and time t-1. Therefore presenting the method as a trajectory-matching imitation learning method would be way simpler for the reader. Too much emphasis is given to RUDDER and the message of the paper is somehow diluted whereas the algorithm could be simply presented. Indeed almost one page is dedicated to present results on reward distribution whereas I would like, for instance, to have more information on how the events are computed and not be referred to a paper. Indeed understanding how events are computed is crucial because they define the intrinsic metric used to compute distances between trajectories. This has been for a long time the main problem of trajectory-matching methods compared to distribution matching method. Therefore it seems crucial to develop this point.\nIt seems also that the number of events should not be too large for the alignement method to work. It would be nice to have more information on that specific problem. In addition such alignement methods may have problems in stochastic environments unless the events are really well defined. Could the authors expand on this (deterministic vs stochastic environments)?\nFinally I find that the experimental section could be more exhaustive qualitatively and quantitatively. More environments (more or less stochastic) are needed, showing an experiment that pushes the algorithm in terms of number of events to see how it behaves, having an experiment showing different type of events and how it impact the performance. In general, as a practitioner, I would like to have a better understanding of the method  and in particular its robustness that is why I am asking for those experiments.\n\nRating: I think the paper could be simplify by focusing on solving the problem of learning from demonstrations by minimising a distance between expert and agent trajectories with an alignment score. This can be framed as trajectory matching imitation learning. In addition, a more extended experimental section is needed to show the limits of the method. At the moment, it is not ready for publication.","rating": "0","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Navya"],"models": []}}
{"id": "rygLK5fqnX","review": "The paper proposed to use the exact empirical Wasserstein distance to supervise the training of generative model. To this end, the authors formulated the optimal transport cost as a linear programming problem. The quantitative results-- empirical Wasserstein distance show the superiority of the proposed methods.\n \nMy concerns come from both theoretical and experimental aspects:\nThe linear-programming problem Eq.(4)-Eq.(7) has been studied in existing literature.\nThe contribution is about combining this existing method to supervise a standard neural network parametrized generator, so I am not quite sure if this contribution is sufficient for the ICLR submission.\nIn such a case, further experimental or theoretical study about the convergence of Algorithm 1 seems important to me.\n \nAs to the experiments, firstly, EWD seems to be a little bit biased since EWD is literally used to supervise the training of the proposed method.\nOther quantitative metric studies can help justifying the improvement.\nAlso, given that the paper brings the WGAN family into comparison, the large scale image dataset should be included since WGAN have already demonstrated their success.\n \nLast things, missing parentheses in step 8 of Algorithm 1 and overlength of url in references.","rating": "1","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Navya"],"models": []}}
{"id": "M1kkEjBPQTs","review": "Contributions:\n\nThe authors propose to do ensemble learning (1) to further improve dice score as well as \"accumulating\" the predictions of a single model over test-time augmentation (2) to improve outlier performance . The data used came from the CAMUS dataset, and the model is a U-Net, the same architecture used in the original CAMUS paper.\n\nMethod:\nFor contribution 1, the authors split the patients into 10 folds, kept two as the testing set, and trained eight separate models on the remaining folds, keeping a different separate fold as validation set for all the models. A different model was trained for the two views available for each patient, totaling 16 models trained.\n\nFor contribution 2, the authors \"accumulate\" the predictions of a single model trained over a single fold by augmenting a test image 200 times via a combination of intensity modification, rotation and Gaussian noise. \n\nResults:\nFor contribution 1, a box plot of dice distribution is reported over different structures, separated by view and phase. The results are shown for a single model against the proposed ensemble of models.\nFor contribution 2, the dice score improvement for the accumulated result is reported for a single test image, as well as a qualitative assessment of the segmentation for the same image.\n\nCriticism:\nEnsemble learning is generally recognized as an easy way to improve results on virtually any task. However, it is not a cheap method and requires n-times the amount of memory and training time. In itself, the reviewer feels it cannot be considered an improvement of a method. In this particular case, as figure 1 shows, the ensembling can hardly be justified as improvements shown via box plot seem to be marginal, at a cost of 8 times the amount of memory.\n\nContribution 2 seems to have significant improvements over the baseline, the authors' own U-Net trained on a single fold. However, test-time augmentation is another commonly used practice and the reviewer also feels it is not a novel idea in itself. Furthermore, it is unclear what \"accumulating\" means, whether it is taking the overlap of the 200 predictions of the noisy image, a threshold per pixel, or any other method. Finally, the reported results are vague and only from a single hand-picked outlier test image. Nothing can be confidently inferred from this result.\n\nWhile the two contributions are orthogonal, no results are reported on the application of the two contributions at the same time.\n\nFinally, only the dice score is reported, while the original CAMUS paper also reported Hausdorff distance and mean absolute distance.\n\nConclusion:\nThe paper does not present any novel idea for cardiac segmentation. Even though the presented article is a short paper, the article glosses over important details and fails to present meaningful results.","rating": "2","sentences": [{"sentence_type": "2","sentence": "The paper does not present any novel idea for cardiac segmentation."}],"comments": null,"annotation_info": {"type": "Manual","humans": ["Navya"],"models": []}}
{"id": "B1xU3LADtV","review": "Notes: \n  -Problem of teaching an RNN to approximate list processing algorithms from a small number of examples.  \n\n  -Learned data augmentation scheme based on parametricity from PL theory.  \n\n  -Helps with small-sample learning.  \n\n  -Algorithmic problems seem to require more training data than should be necessary.  \n\n  -List processing algorithms map from an input list to an output list.  \n\n  -Many algorithms are distinguished from arbitrary functions is that they commute over elementwise changes to the inputs.  \n\n  -Commutative means applying a function to each element of the list and running the list processor \n\n  -While I like the basic idea that there are logical properties that algorithms should obey, the commutative property for arbitrary functions seems too strong to me.  To give one example: multiplying by -1 does not commute with list sorting.  (note: the paper addresses this later).  \n\nSummary: This is a very good workshop paper which presents a simple idea\n\nSome ideas for future work and directions: \n\n  -It might be interesting to consider enforcing this type of commutative property in the hidden states.  It in some ways would require reversing your way of thinking - because you'd need to think about what properties the hidden states would need to have to allow the sequential part to be commutative, but it would have a big advantage that it would require a less data-dependent way of deciding if the function commutes or not.  \n\n  -There is a technique called Mixup (Zhang 2018) as well as Interpolation Consistency Training (Verma 2019) which tries to encourage linear combination of input examples to map to the corresponding interpolations of the outputs.  If you write the interpolation as a function x, you can rewrite this as: mix(f(x)) = f(mix(x)), where f is the neural network (this is literally what (Verma 2019) enforces and (Zhang 2018) does something slightly different).  Thus it is encouraging interpolations to commute with the neural network function.","rating": "0","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Navya"],"models": []}}
{"id": "Hk37PGqlz","review": "Summary: \n\nThis paper proposes a fully connected dense RNN architecture that has connections to every layer and the preceding connections of each layer. The connections are also gated by using a simple gating mechanism. The authors very briefly discusses about the effect of these on the dynamics of the learning. They report results on PTB character-level language modelling task.\n\n\nQuestions:\nWhat is the computational complexity of this approach compared to a vanilla RNN architecture?\nWhat is the implications of these skip connections in terms of memory consumption during BPTT?\nDid you use gradient clipping and have you used any specific type of initialization for the parameters?\nHow would this approach would compare against the Clockwork RNNs which has a block-diagonal weight matrices? [1]\nHow would dense-RNNs compare against to the MANNs [2]?\nHow would you implement this model efficiently?\n\nPros:\nInteresting idea.\nCons:\nLack of experiments and empirical results supporting the arguments.\nHand-wavy theory.\nLack of references to the relevant literature. \n\nGeneral Comments:\nIn general the paper is relatively well written despite having some minor typos. The idea is interesting, however the experiments in this paper is seriously lacking. The only results presented in this paper is on PTB. The results are quite behind the SOTA and PTB is a really tiny, toyish language modeling task. The theory is very hand-wavy, the connections to the previous attempts to come up with related properties of the recurrent models should be cited. The Figure 2 is very related to the Gersgorin circle theorem in [3]. The discussion about the skip-connections is very related to the results in [2]. \n\nOverall, I think this paper is rushed and not ready for the publication.\n\n[1] Koutnik, J., Greff, K., Gomez, F., & Schmidhuber, J. (2014, January). A clockwork rnn. In International Conference on Machine Learning (pp. 1863-1871).\n[2] Gulcehre, Caglar, Sarath Chandar, and Yoshua Bengio. \"Memory Augmented Neural Networks with Wormhole Connections.\" arXiv preprint arXiv:1701.08718 (2017).\n[3] Zilly, Julian Georg, Rupesh Kumar Srivastava, Jan Koutník, and Jürgen Schmidhuber. \"Recurrent highway networks.\" arXiv preprint arXiv:1607.03474 (2016).","rating": "2","sentences": [{"sentence_type": "2","sentence": "The idea is interesting, however the experiments in this paper is seriously lacking."},{"sentence_type": "2","sentence": "The results are quite behind the SOTA and PTB is a really tiny, toyish language modeling task."}],"comments": null,"annotation_info": {"type": "Manual","humans": ["Navya"],"models": []}}
{"id": "BylifkEWqH","review": "The authors propose Editable Training that edits/updates a trained model using a model-agnostic training technique. Editable training is able to correct mistakes of trained models without retraining the whole model nor harming the original performance. This is attained via meta-learning techniques to avoid catastrophic forgetting, and an editor function to promise mistake correction. The major contribution is a model-agnostic editable training process that is applicable to various neural nets. This paper has brought attention to mistake correction problem in neural networks and proposes a simple and concise solution. In addition, extensive experiments on both small and large-scale image classification and machine translation tasks demonstrate the effectiveness of different editor functions. Overall, this paper is well-written with extensive experimental results. Below are a few concerns I have to the current status of the paper.\n\n1.\tIt would be interesting to discuss if how a good editor function changes over different models, problems, or even l_e’s. In addition.\n2.\tIn general, a DNN needs to be “edited”/”fixed”, when the training data used are not sufficient, and /or the incoming testing data have a different distribution from the training data. In the latter case, say, if the distribution of new data is significantly different from the training data used so far, it may be worth of re-train the model rather than attempting to “fix” the network. There should be a trade-off between “fixable” vs “not-fixable”. It is unclear how this trade-off is modeled/discussed in the paper.","rating": "0","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Navya"],"models": []}}
{"id": "GdNUZpUxH9L","review": "This paper presents a linear-time attention model based on importance sampling and locality sensitive hashing. The idea is to use Bernoulli sampling to approximate the self-attention - which is quadratic in complexity. \n\nHonestly, this is a very crowded space and already many models exist (https://arxiv.org/abs/2009.06732). The authors are aware of these works, cite them and yet there is no comparison. \n\nThis method seems to be rooted in LSH and a very natural question is how does compare to Reformers. There is not even a sparse transformer or local attention baseline in the experiments. This raises questions about whether this paper will even make any impact at all. (comparisons with longformer is done only on speed/memory but not qualitatively). why? \n\nI also find other flaws with the model. If sampling is used, this essentially makes the model stochastic (correct me if Im wrong here). but there are undesirable properties of this such as having non-deterministic inference.\n\nAnother flaw is that method potentially introduces a lot of instability in training. I think the authors could comment a little on this. Transformers are already notoriously difficult to train and I figure that this method would probably make it way harder for practitioners to get the hyperparameters correct. I think playing with the scaling and normalization of the self-attention weights is something non-ideal, and unless the authors can show this is reasonable stable i am not convinced. \n\nI also find it difficult to understand the choices of tasks. It seems like GLUE benchmark is used, yet most of the tasks (like SST) are relatively shorter sequences. I think the authors need to explore datasets that showcase the model's ability on longer sequences. Artificially raising sequence len during pretraining is not really sufficient to be convincing that the model is doing something useful for longer sequences (since the masked out tokens really depend on local context). \n\nMy constructive feedback to the authors to improve the paper is to have reasonable baselines for comparison. The datasets are also not appropriate. I would suggest some actually long-range tasks in order to showcase the model's capabilities. \n\nAt the rate of the number of new models that tackle this problem, I suspect it would be wise to wrap up your sleeves and add actual efficient transformer baselines.","rating": "1","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Navya"],"models": []}}
{"id": "Uoi5qYXu1X","review": "The paper investigates the sensitivity of BERT representations to different kinds of permutations in the input sentence. These transformations include n-gram permutation, span swaps (with or without crossing syntactic phrase boundaries), adjacent token swaps (with different syntactic distance). The authors measure the l2 distance between representations coming from original and perturbed input.\n\nOverall, the results suggest that BERT is sensitive to hierarchical phrase structure.\n\n------------------------------------------------------------------------------------------------------\nStrengths\n\nThe idea of measuring a network’s response to input transformations is nice and potentially could be used to test different kinds of hypotheses.\n\n------------------------------------------------------------------------------------------------------\nWeaknesses\n\nMy main concern is the measure used for a distance between representations. Namely, this is the l2 distance, which accounts for distance in neurons. Therefore, it does not tell us to what extent representations encode different things but rather how different are their individual neurons. For example, different phenomena can be either focused in a network (encoded by only a few neurons), or distributed (see, e.g., the paper [1] or a more recent [2]). \n\nTherefore,\n1) the phenomena that suffer from perturbations have different impact on the l2 distance because they affect different number of neurons\n2) as a consequence of the above, it is not clear what to conclude from the proposed results: they are likely to be not because of the high-level differences in what original/perturbed representations encode, but rather in\nhow these underlying phenomena affect individual neurons. \n\n(To measure differences in what representations encode you can use, for example, PWCCA (NeurIPS 2019 “Insights on representational similarity in neural networks with canonical correlation”) or other related measures.)\n\n[1] AAAI 2019 “What Is One Grain of Sand in the Desert? Analyzing Individual Neurons in Deep NLP Models”\n\n[2] EMNLP 2020 “Analyzing Individual Neurons in Pre-trained Language Models”\n\nOther concerns.\nIf we put aside the validity of the measure, it is still not clear what to take out of these results: they rather show that the method passes sanity checks rather than tell us something we didn’t know before. For example, there’s a huge line of works showing that BERT “understands” syntax and phrase composition (looking at representations, geometry, attention heads, etc). Hence the results stated in contributions 2, 3, 4 are not surprising. Contribution 1 is also more of a sanity check: of course, shuffling smaller parts has to cause more distortion than shuffling longer phrases. \n\n------------------------------------------------------------------------------------------------------\nQuestions\n\nSection 4.1, paragraph 1: when referring to figure 2b, you say “When we shuffle in units of larger n-grams, it only introduces distortions in the deeper BERT layers compared to smaller n-gram shuffles.”\nI have trouble seeing this from the figure. For all lines, the distortion goes up almost linearly from layer 2 to layers 9-10. Yes, shuffling larger ngrams causes lower distortion, but this is expected. Am I missing something?\n\n------------------------------------------------------------------------------------------------------\nMissing references (in addition to mentioned above)\n\n1) When hypothesizing about heads specializing in syntax (section 4.3), none of the relevant previous work is mentioned. For example,\n[3] - for NMT Transformer, showed that the important heads are specialized, and many of them are syntactic.\n[4] - repeated the previous syntax-heads evaluation for BERT.\n[5] - also mention that some heads track syntax.\n\n2) [6] - looks relevant: it also measures changes in representations across models, layers, for different tokens, etc. Maybe the most relevant to your work is the part showing how different words influence each other (e.g., rare words influence others more than frequent ones; the same analysis for POS). Note also which measure they use - pwcca, which is invariant to linear transformations of representations.\n\n[3] ACL 2019 “Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned”\n\n[4] “Do Attention Heads in BERT Track Syntactic Dependencies?”\n\n[5] BlackBoxNLP 2019 “What Does BERT Look At? An Analysis of BERT's Attention”\n\n[6] EMNLP 2019 “The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives”\n\n------------------------------------------------------------------------------------------------------\nPresentation\n\nThe paper is overall fairly clear, but figures 2d and 3e are not readable. Namely, (even with a maximum zoom) it is very hard to distinguish between solid, dashed and dash-dot lines of the same color.","rating": "0","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Navya"],"models": []}}
{"id": "ByWUtfoef","review": "This paper proposes a method to solve the 'word analogy problem', which was proposed as a way of understanding and evaluating word embeddings by Mikolov et al. There are some nice analyses in the paper which, if better organised, could lead to an improved understanding of semantic word spaces in neural nets. \n\ncomments: \n\nThe word analogy task was developed as an interesting way to analyse and understand word embedding spaces, but motivation for learning word embeddings was as general-purpose representations for language processing tasks (as in collobert et al, 2011), not as a way of resolving analogy questions. The authors develop a specialist method for resolving analogies, and it works (mostly) better than using the additive geometry of word-embedding spaces. But I don't think that comparison is 'fair' - the analogy thing is just a side-effect of word embedding spaces. \n\nGiven that the authors focus on the word-analogy problem as an end in itself, I think there should be much more justification of why this is a useful problem to solve. Analogy seems to be fundamental to human cognition and reasoning, so maybe that is part of the reason, but it's not clear to the reader. \n\nThe algorithm seems to be simple and intuitive, but the presentation is overly formal and unclear. It would be much easier for the reader to simply put into plain terms what the algorithm does.\n\nUsing a POS-tagger to strip out nouns is a form of supervision (the pos-tagger was trained on labelled data) that word-embedding methods do not use, which should at least be acknowledged when making a comparison. Similarly, it is nice that the present method works on less data, but the beauty of word embeddings is that they can be trained on any text - i.e. data is not a problem, and 'work' for any word type. Stripping away everything but nouns clearly allows co-occurrence semantic patterns to emerge from less data, but at the cost of the supervision mentioned above. Moreover, I suspect that the use of wikipedia is important for the proposed algorithm, as the pertinent relations are often explicit in the first sentence of articles \"Paris the largest city and capital of France...\". Would the same method work on any text? I would expect this question to be explored, even if the answer is negative. \n\nThe goal of understanding word2vec and embedding spaces in general (section 5) is a really important one (as it can tell us a lot about how language and meaning is encoded in deep learning models in general), and I think that's one of the strongest aspects of this work. However, the conclusions from this section (and other related conclusions in other sections) are a little unclear to me. Perhaps that is because I don't quite get algorithm 3, which would be mitigated by an intuitive explanation to complement the pseudocode. I'm also confused by the assertion that Vec(A) - Vec(B) conveys the 'common information' in A and B. How can a non-symmetric operation convey 'common information'. Surely it conveys something about the relationship between A and B?\n\nMinor point:\n\"may not the be indicative of the model's ability to learn the relationship between a word pair the way a human does\" (Abstract)\n- I'm not sure we know how humans learn the relationships between word pairs. Are you referring to formal semantic relations i.e. in taxonomies in WordNet? This sentence seems dangerous, and the claim about humans is not really treated in the article itself. \n\nThe a+cknowledgements compromise the anonymity of the authors.","rating": "0","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Navya"],"models": []}}
{"id": "rGflTrxuAW5","review": "This paper clearly contains enough material for a full conference paper, but the way it is compressed into 5 pages (and 13 appendix pages) makes it difficult to understand the key concepts / contributions. Many pages are spent on the background and the derivation of the loss function, while other essential parts of the method are described in one paragraph with a reference to an appendix. I believe a much better version of this paper could be made by highlighting the key concepts, succinctly describing the entire method, giving an intuitive explanation of the loss function, and delegating its derivation to the appendix. At the same time, I am sympathetic to the fact that the authors are dealing with a topic that is technically challenging.\n\nThe paper proposes a method for learning a model of a POMDP with the underlying state decomposed into factors. A key question the paper poses is what the minimum number of factors is to describe a particular environment.\n\nThe paper is definitely relevant to the workshop, but it is unclear to me if the main five pages can foster a good discussion. It definitely does not adhere to the following: \"We ask authors to use the supplementary material only for minor details that do not fit in the main paper.\" I am uncertain about acceptance.","rating": "0","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Navya"],"models": []}}
{"id": "zvBJALHzCXw","review": "By integrating Adaboosting and a fully connected layer, this paper provides a new graph neural network structure. The objective of this paper is to design a deeper graph models in an efficient way for better performance. The computational efficiency and performance of the proposed algorithm are evaluated using the task of node property prediction on several public datasets. This is a new variant of GNN, but the quality this paper is lower than the expectation regarding to the clarity and organisation. \n\nPros: \n1.\tThe algorithm integrated Adaboosting for graph data. Thus, AdaGCN could utilise different levels of node features for final prediction. \n2.\tThe method is optimised in a layer-wise way rather than the traditional GCN optimisation, which is similar to the optimisation of recurrent neural networks. \n3.\tAuthors compared the structure of AdaGCN with that of other GNN variants. \n4.\tFor the experiments, the proposed algorithm is more computationally efficient, and achieves better performance on the task of node property prediction.  The performance of AdaGCN is slightly more robust than previous methods. The performance drop is not observed within 10 layers for AdaGCN as shown in Fig 3. \n\nCons: \n1.\tSpeaking of the state of art performance, GraphSAGE with LSTM also achieves a 95.4% F1 score on the Reddit dataset for node classification tasks. Thus, the authors may need to compare the training time and performance with more recent algorithms, like ClusterGCN and GraphSAGE. \n2.\tThe paper is not well written. Many typos are discovered. For example, extra space is added in the first sentence after equation 3. Meanwhile, the punctuation around equations is not consistent. For the full sentence following an equation, one would place a full stop after the equation. However, there is no full stop after equations 5, 6, 7, and 8. Abbreviations, such as \"JK\", \"APPNP\", and \"PPNP\", are used before introduction. \n3.\tSome notations are confusing and misleading. $K$ refers to the number of node categories, and $k$ refers to a category of a node. Meanwhile, $w_i$ and $W^{l}$ have completely irrelevant definitions. \n4.\tTo evaluate the efficiency of different GCN approaches, the authors listed the per-epoch training time of methods. The implementation of GCN with different frameworks would result in the large variance of training time. It is better that the authors could include the time and memory complexity of each algorithm. \n5.\tFig 4, after 10 layers, it is not clear whether the linear trend would continue. This result is a bit misleading.","rating": "1","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Navya"],"models": []}}
{"id": "ZvkFh9VHIQ","review": "Summary (Long)\n- The authors did an in-depth analysis of the search logs related to the vaccines to detect an individual’s vaccine intent and further discovered insights on the behavioral difference of (i) early vaccine adaptors and (ii) vaccine-resistant groups. Their pipeline of the vaccine intent classifier includes finding top candidates for user URLs, using personalized PageRank, followed by annotation via crowdsourcing, and expanding URLs via GNNs. They also prepared an ontology of vaccine concerns by applying a community detection algorithm. Though some of the decisions of model choices are not well justified, overall, the paper is well written, is original, and would help the community to understand behavioral patterns from the web search logs.\n\nStrong points (Pros)\n- Overall, their method could fill the gaps in understanding individual vaccine intentions and behaviors through web search logs.\n- Their vaccine intention classifier design is well-motivated, easy to follow, and performs well at 0.9 AUC.\n- Authors did an in-depth study on this problem and provided enough details and additional analyses in the appendix. \n\nWeak points (Cons)\n- The evaluation of their vaccine intention classifier is insufficient, especially because their model is not compared with other baseline methods. If there are no direct methods to evaluate, the authors should do some literature review on somewhat relevant papers that uses search logs in predictive modeling and have those as a set of baselines to compare the performance of the method.\n- Design decisions of their modeling are often not justified. E.g., in section 3.1, the authors chose to use personalized page rank as it is a common technique for seed expansion methods. In fact, seed set expansion itself is a well-studied problem, and there exist many more methods developed for this problem in the past decade. I’d suggest authors review state-of-the-art methods in the seed set expansion problem and explore some other methods in their pipeline. Some examples are:\n    - Whang, Joyce Jiyoung, David F. Gleich, and Inderjit S. Dhillon. \"Overlapping community detection using seed set expansion.\" Proceedings of the 22nd ACM international conference on Information & Knowledge Management. 2013.\n    - Li Y, He K, Bindel D, Hopcroft JE. Uncovering the small community structure in large networks: A local spectral approach. In Proceedings of the 24th international conference on world wide web 2015 May 18 (pp. 658-668).\n- Authors claim that vaccine concerns differ significantly within holdouts. If this is true, I am worried that the performance of the ‘binary classifier,’ the vaccine intention classifier, may be suboptimal because there could be a large variance in those in holdouts. In such cases treating the problem as clustering and finding the clusters of holdouts with similar vaccine concerns may make more sense. \n\nMinor comments\n- In the abstract, please provide some details about your claims. E.g. the first claim is ‘vaccine intent classifier that can accurately detect …’ – here, please provide how accurate it was. Also, in the abstract ‘… find that key indicators emerge…’ – please list the indicators (maybe provide the most important ones).\n- The captions for the tables should be placed on the top of the table, not below the table.\n- Please justify the usage of CNNs for capturing textual information in the queries and URLs.\n- Please justify using the Louvain algorithm for the community detection problem in section 5. \n- There's a typo in section 3.1 . Please change S-PRR to S-PPR","rating": "0","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Navya"],"models": []}}
{"id": "SJe6N0zcs4","review": "The paper focuses on a theoretical framework for inducing a discrete planning domain in a very general sense, from the perspective of an agent that can act in an environment and receive numeric sensor data.\n\nPerhaps the most attractive aspect of this paper is the ambitious generality of the model learning: it's not just filling in templated action schemas, but going to the point of having an unbounded model capacity a priori (where new values for a discrete domain can be dynamically learned). As far as I can tell, the set of actions and variables are fixed, but what is presented is a nice and important step for the general setting of learning abstract action representations from noisy and continuous environments.\n\nThe work is directly aligned with the special focus of KEPS this year, and my recommendation is acceptance. That said, I think there are a number of areas for improvement that the authors should consider.\n\n* One major missing piece of related work is that of Konidaris et al.\n- http://lis.csail.mit.edu/pubs/konidaris-jair18.pdf\n- Additionally, contrasting things with the model reconciliation work would be useful, especially in the context of modifications to the action theory.\n\n* It took a while to understand that \"states for the system\" in fact meant the valid reachable states (through the specification of invariants). This should be clarified. Perhaps it is not strictly the set of reachable states, but at least it represents a subset of the full state space represented by the product of variables (which is typically what is meant by \"all states for a domain\").\n\n* I found the details on the perception functions extremely hard to parse (e.g., the definitions just before section 3). Some added focus here would help with clarity.\n\n* There are a number of parameters you use, and their descriptions are scattered throughout the algorithm text. It would help to have them in a single table with the intuitive explanations listed (e.g., \"\\eps expresses how much we believe that the set of states learned so far are sufficient for the planning domain to model the real world\"). These intuitive explanations are really enlightening, and help the reader to understand the formulae they appear in -- put them together in a central location to surface these important points.\n\n* Small error: top of page 5: \"prem\" should be \"prec\"\n\n* It seems as though errors will cascade as part of the system: you incorporate the observations in a 1-step fashion. Conversely, if you were to look at an entire episode, or a longer sequence of actions, you should be able to define an update that maximizes the posterior likelihood across the entire sequence. There are obvious connections to RL, and in particular you would have a nice Bayesian analogy to TD(n) learning for your model updates (and subsequently can discuss the tradeoff between accuracy from more history and the computational cost it comes with).\n\n* The examples (especially the third) seem to be a happy path coincidence (your parameters happen to be set the right way, etc). But it's worth also pointing out a sad path for this work -- when do things fail and cascade to a horribly learned model? I don't believe you are protected from such failures (especially with the myopic view of the updates), and it would help with understanding to know both how it works as well as how it fails.\n\n* I realized that the aim is to learn a deterministic model, but even the motivating example is inherently stochastic (with other agents -- i.e., cats -- messing with the system dynamics). A discussion about issues that arise here would be interesting. Note that a mostly deterministic environment that changes is characteristically different than a stochastic environment. In the former, you can recognize when things have shifted (and perhaps dynamically adjust the 4 parameters in response), but in the latter you will constantly thrash between deciding the one true outcome of actions.","rating": "0","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Navya"],"models": []}}
{"id": "S1evAPSTtS","review": "The paper explores learning dilation-invariant sentence representations, with a goal of improving downstream task performance on rare events. A pre-trained embedding is encoded as a latent variable Z, which is constrained to be multi-variate heavy tailed. Separate classifiers are trained on the head and tail of the distribution. Similarly, separate sentence generators are trained on the head and tail of the distribution, in order to allow data augmentation (creating diversity in the outputs by scaling the representation). While the high level motivation and algorithm is interesting, I found the paper very hard to follow, and the experiments are weak.\n\nI have quite a few concerns:\n- The algorithm takes a sentence embedding from BERT as input. BERT produces contextualized word representations, not sentence embeddings, so I don't know what the authors did here (the intro also claims that ELMo and GPT learn sentence embeddings, which is also confusing).\n- The paper argues that with empirical risk minimization, \"nothing guarantees that such classifiers perform satisfactorily\non the tails of the explanatory variables\". However, I could not follow what such guarantees the proposed method offers, if any.\n- Experiment 4.1 is impossible to follow without reading the appendix. This section should be expanded, or completely moved to the appendix.\n- The authors claim without evidence that a baseline of a neural network trained on top of the \"BERT embedding\" is state-of-the-art for sentiment classification. While there isn't enough information to know what was done, most state-of-the-art approaches involve fine-tuning BERT.\n- No comparisons are made with any other work, despite the method attempting a very general and well studied problem of text classification.\n- The submission claims that \"Applying a dilation is equivalent to assess the generalization of classifiers outside\nthe envelope of both training and testing samples.\". It isn't obvious to me that dilation captures the variation in embeddings you'd get from out-of-domain training samples.\n- The authors compare their data augmentation results to \"backtranslation\". The citation for the method appears to be a class project, and in fact does round-trip translation for paraphrasing, and not back translation.\n- No attempt is made to show if the data augmentation approach actually improves end task performance.","rating": "0","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Navya"],"models": []}}
{"id": "ixQx4BmA2T","review": "Weaknesses:\n- The paper does not contain new methodology; it is purely a combination of existing techniques (generalized random forest and doubly robust estimator). \n- The paper does not provide a theoretical analysis of the proposed method.\n- The authors oversell their contribution: Generalizing the Generalized Random Forest (Athey et al. 2019) from a partially lienar model to a full nonparametric one is not very creative. \n- It is unclear to me how the estimator $\\hat{\\mu}(t, X_i)$ is constructed. It is the weighted average of something (?), where the weights w_i(x) are presumably the frequency with which the ith training example falls into the the same leaf as x (see Athey et al. 2019).\n- The simulation study is not reproducible because the authors do not include values for sample size $n$ and dimension $p$. \n- The simulation study and real data analysis are very small scale.\n- Unsupported claims: In the introduction the authors claim that their method allowed for faster hyper-parameter tuning than competing methods. However, they do not provide any evidence to back-up this claim.\n\nAdditional comments:\n- Poor notation: For example, on page 3 $\\Omega$ is defined as a single vector, on page 4 the authors draw samples from $\\Omega$, in Algorithm 1 on page 5, $\\Omega$ denotes the dat set.\n- The exposition is unorganized and contains many abbreviations which are hard to keep track of.\n- The language is unnecessarily pompous and riddled with grammatical mistakes to an extend that makes it difficult to follow the authors ideas.","rating": "1","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Navya"],"models": []}}
{"id": "bnqE38fHCY3","review": "The paper is written and explained very well. The authors have employed agent-based simulation, incorporating six distinct states and separate sampling for household and non-household contacts. The authors have introduced risk based ring vaccination and showed that it is more effective compared to the random allocation and ring allocation. Furthermore, the authors have provided insightful suggestions for potential future research directions, all of which are highly intriguing and would greatly enhance the existing work.\n\nThe assumptions regarding within-household contact appear logical, while estimates for non-household contact draw from a social contact pattern study conducted in Malawi. It's important to talk about why these assumptions and estimates are important and how they affect the proposed vaccine strategy.\n\nIt would be interesting to discuss the C.I. patterns shown in Figure 1. Specifically, we could look at whether the variability decreases for certain vaccine allocation strategies after a certain number of days. Notably, in Figure 1(b), why does the curve based on ring vaccination exhibit such a narrow range between 80 and 100 (around 90-95)?","rating": "0","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Navya"],"models": []}}
{"id": "SylJiIjpqr","review": "Summary of the paper: The authors propose a latent variable model RaDOGAGA, a generative autoencoding model. The model is trained via a tradeoff between distortion (the reconstruction error) and the rate (the capacity of the latent space, measured by entropy). The paper provides an analysis of theoretical properties of their approach, and presents supporting experimental results.\n\nReview tl;dr: weak reject, for three main reasons:\n(i) While the existing literature around VAEs, beta-VAEs,  and Rate-Distortion theory is mentioned in the related work, the connections are not nearly discussed sufficiently. \n(ii) On top of (i), the derivation of their loss function and architecture is not sufficiently motivated. This is in astonishing contrast to 1.5 pages of main text and 8 pages of (much appreciated!) analysis of properties.\n(iii) Given the paper is clearly related to existing approaches in the literature, the experiments would require a much more careful comparison to existing models. It remains unclear why an interested user should favor your model over conceptually simpler generative models with fewer hyperparameters.\n\nDetailed review:\n\nNota bene: This review is a late reassignment. While I reviewed the paper to the best of my ability, time constraints did not allow me to review parts of the paper in depth.  I am open to reassess my review during the second stage.\n\nConnection to prior art: As a probabilistic, neural autoencoding model, the connections to the family of VAE models are obvious. The loss function (eq. (4)) still looks very much like the ELBO, where the typical conditional log-likelihood was split into two distortion terms. How is this different from e.g. a beta-VAE? Particularly, what is the connection between the rate-distortion analysis of beta-VAE by Alemi et al. and yours? These things need to be discussed explicitly, with more than a sentence or two in the related work section.\nA lesser, but still important omission in your discussion of prior work: The Jacobian of the generator has also been studied, even for the VAE, cf. e.g. [1]. I believe this deserves more attention in your assessment of prior art.\n\nMotivation: You use two distortion terms: actual sample vs. undistorted reconstruction. Why is that? What is the interpretation of the multipliers? How do I choose them? Why is a large part of your architecture (the pipeline from x to \\hat(x)) actually deterministic? Why are you using the entropy of the prior over the latents, rather than the KL divergence between encoder and a prior? I think an interested reader could learn much more from your paper if you discussed your model embedded in th related work rather than in isolation.\n\nTheory: Due to aforementioned time constraints, I was not able to review the extensive theoretical analysis in depth. Still, I would strongly recommend structuring the respective sections more clearly. Separate model and architecture description from the theoretical analysis; precisely formulate your claims. In particular, state your assumptions clearly. For instance, you assume \"that each function's parameter is rich enough to fit ideally\" (and similar e.g. in Appendix A). Does this only mean that the true distributions are part of the parametric family? What if this is not the case? Do your parameters need to be in the optimum for your analysis to hold true? \n\nGiven that the full 20-page manuscript spends 10 pages on theory, I think this contribution is not given appropriate space in the main text.\n\nExperiments: There are three experiments: a simple 3D proof of concept; anomaly detection; analysis of the latent state in CelebA. As mentioned in my review of the methods section, I believe the approach to be very similar to established models. None of the experiments provides convincing evidence why I should prefer the new, arguably more complex model.\nFor instance, I would have much preferred that you investigate properties of your model against alternatives over the anomaly detection experiments, which did not further my understanding of the proposed model. \n\nSummary: The paper tackles an important problem, namely the lack of control over the latent embedding in autoencoding generative models. I believe the author's contribution can be valuable, and I particularly appreciate the effort to investigate theoretical properties. As is, the case is not sufficiently convincing to be accepted, but I encourage the authors to improve the paper.\n\nMinor comments:\n1. While I appreciate a pun, I would recommend to rename the model along with the acronym to a more concise name.\n2. Please revise your notation and typsetting. Examples: x1 instead of x_1, f of f(\\cdot) instead of f(), \\log instead of log.\n3. Introduce acronyms before using them (e.g. VAE, MSE, SSIM), even when they seem obvious to you.\n4. Please carefully check the manuscript for typos, missing articles, missing spaces etc.\n5. Your citations are inconsistent, in that they sometimes use first names, sometimes first name initials, and sometimes no first names.\n6. To my knowledge, the term scale function does not have an obvious definition. I think you are simply referring to monotonically increasing functions. Please clarify!\n7. Your figures should be understandable without too much context, they need more detailed captions.\n\n[1] http://proceedings.mlr.press/v84/chen18e.html","rating": "0","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Navya"],"models": []}}
{"id": "Mz4W59bZ6d","review": "## 1. Brief summary\nThe authors study what they call a negative pretraining effect = models pretrained on task 1 and tuned on task 2 sometimes underperform compared to just training on task 2 from scratch. This is an important factor in many forms of life long learning, multi-task learning and curriculum learning. They investigate 3 potential remedies / setups: a) using different learning rates for task 1 and task 2, b) changing from task 1 to task 2 more smoothly, and c) resetting network biases at different stages of the process. The perform with a single ResNet18 architecture on MNIST, Fashion MNIST, SVHN and CIFAR-10.\n\n## 2. Positive things\n* I think the problem is very important and applicable in many problems in ML, especially in its practical deployment\n* The problem is important and interesting from the theoretical point of view as well\n* The interventions / experimental setup the authors use (a) LR changes, b) smooth task transition, and c) bias resetting) are all reasonable and potentially actionable if shown effective\n* I like that the authors report results of multiple random seeds and show the full distribution of the results. This allows the reader to better judge the validity of the claims made and \n* I like many aspects of your experimental setup\n* I like the idea with path discretization and studying the effect of the number of discrete steps\n\n## 3. Negative things / points of confusion\n\n1. The experiments are way to limited in scope to establish generally applicable results. I'm not asking you to go all the way to ImageNet, but several things would greatly improve the potential value of your work:\n\na. Adding datasets with a different number of classes than 10. All the datasets you used have 10 classes (MNIST, Fashion MNIST, SVHN, CIFAR-10) and it is at least plausible that some of the effects you see could be caused by some sort of class specialization. It would be very helpful to see smaller number of classes (you can restrict some of the datasets you have already) and let's say CIFAR-100 on the other end. I think these would not be crazy hard to add but would greatly increase the value of your paper.\n\nb. You only use a single architecture. It would be good to try others. Especially the learning rate considerations and experiments could be influenced by the fact that you chose a ResNet which are often hard to train from initialization at low learning rate (people sometimes preheat them by high LR initial phase of training). You could try a simple multi-layer CNN, and fully-connected multi-layer net. It would be good to see that your conclusions hold generally, not just for ResNets.\n\n2. I am not extremely convinced by some features of your experiment A where you use different learning rates for the blurred task 1 and the sharp task 2. Firstly, why is the baseline performance a fixed LR = 10e-4? Did you use a hyperparameter search of sorts of find it? Because if it were optimal, the fact that e.g. panel 1 in Figure 3 the first two results perform worse could be just because their LRs are smaller, and not because of the effect you're trying to observe (the negative pretraining). It is at least suspicious that the blurred -- sharp task starts performing equally to the default sharp task *precisely* when the LR of the blurred -- sharp task is at least as big as the default task. It is certainly a confounding factor that does not allow you to draw the conclusion you do -- namely that it is the LR change that helps, rather than just increasing the LR. I suggest you show multiple baselines with different LRs so that we can compare to them. Also, please show the errorbars for the baseline performance as well, so that we can judge the overlap of the uncertainty intervals.\n\n3. In Figure 3 there is essentially no signal there for Fashion MNIST and a week signal for SVHN. How does that fit with the causal explanation you are offering. I'm really not seeing how you can draw a conclusion as strong as \"the precise order of learning rates across tasks can be crucial\". I don't think Figure 3 shows that clearly, or possibly at all. Especially considering the potential confounding that I described in point 2.\n\n4. For experiment A in Figure 3, wouldn't you expect that the ordering of the LRs would matter. In panel 1 [5,5] and [10,5] look the same, why? and [10,20] and [20,20] also look the same. How could both be true if you're expecting that you should either increase/decrease the LR to get good performance.\n\n5. This is a point that I am not sure about, so please correct my if I'm wrong. I don't get how precisely you do the bias resetting. Do you set all biases to 0, or reinitialize them again from a distribution. Why would doing this at the begging before the blurred task have any effect. And if it does, doesn't it just mean that your original initialization was not good enough and now you made it better. It might not have much to do with the sequential learning but rather with this. Let me know if I'm wrong here.\n\n6. In Section 7 you say \"negative pretraining is not an optimization issue\". That is possible, but I don't think you show that. The fact that it's a question of generalization and not the training loss going to zero does not mean it's not a question of optimization -- optimization includes the generalization properties of whatever optimum it found, so it definitely does deal with this. This might be an issue of semantics and if so just let me know, this is not a major point.\n\n7. Figure 6 results don't look very conclusive to me, which probably means that the bias reset doesn't work too well (method C).\n\n## 4. Tiny issues and suggestions (I'm not judging the paper based on those):\n* You use e.g. 1e-1 instead of $1\\times10^{-1}$ and it doesn't look very good (it's also kind of hard to read). It might be worth typesetting those values in a more aesthetic way.\n\n## 5. Potentially relevant papers\nWhen you discuss the effect of learning rate on updates, it sounds similar to this paper:\n[1] Stiffness: A New Perspective on Generalization in Neural Networks (https://arxiv.org/abs/1901.09491) where they measured how \"stiff\" a NN is when trained with different LRs by comparing the learning effect of one image on another (also between datasts).\n\nFor the bias resets the class structure the DNN learns might be crucial. Here's a good overview of such class-specific effects:\n[2] Traces of Class/Cross-Class Structure Pervade Deep Learning Spectra (https://arxiv.org/abs/2008.11865)\n\nIn the NTK discussion [3] discusses how the NTK behaves when you expand it around a partially pre-trained point in the weight space which sounds relevant to the way you study how the task 2 training depends on where task 1 got you:\n[3] Deep learning versus kernel learning: an empirical study of loss landscape geometry and the time evolution of the Neural Tangent Kernel (https://arxiv.org/abs/2010.15110, NeurIPS 2020). It also discussed the importance of the early phases of training that you discuss.\n\n[4] The Break-Even Point on Optimization Trajectories of Deep Neural Networks (https://arxiv.org/abs/2002.09572, ICLR 2020) also looks at the crucial effect of the early phases of training.\n\n## 5. Conclusion\nI think the question asked is interesting and the approach the authors took promising. However, the breath of experiments is not large enough and I think there are significant potential confounding effects in at least the setup A (changing the learning rate) that make it really hard / inconclusive to draw strong causal conclusions about the effect of the intervention on the negative pretraining effect. I encourage the authors to improve the paper and resubmit -- this seems like a really promising piece of research, but as it is it's not strong enough.","rating": "0","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Navya"],"models": []}}
{"id": "nOUBG5ttHQc", "review": "This paper empirically studies whether a well-calibrated, pre-trained network can be used for sample selection during the training of another network. The idea is very intuitive: well-calibrated predictions $\\implies$ good entropy estimates $\\implies$ reliable selection of most informative training samples. I, therefore, think that this paper can facilitate interesting discussions.", "rating": "0", "sentences": [], "comments": null, "annotation_info": {"type": "Manual","humans": ["Navya"],"models": []}}
{"id": "H1eeawVR37","review": "The paper proposes an algorithm for mental fatigue monitoring, relating a subjects' EEG signals to their reaction time (RT) during a simulated driving task, as an ordinal regression problem. The authors argue that RTs could be heavily skewed and/or non-smooth, making traditional regression approaches unstable due to outlier values. They propose a brain dynamic ranking algorithm,  BDrank, using a generalized EM algorithm to estimate its parameters, and compare it to support vector regression and Logistic Ordinal Regression, where they show improved performance by accuracy and root mean squared error (RMSE) over a database of 44 subjects.\n\nGeneral comments, in no particular order:\n\n1. There are some minor grammatical errors throughout. The paper could benefit from another read-through to correct these errors.\n\n2. It is unclear to me how the model works at test time; as the model is essentially building a relational structure in the data, does the user have to provide multiple EEG trials at time of prediction?\n\n3. There is notation early on in the paper that doesn't appear to be appropriately defined. For example, Equation (1) describes two sets of propositions, with M1 and M2 elements, respectively. How is M1 and M2, the total set of propositions, calculated? It appears to be all pair-wise comparisons of RTs but then it's unclear why there are two indexes associated with them. Also, what does it mean for the orderings to be significant? (i.e.: that the \"type-1 preference propositions that the orderings between the RTs are significant\"). The authors then switch to a new notation x^1 and x^2 without defining them. Notational problems also persist throughout the paper, making it hard to gauge what is being done at each step.\n\n4. The authors describe using an FFT to transform the EEG data into the frequency domain. I'm assuming they are doing the FFT on the entire 10-s interval but the paper does not make this clear. Also, the authors state using EEG power between 0-30Hz for their analysis; do they further sub-divide this range (for example, to the standard theta/alpha/beta power ranges) or just use the power across the entire 0-30Hz band?\n\n5. I am concerned with the relatively sparse set of comparison algorithms the authors use. The authors only compare to relatively simple approaches (support vector regression and logistic ordinal regression), yet they cite many previous works in this area but do not compare against them, instead just leaving a pretty generic statement of \"The regression assumption of this method between EEG signals and RT is not correct\"; they do not elaborate on this aspect.\n\n \nOverall I think there is limited novelty in the approach; the idea to learn the structure of the data relationally instead of absolutely is pretty straight-forward, and is a standard practice for example in non-parametric statistical modeling. I am also not positive that ICLR is the best venue for this work; perhaps a better avenue for this would be in a more BCI/neural engineering-focused venue.","rating": "0","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Navya"],"models": []}}
{"id": "7b_VWzn-bf","review": "#### Summary:\nThe paper brought up an interesting limitation of a Transformer network, that information about the context is stored mostly in the same element-wise representation, which might limit the processing of properties related to global information. This work proposed adding memory tokens to store non-local representations and creating memory bottleneck for the global information. The evaluation shows positive results adding memory to a Transformer network.\n\n#### Weakness:\nThe idea is not entirely new; Memory augmented networks have been proposed in previous work and the paper does not differentiate the method from related work very well. \n\n-The paper is not clear about how to obtain the memory input token. According to the paper, the memory input token seems like a static vector; however, a memory augmented network can retrieve a dynamic vector for global information, with the ability to read and write to the memory. \n\n-The naive Mem Transformer Layer is counter intuitive, as there is little differentiation between the memory layer and the sequence layers. How the memory layer contains additional global information is not explained. \n-Figure 1c is confusing, are the arrows crossed or not?\n\n-The formulation of memory representation and sequence representation are exactly the same for the MemCtrl Transformer, which again is very counter intuitive. It is intuitive that we should augment the sequence transformer layer with additional global information, however, we might not want the other way. The memory layers should be updated at coarser granularity. \n\n-The paper is poorly written with many errors in the paper. Please proof read the paper before submission. \n\n#### Detailed feedbacks:\nThe reviewer finds this paper hard to read as there is not a flow of story in the paper. The layout of the paper and the design of experiments seems very arbitrary. \n\n-The motivation for a memory network like proposed in the paper is not clearly stated. How the memory network can capture long-term dependencies and information is not clearly explained. The design of variants of Memory Transformers seems very arbitrary. \n\n-The paper also lacks a more detailed comparison with related work, like Transformer-XL, Star-Transformer, Longformer, ETC, etc.\n\n-\"global\"->''global'' (make sure it looks correct in latex). \n\n-\".Surprisingly\"->\". Surprisingly\"\n\n-Please improve the quality of writing and ask native speakers to proof read the paper.","rating": "2","sentences": [{"sentence_type": "2","sentence": "Please improve the quality of writing and ask native speakers to proof read the paper."}],"comments": null,"annotation_info": {"type": "Manual","humans": ["Navya"],"models": []}}
{"id": "mvO_ElbPpg2","review": "**Summary of the Paper:**\nThis paper looks at a problem with slot attention (SA) where similar objects are not treated independently and instead combined to one result which does not represent either one of the objects well. This is argued to be a consequence of the *set-equivariance* property. Therefore, the authors aim to make slot attention *exclusively multiset-equivariant*, i. e. allowing similar inputs to have non-similar results. To this end, they relate the calculation of the attention matrix $A$ in SA to the problem of (entropy-regularized) optimal transport (OT). They introduce several approaches using the Sinkhorn algorithm to obtain $A$. In trying to combine the advantages of those, they end up with **SA-ME** which calculates $A$ through an entropy-regularized OT problem and also minimizes the entropy to allow for \"tiebreaking\" (and therefore multiset-equivariance). Experiments show SA-ME to have better results than other slot attention approaches without a significant increase in computation time.\n\n**Strengths:**\nI like the motivation behind the paper and how it is structured to make the thought process, advantages, and disadvantages behind the introduced SA approaches clear. It is very well-written. Overall, the goal of creating multiset-equivariant methods appears to be a worthwhile endeavor. I also like how the authors discuss the results and include iDSPN despite its better performance on the more strict thresholds. Here, I also agree with the authors that the proposed methodology is still meaningful, both because it increases results compared to SA in general and compared to iDSPN for some thresholds in particular, and because I think the motivation and idea behind this methodology might also be useful elsewhere. Although not very important, I also really like the explanation and example for the similarity matrix $S$ in the appendix.\n\n**Weaknesses/Possible Improvements:**\nThe paper might benefit from (although not require) one or two figures illustrating motivation and results. I would have liked to know more insights on why iDSPN performed better on the stricter thresholds and if SA-ME could get as good results if it was closer to convergence. It would have been nice if there was some experiment which goes back to the motivation, for example a query of two identical or very similar objects in the CLEVR dataset and comparing this to standard SA. Even just a small qualitative experiment here could help to highlight the benefit of the proposed method in such a scenario (again, a figure would have been nice). However, I do acknowledge the page limit and as a result can understand the decisions made, shortening other parts might be difficult.\n\nIs there a specific reason why the re-implementation of SA is so much better than the original SA results (stricter thresholds)? I also find the question in the last sentence (line 211) quite interesting but it is fair to leave this for future work.\n\nLine 138/139: I believe it should say \"tie breaking optimizations\" instead of \"optimization breaking ties\".\n\n**Review Summary:**\nTo sum up my review, I can say that while I was not very familiar with set-/multiset-equivariance, the OT problem and the Sinkhorn algorithm, the paper itself and the cited references made things clear to me. Although I am no expert on this topic, I now feel confident in my assessment and suggest accepting this paper to the workshop. While I would have liked to see a bit more regarding the results, I really like the motivation, methodology, and the explanation of the methodology and think that these points warrant accepting this paper.","rating": "0","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Navya"],"models": []}}
{"id": "5rBqDmrBEI","review": "Interesting theoretical work, would love to see some simple NN experiments.","rating": "0","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Navya"],"models": []}}
{"id": "kTKHFRaH2I","review": "The main contribution of this paper is a COVID-19 vaccine-intent classifier that can potentially give an accurate measure of vaccine hesitancy in an individual by analyzing the search history. This classifier is trained on search queries and website clicks from Bing search logs and annotated using Amazon Mechanical Turk. \n\nAnother contribution is an ontology of website URLs which consists of 25,000 vaccine-related URLs, organized into eight top categories to 36 subcategories to 156 low-level URL clusters. They combine this ontology with their vaccine-intent classifier and got improved performance.\n\nThe classifier correlates with the CDC vaccination data in the sense that states having high vaccination rates have low vaccine-hesitancy and states having low vaccination rates have high vaccine-hesitancy.\n\nOne weakness is that they cap their analysis till August 2021, since the FDA approved booster shots in September and their method cannot distinguish between vaccine seeking for the primary series versus boosters. But it still would have been interesting to see how the classifier performs beyond August 2021. Also it is not clear how this method will perform with other vaccines that are not as popular as the COVID-19 vaccine.\n\nBut overall the contribution is nice and I think it should be accepted.","rating": "0","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Navya"],"models": []}}
{"id": "r1lJBVcRF4","review": "The main contribution of this paper is the proposed Domain Invariant VAE (DIVA), which decomposes the latent space of a VAE into 3 parts: one for the class, one for the domain, and one for the rest. Additional auxiliary classifiers are introduced to encourage the separation of domain and class specific latent codes. This framework has also been extended to semi-supervised learning. \n\nPros: The studied problem is interesting, and the model itself is also intuitive. The overall framework looks elegant. \n\nCons:\n(i) There is nothing special that surprises me in the model. It follows standard VAE design and standard extension to semi-supervised learning for VAE. It naturally extends the original VAE to incorporate domain-specific latent code inside. So I feel the whole model design is kind of boring.\n\n(ii) Only experiments on MNIST are considered, so the experiments are relatively less interesting. As also noted by the authors, more interesting experiments on more challenging datasets are desired.","rating": "2","sentences": [{"sentence_type": "2","sentence": "There is nothing special that surprises me in the model."},{"sentence_type": "2","sentence": "I feel the whole model design is kind of boring."}],"comments": null,"annotation_info": {"type": "Manual","humans": ["Navya"],"models": []}}
{"id": "r1xyFC0tYE","review": "This work proposes an approach to tackle the domain adaptation problem in semi-supervised learning, based on a decoupling of the computation of the batch statistics in the batch normalization layers.\n\nIn a setting where the unlabeled data does not follow the supervised data distribution, semi-supervised learning techniques can lead to a degradation of performance with respect to a purely supervised setting. In this work, it is shown that computing the batch normalization statistics separately for the unsupervised and for the supervised data can alleviate the domain shift and lead to improved semi-supervision.\n\nI have a few questions and remarks:\n\na) The introduction mentions the problem of the domain shift for the unlabeled data. I would add that it is unclear how one could benefit from unlabeled samples in the general case if those samples are completely out-of-domain: after all, the core idea of semi-supervised learning is to grasp a better prior on the data domain. I can see that the network can still learn information e.g. when the inputs share the same modality (RGB data) or has an overlap of the classes. Overall, I would make this clearer in the introduction what one expects from semi-supervised learning in an out-of-domain setting. One thing is that semi-supervision should not degrade performance w.r.t. a purely supervised setting, which can happen with current semi-supervised algorithms.\n\nb) I would also experiment with random noise unrelated to the supervised data distribution to see the limits of the approach, and study a case of extreme domain mismatch. In such setting, one would hope to match the purely supervised baseline performance. I expect a batch-norm adaptation to be insufficient for this.\n\nc) I assume that at test-time, the batch norm statistics computed on the supervised set are used; I would make this clear in the document.\n\nI think that adapting batch norm is sufficient in the experiments done but probably not a universal remedy to domain shift in semi-supervised learning, which could be shown with extreme distribution. In general, extra experiments could also show a more progressive evaluation of different shifts, between same-domain unsupervised data, and fully out-of-domain unsupervised data.\n\nI think however that this idea raises some valid points and introduces an easy fix that can be enough in some cases; moreover split-BN can stimulate new ideas related to domain shift and out-of-domain unsupervised learning. Therefore I believe this paper has its place in the workshop.","rating": "0","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Navya"],"models": []}}
{"id": "BrrxbK2dcWc","review": "This paper presents an empirical investigation on the role of different binding mechanisms between actions and objects for world models that are grounded in object-based environments. Specifically, the authors investigate the benefit of two proposed binding mechanisms (soft and hard attention binding) across an extensive set of experiments, ranging from simple toy environments to a more challenging robotic manipulation task.\n\n- I highly appreciate this work and consider it an important contribution towards a better understanding of how to process and model actions in object-based world models.\n- The results and conclusions are very interesting: On the one hand side, they show a strong benefit of using hard attention over soft attention and the baseline in the 2D SHAPES and 3D CUBES environment. On the other hand, a dedicated attention-based binding mechanism does not always seem to be beneficial as is shown in the Atari environments. The authors attribute this to the fact that actions typically only directly affect one object in these environments while every slot captures information about all objects. I do not necessarily think that this is a “negative outcome” but was very pleased to see an open discussion about the reasons and interpretation of this result.\n- The paper is very well written, providing all relevant details and thereby easy to follow.\n- The experiments are very extensive with all results being presented and analyzed in a succinct and technical sound way. \n\nI strongly recommend this paper for acceptance and think it is a great thematic fit for this workshop!","rating": "0","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Navya"],"models": []}}
{"id": "SpnJdMqEoh","review": "The paper suggests improving the speed of ViT training by adding convolutional layers and resolution based curriculum learning.  A benchmark for resource limited ViT training is also proposed.  The speed-ups seem to beat baselines in the 24 hour 1 GPU timeframe proposed in the benchmark, but the performance in this setting is very far from what is achievable with 4 GPUs in 72 hrs, so the paper seems to oversell significantly.","rating": "1","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Navya"],"models": []}}
{"id": "HOWgHAJbZW9","review": "This work proposes a relaxation of the (multivariate) non-central hypergeometric distribution, which can be used for modeling the number of successes when elements from more than one class can be drawn. The relaxation is based on the Gumbel-Softmax trick. Experiments in this paper on weakly-supervised learning demonstrates that it works well in practice.\n\nThe paper is mostly well written. The contribution is clear, namely a relaxation of the non-central hypergeometric distribution. While a lot of its parts have been used before in other relaxation methods (e.g. sequential application of Gumbel-Softmax's), it is still not a trivial extension to the specific distribution presented in the paper, and probably of interest to a part of the audience. The description of the method in Section 2 is detailed enough for comprehending all the steps and parts necessary for implementing this approach.\n\nIn its current form, the paper has three aspects that can be improved. Firstly, the introduction should give clearer examples on where this distribution is useful, especially in a machine learning setting. While the second paragraph of the introduction gives broad examples of biology to social sciences, none of them are very specific, and the experiments conducted in the paper focus on a different aspect. It would be better to align this part with the experiments. The second aspect that can be improved is the clarity of the experiments. For a lot of the experimental setup, the reader is pointed to different papers, and the overall goal/task of the models is not clear to readers who are not familiar with the works. It is advised to clarify the experimental setup (which also combines with the first aspect in the introduction) to open up the paper to a wider audience. Finally, the discussion on the accuracy of the learned representations is not completely fair. In the plot in Figure 1b, the proposed method (HypergeometricVAE) is consistently outperformed in all 4 settings by both baselines, where the LabelVAE's lower std is always above the HypergeometricVAE's upper std performance. The difference is in some cases more than 15%. The paper does not acknowledge or discuss that, but instead says that all methods perform comparatively, which is not fully fair. It is not bad to have worse results than baselines here, but there should be a fair discussion of the numbers, and why the proposed method is lacking behind the two baselines.\n\nOverall, the contributions of this paper outweigh the drawbacks, assuming that the authors will tackle the aspects mentioned above for the final paper version.\n\nMinor points:\n* Page 1, Introduction, line 4: last citation missing brackets.\n* Equation 1: the class weights omega_l/_r have not been introduced at this stage.","rating": "0","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Navya"],"models": []}}
{"id": "TiVyJifRi","review": "The idea of learning prior to distribution on convolution kernels is methodologically sound and appealing. This new way of transfer learning could potentially be more effective than fine-tuning and L2 regularization (which basically is a zero-mean Gaussian prior). The preliminary results are reasonable.\n\nNow the authors should think about how to further extend and validate this work in the following two aspects:\n\n1. how can the generative power of VAE be used in the segmentation model? Can you learn a family of DNNs to improve segmentation or quantify uncertainty?\n\n2. how does the prior compare to other standard regularization approaches?","rating": "0","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Navya"],"models": []}}
{"id": "3c6rO_1ACJ","review": "Nice paper discussing and testing how calibration affects the performance of data prioritization.","rating": "0","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Navya"],"models": []}}
{"id": "BkldhOiScN","review": "This paper proposes a GAN-based model, i.e., LGGAN to generate labeled graphs. The generator of LGGAN is a multi-layer perceptron, which samples from a standard normal distribution to generate the label matrix L and the adjacent matrix A. On the other hand, the discriminator takes a graph sample as input and outputs a scalar.\n\nEmpirical studies are conducted for generating citation and protein graphs. Under the maximum mean discrepancy (MMD) metric, LGGAN outperforms existing methods like MMSB and DeepGMG on both citation and protein graph generation. LGGAN also demonstrates advantages over MMSB in terms of the graph statistics, which is used to measure dissimilarity between the generated graph and the training graph.\n\nOverall, the paper is well-written, the methodology makes sense to me, and the experimental results look good too. So I will vote for acceptance.","rating": "0","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Navya"],"models": []}}
{"id": "k6Vj161yLNo","review": "The paper includes author information, which violates the double-blind policy. Nevertheless, I provide my comments below.\n\nThe paper presents a remote photoplethysmography (rPPG) framework for heart rate detection in far-field environments. The proposed method addresses the limitations of existing close-range heart rate measurement techniques by selecting a larger region of interest (ROI) using feature point tracking. \n\n\nThe authors tackle an important problem -of remote heart rate measurement in far-field environments-, however, GI might not be the right venue for publication. Furthermore, the proposed method is straightforward and relies on well-established techniques. It is unclear where the novelty lies. I also find the experimentation and comparisons to be quite limited.","rating": "1","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Navya"],"models": []}}
{"id": "SJMafiLez","review": "Paper summary:\nAuthors extend [1] to form an auto-encoder CNN network for face mesh representation. Face mesh graph is represented by Fourier basis of graph Laplacian and therefore convolution operator is defined in Fourier space. Chebyshev polynomial is used for faster computations. Max pooling on graph is done by using Graclus multilevel clustering algorithm. Binary tree generated in pooling layers are kept for unpooling layers in decoder network. Authors captured a new facial dataset for their evaluation and reported better results than PCA.\n\nPositive points:\nAuthors tackle irregular data feature extraction and learning using CNNs which is a hot topic in deep learning.\n\nNegative points:\nAlthough proposed idea is interesting, paper has a number of critical problems. Firstly, experiments are the main weakness of the paper. Set of experiments does not prove claims of the paper. \n- It is not clear how authors uses PCA to reconstruct faces in the test set.\n- Authors do not compare to any state of the art on 3D face representation and reconstruction (e.g. [2]) using public datasets (e.g. BU-3DFE). \n- How network behaves by introducing noise on vertices?\n- What is the effect of network hyper-parameters?\n\nSecondly, paper has a lack of novelty. It is a simple extension of [1] without considering and solving problems in [1]. Also, it is not mentioned what is the loss function to train the network. I suppose it is L2 norm loss, but it must be clear in the paper.\n\n[1] M. Defferrard, X. Bresson, and P. Vandergheynst.  Convolutional neural networks on graphs with fast localized spectral filtering. In Advances in Neural Information Processing Systems, pp. 3844–3852, 2016.\n[2] A. Brunton, T. Bolkart, and S. Wuhrer.  Multilinear wavelets: A statistical shape space for human faces. In European Conference on Computer Vision, pp. 297–312, 2014a.\n\nAfter rebuttal:\nThe current version of the paper still needs significant amount of work regarding the experimental part.","rating": "1","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Navya"],"models": []}}
{"id": "H1xNccu5OE","review": "Major comments and summary: \n\n  Overall, I strongly recommend this paper as a workshop paper, but I think it needs more work to become a great conference paper.  The problem is well-motivated and seems very important to me.  My biggest issue is that I feel like one could go much \"further\" with this idea - and it shows empirically - the only improvement from using the noisy labels is to go from 89% -> 90%, which I would say is only a small improvement.  If this problem gets solved I think you should be closer to 95% on CIFAR10.  \n\nI have a few ideas on how to make the algorithm better: \n  -On the noisily labeled data points $D_U$, replace the (noisy) label y_i with a soft-label which reflects the underlying noise process.  For example if you're told that y = 1, but the noise process corrupts to a new random label with 50% chance, then make the new soft-label [0.5/9, 0.5, 0.5/9, 0.5/9, ...].  This is like changing the label randomly each time you see the example but it will have much less variance!  This is just for the L_robust loss.  \n  -Alternatively, what if you replace L_robust with a partial likelihood which reflects what q is?  For example, if each data point is corrupted with 50% chance - then just try to encourage the probability of that point to be >=50%.  Don't try to push the probability on that label up to 100% (in practice this would look like a hinge loss - I think).  \n  -Is \"L\" loss cross-entropy or mean squared error?  For semi-supervised consistency loss I think it helps to use mean squared error instead of cross entropy (cross-entropy is very harsh if the prediction and the label strongly disagree when the label is confident).  \n\nOther Comments: \n\n  -I think the bi-quality data setting is extremely interesting.  However, the introduction could perhaps benefit from giving some more concrete examples of bi-quality data.  I think model-based RL could be one interesting example (the model gives very noise rollouts and the environment gives high-quality rollouts).  \n\n  -The paper motivates the algorithm by saying that the value of q might be unknown.  However, even if q were known, would it be trivial to fuse SSL and RLL?  There would still be a tradeoff between how much to trust the labels from the untrustworthy set and how much to rely on the trustworthy data.  \n\n  -Explicit hyperparameter to mix between the robust objective the semi-supervised objective.  Would it be too hard to learn this hyperparameter?  \n\n  -Is the value of \"q\" and the noise corruption process known?  It doesn't seem to be used in the algorithm block.  If q or the corruption process were known, would you have a way of using it.  \n\n\nPaper reading notes: \n-Deep learning requires labeled data which is both large and clean.  \n\n-This paper proposes to study the robust learning and SSL learning problems jointly - in that we assume that we have \"bi-quality data\" - where a small number of examples are cleanly labeled and a large amount of data has noisy labels.  \n\n-Using just robust learning on all the data performs worse than SSL.  \n\n-Paper proposes to combine SSL and RLL.  \n\n-D_T is the trusted data (clean, labels always correct).  D_U is the untrusted data (labels are sometimes wrong).  \n\n-P refers to the fraction of the total data which is trusted.  \n\n-Also have a score q, such that q=0 means the labels are totally random and q=1 means the labels are perfectly clean.  \n\n-From this perspective SSL corresponds to the q=0 case (where the untrusted data has no label information).  \n\n\nMinor comments: \n\n  -Would be good to also cite this newer paper focusing on mixup and SSL (only published recently, but spells the SSL stuff out more clearly): https://arxiv.org/pdf/1903.03825.pdf\n\n  -\"To realize this goal, whose quality might be 0, we propose...\" -> this line doesn't make sense to me.  Typo?  \n\n  -Please put more space into the algorithm 1 block.","rating": "0","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Navya"],"models": []}}
{"id": "tKNRzG6fyi","review": "The paper proposes a layer-wise approximation to a neural network’s Fisher information matrix, to avoid multiple costly backward passes, which are usually required to compute (an approximation of) the Fisher.\n\nI recommend to accept the paper, since it enables new and more efficient variations of existing approximate natural gradient descent methods.\n\nSome suggestions to improve the paper:\n- Section 1 (Introduction), line 31, footnote: If I’m not missing something, Section 2 does not mention the differences to [17] anymore.\n- It would be interesting to hear some intuition on the effect of discarding the last term in Equation (4).\n- It would interesting to also show how Fishy can be applied to K-FAC and how the resulting method is different from Fishy + Shampoo.\n- The computational complexity of Fishy and regular Shampoo/K-FAC should be compared. Additionally, empirical wall-clock time measurements would be nice.\n- Regarding the experiments, you state that “We find significant improvements over Shampoo with Fishy, especially for the tanh activations”. However, this only seems to hold for the train loss in MNIST with tanh activations experiment. Also, it is interesting that True Fisher does not seem to consistently improve convergence/final evaluation metrics, which invites further investigation. Finally, it probably makes sense to tune the hyperparameters of Shampoo and Fishy separately (cf. line 118).","rating": "0","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Navya"],"models": []}}
{"id": "g2Gko8BDD","review": "This paper presents a method to create a neuroimaging signature of an individual scan. The methods uses SIFT features and a kNN classifier. This paper validates the method based on 8152 scans. The authors analyse the correspondence between the signatures of all pairs of scans to analyse their correspondence. The results show the overlap measurement of the signatures is different between pairs of scans from the same individuals, from twins, from siblings and unrelated scans. \n\nI find the analysis and its results very impressive and interesting. However, I am wondering if MIDL is the appropriate venue to present this, as no deep learning methodology is used.","rating": "0","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Navya"],"models": []}}
{"id": "-McbUfD4UvY","review": "This is an ambitious paper that could offer some promise. Learning to generalize compositionally is an intriguing idea. However, I think the work needs substantially more development along a number of axes: \n\nSCS vs. purely continuous representations\n---------------\n\nThis may partly be an issue of communication (see below), but it's not entirely clear to me why the authors don't consider fully continuous representations in addition to OHE or SCS ones. The argunments of prior work referenced below (e.g. Chalmers, 1990) show that continuous representations can be effective for compositional generalization, as do many of the empirical studies the authors exhibit in RL, as well as the successes of large language models, etc. All modern approaches rely on continuous representations, and often achieve some degree of effective compositional generalization. It certainly seems important to run a continuous-representation comparison condition to justify the value of the SCS encoding.\n\nIndeed, the authors (accurately) criticize OHE for not being able to extend without changing the shape of the vector representations. However, their SCS representation is subject to limitations as well, so far as I can see: it cannot adapt to more latent dimensions than are pre-allocated, and I suspect that it also would fail to extrapolate to dimensions with a larger $d(i)$ than the architecture saw in training (although this is speculation, and perhaps something that could/should be evaluated experimentally in a revised version of this work). \n\nFurthermore, constructing an SCS seems to require a) that the semantics of the task be simple enough to be stated as discrete values along a small number of latent dimensions, and b) that we be able to construct these representations from the inputs (which might generally be e.g. images). Indeed, continuous representations are used in basically all modern applications for just this reason---we don't know how to construct an appropriate discrete representation for the space, and it's not clear if one even necessarily exists. Thus, in order for this method to be applicable beyond toy settings, it would be useful for the paper to explain how this might work. Ideally, the authors would *demonstrate* such success on existing benchmarks for compositional generalization (see below). \n\nClarity\n------------\n\nNeither the paradigm nor the results are stated in sufficient detail or clarity to understand how to build upon the work. For example: \n\n* The rule-based speaker the authors use to evaluate uses compositional language, but what exact form does this take? This should be clearly stated, since it is central to understanding the main experimental evaluations. \n* The only description of how the agent's take communication actions is that the output is \"a distribution over 29 actions, which corresponds to an action space combining both the language output and the decision output.\" How are these combined? \n\nThese kind of details can make a substantial difference in RL, and should certainly be reported. Furthermore, there are many other clarity issues, e.g.:\n\n* The paper spends a great deal of time discussing issues like \"object-centric\" representations vs. \"stimulus-centric\" ones, but I do not see how such notions could apply to the SCS representation as suggested by the paper. Or are the stimuli represented differently as inputs to the agents? Even this detail of the experimental evaluation is not clearly stated.\n* What the paper calls a \"One-Hot Encoding\" OHE seems to really be a multi-hot encoding (that is, it is one-hot along each dimension, but a single stimulus has multiple  1s within it). However, this is not clearly explained, and it's only really communicated if the reader carefully examines figure 1. A point like this should be clarified in the text. \n\n\nEvaluation\n------------\n\nThe results of this approach are barely-above chance performance (29% at most, where chance is 25%). While this may be interesting in some sense, it is not a particularly compelling endorsement of the SCS representation.\n\nIn addition, even this experimental evaluation is limited. First, the results suggest poor tuning of the  hyperparameters: the DNC performs worse than an LSTM memory, but a DNC *contains* an LSTM in the controller, and so should almost certainly perform better (even if just by learning to ignore the memory part). Of course, these are challenging architectures to optimize, but this means that the claims of the paper are suspect: perhaps the performance is low for reasons of poor training, rather than inherent difficult of the task. Furthermore, the authors perform effectively no ablation studies, or further evaluation of which aspects of the approach matter. All of these would be necessary to make a valuable contribution to the literature.  \nFinally, it would be much more interesting if the authors could demonstrate the value of their approach on existing compositional generalization challenges such as SCAN or gSCAN (which they cite), where baselines do exist. Could SCS offer benefits in these more complex settings? This would force the authors to grapple with some of the challenges outlined above about using SCS in environments with slightly more complexity (but still much simpler and more amenable to structured representations than the stimuli used for reference games in cognitive psychology). \n\nLiterature\n-----------\n\nAs the authors note, compositional generalization has been a major topic of research for some time. The work needs to be better situated within this broader literature, to clarify its contributions. For example, the author's points about discrete vs. continuous representations overlap with long-standing replies to Fodor & Pylyshyn's claims about compositionality. For example, see Chalmers (1990) or Smolensky (1987), as well as much of Smolensky's subsequent research (e.g. Smolensky, 1990; McCoy et al., 2018). This paper would convey its contributions much more clearly if it were situated within this broader literature. \n\nI don't think it's as essential to engage with with, but the authors may also be interested in Santoro et al. (2021), an opinion piece which discusses a number of issues related to several aspects of this paper, including discussions of discrete vs. continuous symbol representations, and arguments that communication provides a unique path to instilling symbols. \n\n\n\n\n\n\n\nReferences\n-------------\n\nChalmers, D. (1990, July). Why Fodor and Pylyshyn were wrong: The simplest refutation. In Proceedings of the Twelfth Annual Conference of the Cognitive Science Society, Cambridge, Mass (pp. 340-347).\n\nSantoro, A., et al. (2021). Symbolic behaviour in artificial intelligence. arXiv preprint arXiv:2102.03406.\n\nSmolensky, P. (1987). The constituent structure of connectionist mental states: A reply to Fodor and Pylyshyn. Southern Journal of Philosophy, 26, 137-161.\n\nSmolensky, P. (1990). Tensor product variable binding and the representation of symbolic structures in connectionist systems. Artificial intelligence, 46(1-2), 159-216.\n\nMcCoy, R. T., Linzen, T., Dunbar, E., & Smolensky, P. (2018). RNNs implicitly implement tensor product representations. arXiv preprint arXiv:1812.08718.","rating": "0","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Navya"],"models": []}}
{"id": "Bl5uU8Yr-b","review": "## Contributions and Relevance\n\nThis paper's core contribution is a novel dataset, mini-ARC, akin to ARC (Abstraction and Reasoning Corpus), consisting of 5x5 input/output image pairs, for tasks such as program induction. Annotations include human solve traces and human difficulty ratings. The size limitation was imposed to lessen the burden on computational resources. \nThe dataset is easily employable for neuro-symbolic AI; causality was not discussed. There might be an unmentioned connection to causality via the collected traces of human solvers; this could be expanded upon. With minor changes, I think this paper is quite relevant to the workshop.\n\n## Clarity\n\nThe text is generally mostly clear and well-written; exceptions include 2 apparently unintended negations.\n* 1.2: first paragraph list lacks category \"color\"\n* In subsection on color: s/independent of/dependent on\n* In 1.4, the ref{sec:category} broke.\n* 3.1: s/cannot not/ can not\n\n## Strong points that could use more detail\n\nAs the paper mostly expands upon ARC, its main idea is not novel; the dataset it introduces is though. I can not judge significance of the presented dataset; for that a comparison with all small (e.g. < 7x7) problems in ARC might be helpful, as well as a discussion of runtime/comutational demands on methods currently applied on ARC or comparable problems to justify the size limit. As far as I can tell, miniARC adds significant numbers of samples to ARC, at least in the low-resolution segment, and expands upon ARC by including human solve traces. These strengths could be expanded upon.\n\nThe paper could profit from some background on the original ARC. Citation [2](Chollet 2019) should be placed more clearly and prominently. Chollet 2019 talks a lot about theoretical reasons for the individual tasks, are those considerations mirrored in miniarc?\n\n## Weak points / criticism:\n\n2.3/2.4 - these are essentially dropped features, right? Discussion of such can be much more succinct, as they have no bearing on the dataset and just explore the design space.\nWhile those are interesting discussions, they'd need to be longer and supported by additional data to be worthwhile. Better traces and human solver behavior is interesting; the salient question here would be how can we get this regardless of the obstacles outlined in 2.3/2.4?\n\nPossible deanonymization of authors via footnote 3, page 4?\n\nFig 4: The red X and green check mark symbols suggest that one trace was wrong, when it arrived at the same solution. I'd suggest changing the symbols. E.g. the two traces in (c) could use orange and green background colors to map them to (b). I'd also refrain from implicitly restricting the methods to be applied to the data (... will be the next challenge) and mark that as one future avenue (out of many).\n\nGiven more space (after the concerns mentioned above) I would appreciate more discussion on the varied possible methods that are enabled by this dataset.","rating": "0","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Navya"],"models": []}}
{"id": "Sye8z12W5E","review": "This paper proposes a very interesting idea, using the learning-to-learn framework to learn an attacker. I find this idea very novel in the literature and in retrospect, very natural. Furthermore, I believe using L2L framework to this adversarial setting is very promising as we can naturally generate many samples to fit L2L framework.\n\nThe experiments also look promising. I think this is a strong paper.","rating": "0","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Navya"],"models": []}}
{"id": "H1S_cEcxM","review": "The authors consider the problem of ultra-low precision neural networks motivated by \nlimited computation and bandwidth. Their approach first posits a Bayesian neural network\na discrete prior on the weights followed by central limit approximations to efficiently \napproximate the likelihood. The authors propose several tricks like normalization and cost \nrescaling to help performance. They compare their results on several versions of MNIST. The \npaper is promising, but I have several questions:\n\n1) One major concern is that the experimental results are only on MNIST. It's important \nto have another (larger) dataset to understand how sensitive the approach is to \ncharacteristics of the data. It seems plausible that a more difficulty problem may \nrequire more precision.\n\n2) Likelihood weighting is related to annealing and variational tempering\n\n3) The structure of the paper could be improved:\n - The introduction contains way too many details about the method \n    and related work without a clear boundary.\n - I would add the model up front at the start of section 2\n - Section 2.1 could be reversed or equations 2-5 could be broken with text \n   explaining each choice \n\n4) What does training time look like? Is the Bayesian optimization necessary?","rating": "0","sentences": [],"comments": null,"annotation_info": {"type": "Manual","humans": ["Navya"],"models": []}}
{"id": "EceCkjTTT-8", "review": "The step decay (constant then cut) strategy is widely used in training deep neural networks. There have been quite a few works dedicated to it, both theoretical and empirical. This paper does a great job in synthesizing the related works, identifying their contributions and limits, and pointing out where this work stands.\n\nThis strategy is of course not novel, nor are the rates they obtained in any setting, but providing the theoretical guarantees for the step decay schedule as proved in this paper for various settings are new. In addition, the sampling scheme of being inversely proportional to $\\eta_t$ is novel and very intuitive in giving weights to last iterates.\n\nSome of my concerns are:\n1. The assumption on the upper-boundedness of $f$ is worrisome, especially when it appears in the bound with the term $f_{max} - f^*$ showing that if it is unbounded then the bound is trivial. If the domain is bounded then this assumption would be more natural but for unbounded domain, this assumption seems too restrictive. Note that other results typically include a term $f(x_1) - f^*$ which is fine as $f(x_1)$ can be manually selected and easily be bounded. Looking at the proofs, however, the term $f_{max} - f^*$ is not just a simple upper-bound of $f(x_1) - f^*$ and the assumption on $f_{max} < \\infty$ is indispensable.\n2. In the paper on exponential step sizes [Li et al., 2020], they also studied the cosine step sizes and showed that it also performs really well and surpasses the exponential step sizes and the step decay in some settings. Also, in their paper, the step decay schedule has the flexibility of using different inner-loop sizes thus should be different from your definition in this paper. Hence, I would like to see a comparison with it.\n3. I see that you answered Yes to Checklist 3.a, but could not find your codes on implementing the algorithm and comparing with other optimizers, apart from download link to datasets you used.", "rating": "0", "sentences": [{"sentence_type": "positive", "sentence": "This paper does a great job in synthesizing the related works, identifying their contributions and limits, and pointing out where this work stands."}], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Rafael"], "models": [""]}}
{"id": "I4MlIhkjRD2", "review": "Overall I like the theoretical results the authors obtained. But the write-up and presentation can be further polished and improved. For example, there are still quite a few typos or grammatical errors in the submission. The technical proof is quite dense, which is evident from the length of the supplementary materials. A guide for readers on how to read the supplementary materials is necessary for this paper to have more impact if they hope machine learning researchers could use their tools to perform honest statistical inference.\n\nIn terms of technical content, the authors propose a condition called $\\mathbb{C}^{3}$-approximability and use a surrogate $\\mathbb{C}^{3}$-function sequence as an intermediate step to show bootstrap consistency. This is an important assumption but the authors delay the discussion of this assumption to Appendix C. I understand due to NeurIPS's page limit there has to be a trade-off on what to be included in the main text. But I am worried that the supplementary materials will eventually be buried under most readers' desk drawers. I also hope that the authors can provide one example (if they could) that violates Assumption 1 yet the bootstrap consistency still holds.\n\nThe authors advocate that the strength of their paper provides tools to show bootstrap consistency for general nonlinear functionals beyond what people have done case by case. But it would be useful if the authors could recover several established results as special cases of their master theorem.\n\nIn addition, the authors admit that their technical results are inspired by Chatterjee's work. Then what technical challenge do they need to solve? Are the proofs, even though very long, standard or requiring some innovations? These should be made clearer.\n\nSome extra comments:\n\n(1) Lines 47-49: \"Notably, this intuition has already been exploited to show that the bootstrap method is consistent in particular applications in the econometric literature, such as the construction of uniform confidence bands.\" This sentence should be accompanied by relevant citations.\n\n(2) Lines 88-89: \"We note that the limiting distribution of those statistics are in general not Gaussian [22].\" (i) \"distribution\" should be \"distributions\"; (ii) I understand that the authors try to tell readers Gaussian limiting distributions for these statistics are established under specific conditions, e.g. $U$-statistics with dominating first-order terms in Hoeffiding's decomposition. But this and later sentences seem to be quite disconnected from previous sentences without adding more contexts. For example, Xiaohui Chen's work [15] is about Gaussian limiting distribution for high-dimensional $U$-statistics.\n\n(3) I would say 70% of NeurIPS submission does not contain such a long supplementary material filled with technical lemmas and proofs. One presentation style that I found very useful for technical papers in statistics and theoretical machine learning is to draw a diagram indicating which technical lemma was used to prove Theorem X and how different technical results are connected.\n\n(4) Lines 146-147: \"Given that these results typically require stronger conditions on the statistic and many times Gaussian limits.\" What does \"many times Gaussian limits\" mean exactly?\n\n(5) Line 231: \"the test statistics is computed on\" should be \"the test statistics are computed on\"\n\n(6) A recent paper (first appeared in 2020) by Vladimir Koltchinskii (https://arxiv.org/pdf/2011.03789.pdf) considered a relatively similar setup, but he focused on Gaussian limit and root-n parametric theory for estimating nonlinear statistics. I suggest the authors also cite this paper and discuss the connection and difference between their work and Koltchinskii's. \n\n(7) No discussion is written. The authors are recommended to write at least some sentences on future works or caveats... This is also strongly encouraged by NeurIPS.\n\nGiven above, I temporarily give the authors a score of 5 but if they could convince me with their rebuttal I am willing to change my evaluation. Also, this paper seems to fit better in a journal in statistics, JMLR or a TCS-type conference. The page limit of NeurIPS may have a negative impact on this paper.", "rating": "2", "sentences": [{"sentence_type": "2", "sentence": "Then what technical challenge do they need to solve? Are the proofs, even though very long, standard or requiring some innovations? These should be made clearer."}], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Rafael"], "models": [""]}}
{"id": "Hkg4oaLsn7", "review": "This paper addresses long-text generation, with a specific task of being given a prefix of a review and needing to add the next five sentences coherently.  The paper proposes adding two discriminators, one trained to maximize a cosine similarity between source sentences and target sentences (D_{coherence}) and one trained to maximize a cosine similarity between two consecutive sentences.  On some automatic metrics like BLEU and perplexity, an MLE model with these discriminators performs a little bit better than without.\n\nThis paper does not include any manual evaluation, which is critical for evaluating the quality of generated output, especially for evaluating coherence and cohesion.  This paper uses the task setup and dataset from \"Learning to Write with Cooperative Discriminators\", Holtzman et al., ACL 2018.  That paper also includes many specified aspects to improve the coherence (from the abstract of that paper \"Human evaluation demonstrates that text generated by our model is preferred over that of baselines by a large margin, significantly enhancing the overall coherence, style, and information of the generations.\").  But this paper:\n--Does not compare against the method described in Holtzman et al., or any other prior work\n--Does not include any human evaluations, even though they were the main measure of evaluation in prior work.\n\nThis paper states that \"To the best of our knowledge, this paper is the first attempt to explicitly capture cross-sentence linguistic properties, i.e., coherence and cohesion, for long text generation.\"  There is much past work in the NLP community on these.  For example, see:\n \"Modeling local coherence: An entity-based approach\" by Barzilay and Lapata, 2005 (which has 500+ citations). \nIt has been widely studied in the area of summarization, for example, \n\"Using Cohesion and Coherence Models for Text Summarization\", Mani et al., AAAI 1998, and follow-up work.\nAnd in more recent work, the \"Learning to Write\" paper that the dataset and task follow from addresses several linguistically informed cross-sentence issues like repetition and entailment.  \n\nThe cosine similarity metric in the model is not very well suited to the tasks of coherence and cohesion, as it is symmetric, while natural language isn't.  The pair:\n\"John went to the store to buy some milk.\"\n\"When he got there, they were all out.\"\n\nand \n\n\"When he got there, they were all out.\"\n\"John went to the store to buy some milk.\"\n\nwould have identical scores according to a cosine similarity metric, while the first ordering is much more coherent than the second.\n\nThe conclusion says \"we showed a significant improvement\": how was significance determined here?\n", "rating": "1", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Rafael"], "models": [""]}}
{"id": "rygT8T4Io4", "review": "The paper describes a general software suite for deploying executables of quantum algorithms to specific hardware.  It leverages both AI Planning and Constraint Programming to form a general solution that allows the user to specify the hardware architecture and algorithm, which is then automatically compiled to the hardware.  A GUI shows the goal graph, machine graph and Quantum Circuit Compilation in a format that can aid the user in understanding the way in which the algorithm is running on the hardware.   A key benefit of this software suite is its generality due to its use of general problem solving techniques, which means that the same software can be applied to many different algorithms and, perhaps more importantly, many different hardware types.  \n\nThe paper is easy to read, clearly well motivated, and generally of high quality.  The paper would be a great addition to the SPARK workshop.  \n\nMy only minor suggestion is to switch to an Author (Year) style of citation when a citation refers to a work.  For example, prefer \"Author et al. (YEAR) discuss ...\" to \"(Author et al., YEAR) discuss\".\n\np3: cross-talks constraints -> cross-talk constraints", "rating": "0", "sentences": [{"sentence_type": "positive", "sentence": "The paper is easy to read, clearly well motivated, and generally of high quality.  The paper would be a great addition to the SPARK workshop.  "}], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Rafael"], "models": [""]}}
{"id": "B1gpjwjOOS", "review": "The authors investigate the role of entropy maximization in SAC and show that entropy regularization does not do what is usually thought: in the examples they investigate, where the output of the policy network needs to be squashed to fit in the action space domain, squashing would result in having only action at the boundaries, but entropy regularization maintains some intermediate values, hence exploration. From this insight, the authors replace entropy regularization by a simpler normalization process and show equivalent performance with their simpler Streamlined Off-Policy (SOP) algorithm. Then they introduce a second \"Emphasizing Recent Experience\" mechanism and show that SOP+ERE performs better than SAC.\n\nA good point for the paper is that the entropy regularization  study is very nice, more papers in the field should show similar detailed analyses of internal processes. But the paper suffers from a few serious weaknesses:\n\n- The TD3 mechanism goes beyond the Double Q-learning (or DDQN) mechanism of Van Hasselt et al: it takes the min over two critics. This should be explained properly.\n- the title, abstract and introduction insist more on SOP, but performance improvement seem to result more from ERE. If this is possible, studying the performance of SAC + ERE would disambiguate the relative contribution of both mechanisms.\n\nAbout gradient squashing issues, the authors main mention de gradient inverter idea from this paper:\n\n@article{hausknecht2015deep,\n  title={Deep reinforcement learning in parameterized action space},\n  author={Hausknecht, Matthew and Stone, Peter},\n  journal={arXiv preprint arXiv:1511.04143},\n  year={2015}\n}\n\nThe authors should also probably also cite (and read the latest arxiv version of):\n@inproceedings{ahmed2019understanding,\n  title={Understanding the impact of entropy on policy optimization},\n  author={Ahmed, Zafarali and Le Roux, Nicolas and Norouzi, Mohammad and Schuurmans, Dale},\n  booktitle={International Conference on Machine Learning},\n  pages={151--160},\n  year={2019}\n}\n\n\nMore local points:\n- \"without performing a careful hyper-parameter search\": so how did you choose these hyper-parameters? I see what you mean, but this is a very vague and slippery statement.\n- I do not find the 23*4 images in Appendix B much useful\n- Fig 3 seems to be repeated in Fig 4. Can't you just remove Fig 3?", "rating": "1", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Rafael"], "models": [""]}}
{"id": "ByeQl_FmqN", "review": "This was submitted as a workshop paper but I think with just a little more detail it would be a strong contender for a regular conference paper. The authors show how a combination of primal-dual estimation for Lagrangian problems with neural ODEs for density estimation can be used to solve the regularized optimal transport problem on high dimensional spaces. They show with numerous experiments on challenging domain-to-domain problems that the joint probability distribution they learn can be used for meaningful joint generation tasks, such as pix2pix-like style transfer in the image domain. Overall this paper was a pleasure to read - clearly motivated, tackling an important problem and using state-of-the-art methods to achieve it. I would have appreciated a little more discussion of the stability of gradient ascent/descent for solving the Lagrangian multiplier formulation of their objective, as I have found these kinds of problems very hard to work with in a stochastic domain, but overall the paper was compelling and timely.", "rating": "0", "sentences": [{"sentence_type": "positive", "sentence": "This was submitted as a workshop paper but I think with just a little more detail it would be a strong contender for a regular conference paper."}, {"sentence_type": "positive", "sentence": "Overall this paper was a pleasure to read - clearly motivated, tackling an important problem and using state-of-the-art methods to achieve it."}, {"sentence_type": "positive", "sentence": " would have appreciated a little more discussion of the stability of gradient ascent/descent for solving the Lagrangian multiplier formulation of their objective, as I have found these kinds of problems very hard to work with in a stochastic domain, but overall the paper was compelling and timely."}], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Rafael"], "models": [""]}}
{"id": "djRsDQUh2Yh", "review": "This paper proposes to use uncertainty estimates from an ensemble of action-values, to provide a weighting on the updates in Q-learning. The main idea is to use the sigmoid of the negative of this uncertainty in the next state, to produce a weighting between 0.5 and 1 to downweight updates with high uncertainty targets. This uncertainty estimate from the ensemble is also used to improve exploration, in a combined algorithm called Sunrise that leverages learning an ensemble in these two ways. \n\nThe idea of using weighted Bellman updates is, as far as I know, novel. The evidence for the idea, however, needs more work. First, the weighted update in Eq (4) is not motivated from first principles. Second, the empirical evidence is weak because the experiments highlighting the role of the weighting do not demonstrate significant differences.  \n\nThe first issue is the justification for the approach. The ensemble of Q-learning agents is trained using the weighting, derived from that ensemble. There are natural questions as to the interaction between the ensemble uncertainty estimates and the ensemble estimates. Does it result in any instability? What is the final point of convergence? Does it change the solution?\n\nBut, one could argue that that is not much of a problem, since the weighting w(s,a) is always between 0.5 and 1, so it is not that skewed. Then the question arises how much it is helping, and why this small reduction in weight helps. This is particularly important to ask, considering the algorithm requires an ensemble to be learned, with subsets of data used for each action-value. There is a lot of effort expended for that weighting.   \n\nThe experiments then do include ablations, to examine the effect of these weightings. Unfortunately, the results are inconclusive. The experimental time spent must have been large to get all the results in this paper, across so many environments and algorithms. But, the ablations themselves are not sufficiently in-depth to provide insight into the idea and algorithm. The results in Figure 2 are key, since that figure examines Sunrise with and without the weighting. Due to the variance across runs, with only 4 runs, there are large standard errors (and so even larger 95% confidence intervals); it is hard to conclude that weighting is helping. The additional results in Figure 5 in the appendix have a similar issue.\n\nThe results in Figure 3, which motivate the exploration utility, are more clear in Cartpole. This provides some motivation for learning ensembles, so they can be used for exploration. But, this exploration approach with ensembles is an existing method. The main novelty in this work is the weighting. \n\nI highly recommend taking a few domains and carefully studying the impact of the weight. More runs would help for significance, as well as parameter sensitivity analysis to gain insight into the generality of the improvement. Sometimes performance gains are from hyperparameter tuning, rather than from the utility of an idea; here, you really want to know if and why this weighting improves performance. \n\nAs a more minor comment, Sunrise is pitched as combining three ideas for using ensembles: your weighting, bootstrapping and UCB exploration. However, I see Sunrise as combining two ideas: weighting and UCB exploration. The Bootstrap DQN approach gives you a way to learn your ensemble of bootstrap models, so that it provides a useful uncertainty estimate. Given that ensemble, you can then use it to compute a weighting and optimistic action. It would be more clear to separate it out that way, rather than saying \"Furthermore, since our weighted Bellman backups rely on maintaining an ensemble, we investigate how weighted Bellman backups interact with other benefits previously derived from ensembles: (a) Bootstrap; (b) UCB Exploration.\" The bootstrap is arguably not a benefit, but an approach to obtain confidence (uncertainty estimates). \n \nMinor comments:\n1. Bootstrap DQN is listed under \"Ensemble Methods in RL\", rather than under \"Exploration in RL\", but is it an exploration approach.\n2. \"Recently, Kumar et al. (2020) showed that this error propagation can cause inconsistency and unstable convergence.\" The terms inconsistency and unstable convergence should be explained, since they seem like technical terms. \n3. Bellman backup seems to be used to describe the squared error to the expectation over next action, in Equation (2), and then to a stochastic sample of the action in (4). Which is it?\n4. What is meant by the signal-to-noise in Q-updates? \n5. A natural baseline to include is to tune an agent that uses random weights between 0.5 and 1 in the update, but keeping other parts of Sunrise the same. The ablation removes the weighting all together, which is also important to include. But, it's worthwhile observing if random weights performs similarly, especially if that agent is tuned. \n\n------------ Update\nThank you for the clear reply. Unfortunately, I remain concerned about the significance of experiments. I mentioned above that 4 or 5 runs is typically not enough, and because the standard errors are overlapping, the differences could be due to chance. The addition of a result with 10 runs is a good step. But, as part of the reply, the authors state: \"Figure 3(a) shows the learning curves of all methods on the SlimHumanoid-ET environment over 10 random seeds. First, one can not that SUNRISE with random weights (red curve) is worse than SUNRISE with the proposed weighted Bellman backups (blue curve). Additionally, even without UCB exploration, SUNRISE with the proposed weighted Bellman backups (purple curve) outperforms all baselines. This implies that the proposed weighted Bellman backups can handle the error propagation effectively even though there is a large noise in reward function.\" However, if you look at this figure, the error bars all still overlap. 10 random seeds is still not enough. \n\nI am also not confident that the issue will be remedied, as the authors additionally state in the rebuttal: \"we believe that SUNRISE is evaluated in a broad collection of domains in the RL literature and the performance gap is also noticeable.\" An insignificant gap across many domains does not tell us anything. Actually, if you take the runs and tried to do significance tests by pooling all the runs across environments, then maybe the result might actually be significant. But, of course, there will be higher variance due to differences in the environments, so it is not obvious this would be true. Nonetheless, this could be a natural next step.", "rating": "1", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Rafael"], "models": [""]}}
{"id": "FERFTRjLBDm", "review": "The authors study the identifiability of latent variables under observation of high-level variables that are generated via a polynomial function. The central result is identifiability of the latents up to affine functions under certain assumptions on the polynomial as well as support of the latents.\n\nThe paper is clear to read and the main ideas are conveyed quickly.\n\nA major flaw with the writing is the notation of interventional data. In fact, the authors use $z_{-i} \\sim \\mathbb{P} (Z_{-i}\\mid z_i = z^*)$, which does *not* denote the interventional data when $z_i$ is intervened via *do*, but simply the conditional probability after observing $z_i$ as a constant. The main outline of the paper as well as the motivation are thus misleading, even if it might have been a non intentional error (still a very major one). Upon initial reading, this mathematical mistake led me to question the correctness of the results. Nonetheless, the most important assumption seems to actually be the non-emptiness of the support of the latent variables (in particular Ass. 4) which can hold both for actual interventional data as well as observational data where we only observe a constant $z_i$. \n\nWhile I haven’t checked the proofs in detail, on first glance they seem to be correct and rely on functional analysis, in particular Theorem 1.\n\nI believe the paper would much benefit from even just a toy example to show how the latents can be learned, and it shouldn’t be too involved to code this down for a small example. As such, an important question and discussion for the results are the practicality — could a polynomial decoder give good empirical results on real-world data? \n\nSince I cannot give more nuanced recommendations other than reject or accept, I decided to give a score of two as I believe the mistake can be fixed quickly and this is a workshop where we encourage a wide range of discussion, and I strongly advise the authors to correct the notation and make sure that the theoretical results still hold. Also, a discussion like the one mentioned above could be of value for the paper and discussions in the workshop.", "rating": "0", "sentences": [{"sentence_type": "positive", "sentence": "The paper is clear to read and the main ideas are conveyed quickly."}, {"sentence_type": "positive", "sentence": " I decided to give a score of two as I believe the mistake can be fixed quickly and this is a workshop where we encourage a wide range of discussion, and I strongly advise the authors to correct the notation and make sure that the theoretical results still hold"}, {"sentence_type": "positive", "sentence": "Also, a discussion like the one mentioned above could be of value for the paper and discussions in the workshop."}], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Rafael"], "models": [""]}}
{"id": "S1xUzwOgz", "review": "This paper studies the problem of multi-label learning for text copora. The paper proposed a latent variable model for the documents and their labels, and used spectral algorithms to provably learn the parameters.\n\nThe model is fairly simplistic: the topic can be one of k topics (pure topic model), based on the topic, there is a probability distribution over documents, and a probabilistic distribution over labels. The model between document and topic is very similar to previous pure topic models (see more discussions below), and because it is a pure topic, the label is just modeled by a conditional distribution.\n\nThe paper tried to stress that the model is different from Anandkumar et al. because the use of \"expectations vs. probabilities\", but that is only different by a normalization factor. The model defined here is also very strange, especially Equation (2) is not really consistent with Equation (7). \n\nJust to elaborate: in equation (2), the probability of a document is related to the set of distinct words, so it does not distinguish between documents where a word appear multiple times or only once. This is different from the standard bag-of-words model where words are sampled independently and word counts do matter. However, in the calculation before Equation (7), it was trying to compute the probability that a pair of words are equal to v_i and v_j, and it assumed words w_1 and w_2 are independent and both of them satisfy the conditional distribution P[v_i|h = k], this is back to the standard bag-of-words model. To see why these models are different, if it is the model of (2), and we look at only distinct words, the diagonal of the matrix P[v_i,v_i] does not really make sense and certainly will not follow Equation (7). Equation (7) and also (9) only works in the standard bag-of-words model that is also used in Anandkumar et al. (the same equations were also proved).\n\nThe main novelty in this paper is that it uses the label as a third view of a multi-view model and make use of cross moments. The reviewer feels this alone is not enough contribution.", "rating": "1", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Rafael"], "models": [""]}}
{"id": "H1eUlfF_tH", "review": "This paper studies the teacher ensembles setting for differentially private learning. In this setting, each teacher holds part of the training set and trains a local model. The student uses unlabeled examples to query teacher model. Then the student trains a model from scratch using the examples labeled by teachers.\n\nIn order to make the labeling process differentially private, previous work uses noisy argmax mechanism. Each class of label is assigned with a count number. The student first queries the same example to multiple teachers. To guarantee differential privacy, the counts are perturbed by noise before releasing. Then, because of the post-processing property of differential privacy, the argmax operator on such noisy counts are still differentially private.\n\nThis paper proposes to add a constant c to the largest count before perturbing and releasing the counts. The authors argue this would improve the accuracy of the noisy argmax operator and yield the same privacy loss as previous approach. However, adding a constant c would increase the sensitivity and therefore degenerates the privacy guarantee. The added noise cannot guarantee the privacy if all others are the same as previous work. To see this clearer, for example, if c=0, then one sample point can at most change the count by 1. If c>0, then one sample point can change the count by 1+c. Because of this, the proposed method cannot guarantee the amount of differential privacy as the paper claimed.\n", "rating": "1", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Rafael"], "models": [""]}}
{"id": "GdtaTecdUus", "review": "The paper proposes the Low-Rank Global Attention (LRGA) module augmented to GNNs to improve generalization power. In particular, given an input graph, the model runs the LRGA module and the GNN module to aggregate node representations on this graph. The input and the outputs of these two modules are concatenated at each layer, followed by a single fully-connect layer (m5) to produce input for the next layer. The LRGA module applies the self-attention mechanism [1], but replacing the softmax layer by the global normalization. \n\nPros. The results are promising.\n\nCons. \n\ni) The motivation to propose LRGA by replacing the softmax layer in the self-attention mechanism [1] by the global normalization is not well enough. The graph self-attention networks (such as [3,4]) show competitive results, and they can be applied for large graphs. Thus, LRGA is incremental and not technically sound.\n\nii) The paper does not discuss the most closely related work, Dual Graph Convolutional Networks (DualGCN) [2]. The architecture LRGA+GNN is similar to DualGCN. Changing from using GCN to another GNN is straightforward, thus the work lacks novelty.\n\niii) The roles of m1, m2, and m3 are similar to the query, key, and value matrices in the self-attention mechanism, respectively. But why LRGA employs m4? m4 does not have a specific role as it can be placed outside LRGA and put inside Equation 1. \nNote that [5] shows that using the vector concatenation/sum-pooling/LSTM over different layers can improve the performance. But, I do not see the role of X^l in Equation 1. What is it?\n\niv) Given the same GNN module with the same hidden size, the proposed LRGA+GNN has much larger parameters than GNN. This limitation restricts to use of deeper layers. \n\nMinor things: Parentheses in Equation 2 should use between m1 and m2, not m2 and m3.\n\n[1] Attention is all you need. NIPS 2017.\n[2] Dual Graph Convolutional Networks for Graph-Based Semi-Supervised Classification. WWW 2018.\n[3] Graph-Bert: Only Attention is Needed for Learning Graph Representations. https://arxiv.org/abs/2001.05140\n[4] Hyper-SAGNN: a self-attention based graph neural network for hypergraph. ICLR 2020.\n[5] Representation Learning on Graphs with Jumping Knowledge Networks. ICML 2018.\n\n=======================\nAfter reading the authors' response:\n\ni. As shown in (new) Table 3 in the revised version, the results of using the global normalization are not better than that of using the softmax layer in the self-attention mechanism. Hence the motivation is not enough.\n\nii. To have the faster computation, we have SGC[1], FastGCN[2]. To have powerful GNNs, we have GIN[3]. Inspired by DualGCN, we can build a new combination (e.g., SGC+GIN) together with using the vector concatenation/sum-pooling/LSTM over different layers [5] to further improve the performance and have a faster computation. That's reason why the novelty of LRGA+GNN is weak.\n\n[1] Simplifying Graph Convolutional Networks. ICML 2019. [2] Fastgcn: Fast learning with graph convolutional networks via importance sampling. ICML 2018. [3] How Powerful are Graph Neural Networks? ICLR 2019. [5] Representation Learning on Graphs with Jumping Knowledge Networks. ICML 2018.\n\nI keep my score unchanged.\n", "rating": "1", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Rafael"], "models": [""]}}
{"id": "IHJeBjXTO-", "review": "## Summary\nThe paper proposes a new neuro-symbolic framework, which can perform learning to translate instructions to grounded robot plans. The addressed problem is to solve planning problems where the input is given as a pair of a state as a depth image and natural language instruction. The task is difficult since the agent needs to understand both the visual state and the natural language instruction and to perform planning on top of them. To address the problem, the paper proposes a new framework, that consists of language reasoner, visual extractor, visual reasoner, and action simulator. Each component processes information so that the entire system can execute symbolic programs defined in the DSL in a differentiable manner to solve the visual planning problem. In the experiment, the proposed approach outperformed a neural baseline, and moreover, it showed strong generalization results in terms of the number of objects and the number of steps of the planning. \n\n## Pros\n- The paper is very well written\n- The paper has a good technical quality, i.e., everything is formulated without technical flaws\n- The proposed approach is novel in the sense that it solves visual planning problems using the NS-CL approach beyond VQA tasks\n- The paper leads to many important applications\n\n## Cons\nThe paper is overall well-written, however, I noticed some minor points that can be addressed.\n- In Fig. 1, Visual Extractor has Relational embeddings, however, it is not explained in the main text. If not the case I'm missing something, please explain it properly somewhere in the paper.\n- In line 107, I think Action simulator should be capitalized as Action **S**imulator\n\n### Questions\n- The limitation of the proposed approach is not discussed in the paper. What would be the limitation?\n- Is the proposed framework capable of parallelized batch computation? The implementation of neural networks in general can process a given batch of examples in parallel on GPUs. For the proposed approach, if the user gives several examples as input, are they processed in parallel? If not, how long does it take to train the proposed model? Would it take longer compared to the neural-based baseline?", "rating": "0", "sentences": [{"sentence_type": "positive", "sentence": "The paper is very well written"}, {"sentence_type": "positive", "sentence": "The paper has a good technical quality, i.e., everything is formulated without technical flaws"}, {"sentence_type": "positive", "sentence": "The proposed approach is novel in the sense that it solves visual planning problems"}, {"sentence_type": "positive", "sentence": "The paper leads to many important applications"}], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Rafael"], "models": [""]}}
{"id": "p8A55kU9xd", "review": "This work presents a deep learning approach for error detection of automated segmentation pipelines. The model uses the previously published pix2pix conditional GAN model to learn the original image from the segmentation. In the test phase the predicted image is compared to the original image using a CNN, and an error map is generated. \nNovel approach, nice initial validation. The methods shows good performance, but some false positives that should still be addressed.\n", "rating": "0", "sentences": [{"sentence_type": "positive", "sentence": "Novel approach, nice initial validation. "}, {"sentence_type": "positive", "sentence": "The methods shows good performance, but some false positives that should still be addressed."}], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Rafael"], "models": [""]}}
{"id": "syOyRxg288S", "review": "The manuscript focuses on clinical natural language processing of electronic health records. More precisely, it addresses a text classification task called information extraction or named entity recognition from these clinical records. Its contributions include developing an embedding model to capture clinical prototypes (CPs), via supervised contrastive learning, and presenting experimental evidence of these learnt CPs capturing attribute-specific semantic relationships and being helpful in subsequent clinical natural language processing task of information retrieval and clustering of clustering of physiological signals. I find this text processing methodology interesting, carefully described, and supplementary to other studies.\n\nHowever, unfortunately, the authors demonstrate limited understanding of the related literature. First, the second and third paragraph of the Introduction section have many sentences that require references to be inserted. Second, and more importantly, the Related work section seems to not capture the key papers and trends of the field (I suggest reading some systematic reviews or surveys on clinical natural language processing, information extraction, and information extraction by, for example, Wendy Chapman, Carol Friedman, and Pierre Zweigenbaum), and, for example, as illustrated by the ImageCLEF and CLEFeHealth evaluation labs, computer vision tends to proceed faster than text analytics (see, e.g., https://www.researchprotocols.org/2018/7/e10961/), and not the other way around as claimed by this section.\n\nIn addition, to feel convinced of the presented experimental evidence, I would have wanted to see statistical significance tests, confidence intervals, effect sizes, or similar presented. I could not find this methodology described in the manuscript or its outcomes, although the narrative repeatedly referred to significant performance gains. Please clarify.\n\nAs my main minor comment, I would like to see a clearer separation of the materials, methods, and experiments sections from results, as well as including clearer justifications of this study design. For example, the aforementioned significance topic has not been addressed sufficiently. Another illustration of somewhat difficult task for the reader is to understand the experimental design as a whole and be convinced of this study being rigorous is the Experimental Results section including a lot of methodological details as opposed to only obtained outcomes.\n\nI also suggest including a conclusion statement as well as embedding more evidence (e.g., evaluation materials and methods plus obtained indicators of performance gains, such as measure values and effect sizes) to convince the reader in the abstract. To continue, please remember to punctuate equations and formulae. \n\nFinally, typically the Methods, Experiments, and Experimental Results sections would have been written using a past tense to emphasise a finished (as opposed to an ongoing) study where materials, methods, and experiments have already been chosen, justified, and completed. Most importantly, please avoid having inconsistent tense in these sections (see, e.g., Section 4.5. using a past tense whereas almost all others are having a present tense).", "rating": "2", "sentences": [{"sentence_type": "2", "sentence": "the authors demonstrate limited understanding of the related literature. "}], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Rafael"], "models": [""]}}
{"id": "1wCEYW9Zkt", "review": "This paper proposes to improve the segmentation quality of boundary areas in medical images. It proposes a loss function that is inspired by Laplacian of Gaussian (LoG) filtering for edge detection. This proposed method is claimed to be light-weighted.\n\nPros: \n1)\tThis loss function is inspired by Laplacian of Gaussian (LoG) filtering, this formulation is suitable for the task of medical image segmentation.\n2)\tThis paper is well-written.\n3)\tThis loss seems to be easier to implement than competing methods and has competitive results.\n\nCons:\n1)\t-- As inspired by LoG, this loss is not novel.\n2)\t-- The paper says it does not require post-processing. However, the convolution operation seems to be post-processing.\n3)\t-- The authors argue that this loss is light-weighted, but there is no quantitative evaluation on the computational cost and time consuming compared with other works.\n4)\t-- The results are only comparable to other methods. It also would be interesting to see the combination of this loss with other methods.\n\nBasically, this is a paper with a simple idea and insufficient experiments. As this is a short paper I recommend weak accept, but it is actually not good enough. I would not be upset if it is rejected.", "rating": "2", "sentences": [{"sentence_type": "positive", "sentence": "This paper is well-written"}, {"sentence_type": "2", "sentence": "As this is a short paper I recommend weak accept, but it is actually not good enough"}, {"sentence_type": "2", "sentence": "I would not be upset if it is rejected."}], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Rafael"], "models": [""]}}
{"id": "ZsRnBwSq8K", "review": "Training large DNNs with stochastic optimizers is effective at relatively small batch sizes. Existing techniques to distribute training across many devices achieve high device utilization, but also lead to an increased effective batch size that yields diminishing returns of the optimization. This paper studies a new schedule which features high device utilization at small batch sizes. This is demonstrated on a 52 billion parameter model on 64 V100 GPUs by comparison to other schemes.\n\n---\n\nMiscellaneous comments:\n\n- L89: Missing space between \"models\" and \"and\"\n- L252: \"we use\" → \"uses\"\n", "rating": "1", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Rafael"], "models": [""]}}
{"id": "HkxRKaJct4", "review": "The authors train ResNet-50 networks on a mixture of ImageNet data and BigGAN samples and show that replacing ImageNet data with BigGAN samples leads to a decrease in performance.\n\nPositives:\n- This \"data-replacement\" experiment is very natural to make, therefore I am glad it was done.\n- The metric for identifying BigGAN failures per class is also an interesting byproduct.\n\nRemarks:\na) \"per-class FID\" is said to be likely \"making the per-class estimates unreliable\" due to high variance. I would still like to see what the per-class FID gives and how the best and worst-performing classes compare with the one found by this method. Indeed, even if per-class FID could perform worse, it has the great benefit of not necessitating to train new Resnet-50 networks.\nb) I would be interested to see some samples generated with the best truncation for replacement, and for addition, in order to have a better idea of what kind of \"diversity\" we are talking of (this could replace one of the two subplots of Fig. 2 since top1/top5 follow the same trends).\n\nOverall, I think this paper and its experiments is a nice contribution to the workshop.", "rating": "0", "sentences": [{"sentence_type": "positive", "sentence": "This \"data-replacement\" experiment is very natural to make, therefore I am glad it was done."}, {"sentence_type": "positive", "sentence": "The metric for identifying BigGAN failures per class is also an interesting byproduct."}, {"sentence_type": "positive", "sentence": "Indeed, even if per-class FID could perform worse, it has the great benefit of not necessitating to train new Resnet-50 networks"}, {"sentence_type": "positive", "sentence": " I think this paper and its experiments is a nice contribution to the workshop"}], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Rafael"], "models": [""]}}
{"id": "BJxd_c3AdV", "review": "This paper investigates the representations learned by RNN-based RL agents for solving structured prediction problems when trained with both policy gradient and value-based methods. It studies the conditions leading to state aliasing and highlights strategies to prevent this situation. The hypothesis is that state aliasing happens when different states share the same optimal action and can result into a failure to converge to the optimal policy. The authors study this phenomenon using LSTM and GRU networks on synthetic (toy) environments, and validate their hypothesis.\n\nThe paper is clear, well written, and very relevant to the workshop.", "rating": "0", "sentences": [{"sentence_type": "positive", "sentence": "The paper is clear, well written, and very relevant to the workshop."}], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Rafael"], "models": [""]}}
{"id": "r1lokTbf5H", "review": "This paper highlights the problem of model overlearning - learning more than it is trained to do. Thus, there is leak of privacy and sensitive attributes of images during test/ inference time.\n\nPros:\n1. The paper is well written and easy to follow\n\nCons:\n1. There is very little novelty in this paper - the notion of overlearning is well established in the literature (Osia et al., 2018; Chi et al., 2018; Wang et al., 2018). This paper merely reinstates, what is being already told in the literature.\n\n2. In fact, there are many defence mechanisms proposed in the literature, for example \"Anonymizing k Facial Attributes via Adversarial Perturbations\" IJCAI 2018 - where the authors are performing data perturbations to minimize overlearning. This paper does not suggest or propose any method for solving the issue of overlearning\n\nIn summary, this paper repeats a well established problem of overlearning, showing experiments that are already shown in literature with known datasets, and also NOT proposing a solution to minimize overlearning (as many papers already proposed in literature). \n\n3. Additionally, the experiments are very weak - the authors still perform experiments using LeNet variants and AlexNet, and for text using a textCNN. Why did the authors not perform experiments using more state-of the art CNN/RNN models. Did they not observe overlearning in these models?\n\n4. As for section 4.4, it is pretty understood that lower layers of a DL model, learns very basic low-level features from the images such as edges, corner. Reinstating that, and calling it the reason for overlearning is not very convincing. \n\nAs of now, I find the paper very weak, till a solution to avoid overlearning is not proposed as a part of this paper.", "rating": "1", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Rafael"], "models": [""]}}
{"id": "r1x0L5JR94", "review": "The paper introduces a novel way of explaining why a planning problem is unsolvable, via a combination of choosing an abstraction level (in a hierarchy of projections: the maximally abstract version which is still unsolvable) with generating the first unsolvable landmark at that level (landmarks being taen from the maximally concrete solvable version). The paper conducts first user studies and runs some computational experiments.\n\nOverall, this is a nice contribution at a mature stage. Definitely an accept for XAIP. The issues I see are:\n\nThe introduction makes much too broad claims about the originality of the problem addressed. The authors pretend that this is the first work aiming at explaining unsolvability to humans. However, the 2010 work by Goebelbecker et al had exactly the same aim. It's got \"excuse\" in the title but says \"explanation\" already in the abstract. In any case, this is the exact same objective. The techniques suggested are quite different, and so everything is fine content-wise. But the presentation needs to be corrected. \n\nFurthermore, although the abstraction part of this work relates very closely to previous work by Chakraborti et al on model reconciliation, that work is mentioned for the first time on the last page of the paper. It must be mentioned in the introduction already. \n\nA technical issue is that the constraint-compilation sigma(M) may not actually be defined for a more abstract model, namely if the predicates/planning-state properties referred to in the automaton's transition function are abstracted away. So Proposition 5 is flawed as stated. Actually the automaton has to be defined relative to a task to even be able to formulate this problem. In this sense, Proposition 4 also is flawed as stated. I believe this issues are fixable so don't view them as reason to reject. But they mst be fixed, and it needs to be made clear how this will be done. \n\nIt also must be made much clearer in Section 4 that, and exactly how, \"the methods discussed in earlier sections\" can be used. \n\nI disagree that there is \"no direct way of extracting meaningful subgoals\" (introduction; statement repeated in similar way later on) just because a planning problem is unsolvable. This actually seems to me one more instance of a tendency to formulate claims too broadly. Simple counter-example: What about shared preconditions? If g is a goal and p is a precondition of all actions achieving g, then certainly p is a \"meaningful subgoal\", even if g is unsolvable. Right? Such necessary subgoal analysis is actually all over the place in the landmarks literature. Don't get me wrong, I like your solution to look at the first solvable abstraction layer and use standard landmark definitions there (which indeed per se rely on solvability). But you need to put your approach into the right frame. It is one possible approach. Not the only possible one. \n\nMinor comments:\n\n \"we can also use these subgoals to produce exemplar plans and illustrate their failures alongside the unachievable subgoals\" ==> How so? These subgoals are unsolvable after all. Are you referring here to the generation of abstract plans and pointing out failures in more concrete models?\n\nDef 3 last Pi has an M subscript missing?\n\n\"function free fragment of classical planning\" does not translate to an unambiguous definition in my mind. Do you mean continuous state variables? Or fo you intent to refer to functional strips here as well? If you include statements like this, please be precise.\n\nI'm a little uncertain about the significance of the user studies' results. Playing the devil's advocate, I would surmise that the alternatives provided to the landmarks, based on example plans, are just bad in an obvious/trivial way. The vast majorities of user votes to this effect certainly agree with that perception. Please add a few more words discussing your views/your conclusions from these studies.\n\n\n\n", "rating": "1", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Rafael"], "models": [""]}}
{"id": "HkecY49WqE", "review": "This paper proposes a Transformer-based architecture for generating an amino acid sequence for a protein, given its 3D structure.  The authors define custom geometric features, and feed it to a model that has elements of a Transformer and graph convolutional neural network.\n\nThe main weakness is that the experiments section is limited:\n- Direct comparison with graph convolutional neural networks is missing, despite this being a more standard way to do deep learning over graphs.\n- There should be ablations of the different features explored, and perhaps comparisons to simpler featurization schemes that have been proposed in the past. \n- There is a comparison with SPIN2, but there are some weird methodological issues, namely that pseudocounts were added post hoc to prevent infinite perplexity. There should be a way to fix numerical stability issues directly, without having to add these pseudocounts.\n- RNN baselines seem to basically learn unigram frequencies, which either suggests they were not tuned properly or that they were too weak baselines, and some slightly better baselines should also be explored.\n- The task of mapping structure to amino acid sequence was motivated by the goal of protein generation. However there is no actual evaluation of the generated sequences, only perplexity.\n\nThere were also some points of confusion:\n- In 2.1, the authors mention that they can handle both \"rigid backbone\" and \"flexible backbone\" problems, but then exclusively discuss the rigid case. Since their featurization depends on having the 3D coordinates of all backbone amino acids, which seems to be a hallmark of the \"rigid\" setting, it is unclear how this extends to the \"flexible\" setting.\n- It is unclear how the decoder works, especially because the j-th amino acid sequence is added to the edge features e_{ij}. This would seem to make it hard to use the standard masking trick to decode--do you have to recompute the entire set of features for each step of decoding?", "rating": "1", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Rafael"], "models": [""]}}
{"id": "mYUxvVthARd", "review": "The paper presents a specific diagram of COVID-19 data tracking, monitoring, and collection. The work is practically meaningful and valuable to various communities for future studies:\n\n1. The paper presents a clear and comprehensive process from collecting test samples to final dashboard exhibitions, which provides valuable paradigm experience for data collecting and processing, particularly for college and education communities.\n\n2. The collected data are valuable for public policy and AI modeling communities. For instance, the work uses Wifi data for individual monitoring and contact tracing, which may help establish contact networks and provide a better understanding of how disease can spread within schools. \n\nDespite the meanings of the data collection process, we wish to understand more about the collected data. For instance, it would be good to include non-private or non-sensitive statistical analysis and visualization of the data for the presentation or the final paper. ", "rating": "0", "sentences": [{"sentence_type": "positive", "sentence": "The paper presents a clear and comprehensive process from collecting test samples to final dashboard exhibitions, which provides valuable paradigm experience for data collecting and processing, particularly for college and education communities."}, {"sentence_type": "positive", "sentence": "The collected data are valuable for public policy and AI modeling communities. For instance, the work uses Wifi data for individual monitoring and contact tracing, which may help establish contact networks and provide a better understanding of how disease can spread within schools"}], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Rafael"], "models": [""]}}
{"id": "HJgrkphAcS", "review": "The authors present and analyze a quantum computing algorithm for learning GMMs.\n\nI think this paper cannot be accepted because it violates formatting guidelines. Also, I think it is not appropriate for ICLR since it assumes knowledge of quantum computing that most people at this conference would not have, and I as a reviewer do not possess, and hence cannot evaluate this paper. For example, I do not know bra-ket notation.\n\nIf the ACs disagree, I am happy to revise my review for this paper and try to be more thorough.", "rating": "1", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Rafael"], "models": [""]}}
{"id": "r1e9Lo4-n4", "review": "This paper is well written and motivated. The paper presents an approach that details how the model reconciliation process can be used for deception. This includes explanations that are in fact, lies. The paper considers lies of omission and commission, as well as multiple example walkthroughs. Highly relevant to the workshop and should spark interesting discussion.\n\nThis paper raises a number of very interesting questions for human-robot interaction:\n\nWould humans prefer a lie rather than the truth if it's simpler? Perhaps some of the best explanations , under certain circumstances, could be lies?\n\nIn the conclusion the paper states that most of the deceptive behavior described in the paper needs to be explicitly programmed. I wonder if a more developed agent capable of rebelling might lie to enforce ethical principles, perhaps in the case the goal asked of the agent is one of ill-intent. In such circumstances, the rebellion process might inform the deceptive behavior without it having to be explicitly programmed.", "rating": "0", "sentences": [{"sentence_type": "positive", "sentence": "This paper is well written and motivated."}, {"sentence_type": "positive", "sentence": "Highly relevant to the workshop and should spark interesting discussion."}, {"sentence_type": "positive", "sentence": "This paper raises a number of very interesting questions for human-robot interaction:"}], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Rafael"], "models": [""]}}
{"id": "S1ZbRMqlM", "review": "The paper suggests taking GloVe word vectors, adjust them, and then use a non-Euclidean similarity function between them. The idea is tested on very small data sets (80 and 50 examples, respectively). The proposed techniques are a combination of previously published steps, and the new algorithm fails to reach state-of-the-art on the tiny data sets.\n\nIt isn't clear what the authors are trying to prove, nor whether they have successfully proven what they are trying to prove. Is the point that GloVe is a bad algorithm? That these steps are general? If the latter, then the experimental results are far weaker than what I would find convincing. Why not try on multiple different word embeddings? What happens if you start with random vectors? What happens when you try a bigger data set or a more complex problem?", "rating": "1", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Rafael"], "models": [""]}}
{"id": "eZwM9hTGFx", "review": "This manuscript introduces a new approach for transfer learning of multi-label classification models. The paper is clearly written and presents compelling results on the efficacy of the method, especially on hard instances where images contain multiple objects with significant size differences.", "rating": "0", "sentences": [{"sentence_type": "positive", "sentence": "The paper is clearly written and presents compelling results on the efficacy of the method, especially on hard instances where images contain multiple objects with significant size differences."}], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Rafael"], "models": [""]}}
{"id": "6ndJvDE9NTv", "review": "I welcome the general direction of the work: trying to understand the dynamics\nof neural networks is a complex undertaking that a large community of\nresearchers is pursuing, so the study of simplified models is a promising\navenue.\n\n### Setup of the study\n\nIn studying the *limiting* dynamics however, the authors limit themselves to a\nsetup where I don't see any immediate connections to learning or representations\nof neural networks. The dynamics the authors describe happen *after* resuming\ntraining of a pre-trained neural network, thus I feel like their setup restricts\nthe potential impact of the results of this study.\n\n### Clarity / novelty of the results\n\nI found the article hard to read at times because the authors repeatedly qualify\ntheir observations as \"surprising\", \"contrary to common intuition\",\n\"nonintuitive\", etc. I think these qualifiers can be mistaken as claims of\nnovelty etc. and would hence use them more sparingly. For example, the fact that\nneural networks continue to move through their weight space has been\nwell-established for quite some time now, cf. for example [Jastrzebski et\nal. '17, Chaudhari & Soatto, 18, Baity-Jesi et al. '18, and many more]. Hence I\ndidn't find the observation in Figure 1 \"surprising\" (p 2 after the equation),\nwhich underlines the subjectiveness of these claims - or else I maybe reading\nthe figure incorrectly?\n\nAnother example: I would consider it well-established that OU processes whose\ndiffusion matrix is not isotropic do not follow the naïve Gibbs distribution,\nbut instead equilibrate in a modified potential (see for example Section 5.3\n\"Potential conditions\" of Gardiner's \"Handbook of stochastic methods\" etc.)\nFurthermore, modified losses arising through SGD dynamics have been studied in a\nnumber of recent deep learning papers, some of which are cited by the authors \n\nOther claims about the significance of the results should equally be clarified\nin my opinion, for example:\n\n> The expectation that the training trajectory would reflect the underlying \n> anisotropy of the training loss driving the dynamics is also wrong;\" (p. 9)\n\nIn my understanding, this study is only concerned with the *limiting* dynamics\nof learning, and hence conclusions about the training cannot be drawn\nimmediately?\n\n### Separation of experimental from theoretical results\n\nThe authors should be lauded for trying to connect their theoretical results\nwith empirical results on deep networks. Again though, I think the presentation\nof the results should be revised to clarify which predictions are actually\nderived from theory.  Take the exponent of anomalous diffusion (bottom of Fig\n6): it cannot be estimated of the global displacement (12), as the authors\nexplain. Instead, the authors evaluate the dependence of the diffusion constant\non learning rate, batch size and momentum parameter directly from a simulation\nby fitting a power-law to the empirical displacement. I would present this\nresult separately, as it is not a theoretical prediction, and thus presenting it\nin a section entitled \"Predicting the diffusive behaviour of the limiting\ndynamics\" could cause confusion in my opinion.\n", "rating": "1", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Rafael"], "models": [""]}}
{"id": "S1eu25Nqtr", "review": "The authors present an algorithm for postprocessing neural networks to ensure calibration under domain shift.\nCalibration under domain shift is an interesting challenge that has been receiving increasing attention and tackling this in an unsupervised manner is an interesting approach. However, I have 2 major concerns regarding the approach presented by the authors.\n\nWhat makes calibration under domain shift useful and appealing is that the model is then robust against any changes in the test distribution that can occur during the life cycle of a model. These often include erroneous/samples (corresponding to truly OOD samples), but also gradual domain shift, where the test distribution continuously moves away from the training distribution (e.g. due to a continuous drift in user behaviour/change in customer base) or unforeseen changes. My first major concern is regarding the requirements for UTS, which render this approach not very useful in many of these practical  applications: UTS first requires knowledge of and access to the test distribution; in addition it assumes that the distribution of the labels remains unaffected under domain shift. These assumptions are violated in the practical applications described above, in particular those where a gradual, continuous domain shift occurs - in this case, access to the test distribution is difficult since it changes continuously. On this note I also would have liked to see some analysis on how performance depends on the number of samples that are available from the test set, since in practice this might be substantially smaller than the full test set used.\nFurthermore, I find the assumption that the distribution of labels remains unchanged problematic (q_s(y) = q_t(y) and even q_s(y|x)=q_t(y|x)): once sufficiently out-of-domain, labels become meaningless and predictions for truly OOD samples should have maximum entropy. Even for small domain shifts in practical applications it is not clear why q_s(y|x)=q_t(y|x) should hold and it would have been useful to see a discussion and some robustness analysis on this.\nFinally,  the algorithm requires re-calibration whenever the test distribution changes, which in practice is  often not clear (and part of the reason why dealing with predictions under domain shift is so challenging). \n\nIn addition to doubts on practical applicability, my second major concern is regarding the depth of the evaluation.\nFirst, while the authors present some comparisons to probabilistic methods, I am missing a crucial comparison to Evidential Deep Learning (Sensoy et al, NeurIPS 2018), which results in far superior performance than deep ensembles, SVI or dropout. Importantly, the comparisons to probabilistic approaches presented by the authors are very limited. The big advantage of those approaches is that, once trained, no further recalibration is necessary and well calibrated predictions can be made for any level of domain shift, whereas UTS requires a recalibration step for very level of domain shift. That is why I think it is crucial to not only show one arbitrarily picked level of domain shift for each dataset/perturbation, but calibration across all levels of domain shift, as for TS and TS-Target; since no recalibration is required for those probabilistic approaches \n this is very straight-forward and would be very informative - especially since e.g Figure 5 shows that UTS has only very minor advantages over TS in many settings. \nI appreciate that the authors report some performance in terms of ECE in the supplement, but I think it would be very informative to report performance in terms of ECE for all domain-shift experiments: The Brier score conflates accuracy with calibration (see eg the 2 component decomposition), whereas ECE directly quantifies calibration and is hence easier to interpret and arguably the more meaningful measure when quantifying calibration. \n\nMinor:  I find the manuscript lacks clarity. Aspects such as the definition of calibration as well as implications and interpretation of Proposition 1 should be described in more detail in the manuscript. \n", "rating": "1", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Rafael"], "models": [""]}}
{"id": "ByewhdO65r", "review": "The paper proposes to do a coupled inference over pairs of geographically close images instead of a single image for satellite imagery. The coupling is done with an average pooling of the feature vectors when the neighbouring patches are detected to be similar enough based on a threshold on the L2 distance of these features. The method is applied to tasks of estimating crowding population, and diseases density, from satellite images. \n\nThe paper have little novelty. The approach reduces to a smoothing method over pairs of neighbouring patches, that is only activated sometimes based on a hard threshold. This seems arbitrary and there are many competing approaches that could be applied. \nOne could think about taking all the geograpical neighbourhood of a patch into account when making a prediction, e.g. with a coarse-to-fine prediction approach; the aggregation of features can be learned and more sophisticated than average pooling. Using a single-image baseline is not fair. The discussion is not up to the level of ICLR and offers mostly guesswork.", "rating": "1", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Rafael"], "models": [""]}}
{"id": "B1xt4BU9Fr", "review": "Update: I have read the author responses. As mentioned before I don't have the background to carefully assess the experiments and will have to rely on my fellow reviewers here. However I stand by my opinion that the experimental results would need to be very strong to warrant an acceptance, since the conceptual contribution is relatively limited.\nI was also disappointed by the authors unwillingness to back up their claims of \"theoretically convergent\" with a proof, or at least a theorem. Therefore I still tend towards rejecting the paper.\n-----------------------------------------------\nSummary\nThe present work proposes to use the recently developed Stochastic Gradient Langevin Dynamics (SGLD) to compute mixed approximate equilibria in two-player reinforcement learning, following the methodology proposed by hsieh et al for generative adversarial networks. The authors report practical improvements compared to pure strategies computed as proposed by Tessler et al.\n\nDecision\nThe idea of using randomized strategies for two-player reinforcement learning is interesting and natural. However, as the authors note, mixed strategies are classical in game theory. Furthermore, the adaption of the methodology of hsieh et al is straightforward, limiting the strength of the theoretical contribution. \nUnfortunately, the theoretical contribution is not stated consistently, since in the introduction, the paper states \"Our paper precisely bridges this gap between theory and practice in previous works, by proposing the ﬁrst theoretically convergent algorithm for robust RL\", but this claim is missing in the abstract or conclusion, and there is no theorem that justifies this rather strong claim. \nIn my opinion, this paper should only be accepted if it provides very convincing numerical experiments, which I am not qualified to assess. I happy to increase my score if the experimental results are deemed strong by the reviewers with more expertise in practical reinforcement learning.\n\nQuestions to authors\nYou write \"Our paper precisely bridges this gap between theory and practice in previous works, by proposing the ﬁrst theoretically convergent algorithm for robust RL\". What is the exact mathematical statement here? Does this refer to the algorithm that is used in the numerical experiments?", "rating": "1", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Rafael"], "models": [""]}}
{"id": "B1eRGEa1tV", "review": "Summary: This paper discusses some features of simple_rl, a framework for RL in Python that emphasizes simplicity and tools for reproducibility.  This is a nice workshop paper but would benefit from a clearer discussion of the related work.  \n\nNotes: \n  -SimpleRL is an algorithm for RL experiments in Python.  \n  -After creating agents and MDPs, an experiment log as a json is produced.  \n  -Practitioners can share a copy of the experiment file to ensure reproducibility.  However with docker or containers this should always be achievable, unless there’s some guarantee that all of the seeds are in the experiment file?  \n  -Consists of MDP objects and agent objects.  \n  -Main design goal is simplicity.  \n  -MDP has “transition function” and “reward function” objects.  I wonder how well the structure generalizes to model-based RL?  \n  -Some utilities for reproducing results from the json files.  \n  -Plotting utilities are included.  \n\nComments: \n  -Section 2 could do a better job of making it clearer how the simplicity of simple_rl isn’t achieved by the other libraries.  Nonetheless, it’s still a nice overview.  \n", "rating": "0", "sentences": [{"sentence_type": "positive", "sentence": "This is a nice workshop paper but would benefit from a clearer discussion of the related work."}], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Rafael"], "models": [""]}}
{"id": "OUqoXfqliy", "review": "This paper proposes a method for automatically generating accompaniments using Mel-spectrograms as inputs to a CycleGAN. Overall I think the paper requires significant revision and additional work before it can be accepted as a conference publication. \n\nTitle: \n\n-The title is misleading. The title claims that the proposed model is for \"Automatic Music Production\". However the actual task considered is more restrictive. The authors propose a model for automatic accompaniment. Music Production involves many other tasks like mixing, mastering and so on, none of which are a part  of this study. The title should therefore be updated to be more specific. \n\nAbstract: \n\n-\"Despite consistent demands from producers and artists...\": I think this sentence should be rephrased to motivate the need for automatic accompaniment from a different angle. If not, the authors should present some justification for the demand for this technology from artists and producers. \n\n-\"Automatic music arrangement from raw audio in the frequency domain\": why not simply say automatic music arrangement/accompaniment in the Mel-frequency domain? I find the raw audio part of the description unnecessary and confusing. \n\n-The authors claim that the they are the first to treat music audio as images and then apply techniques from computer vision. However, treating spectrograms as images is the current standard for many MIR tasks like music transcription, chord recognition and so on e.g. \"An end-to-end Neural Network for Automatic Music Transcription\": https://ieeexplore.ieee.org/abstract/document/7416164/. There are hundreds of other publications that are similar to this approach. \n\nIntroduction: \n\n-The authors claim that automatic accompaniment in the waveform/frequency domain has many advantages. However they fail to motivate the short-comings of this approach. Namely the lack of source separated training data and the extreme difficulty in source separation for music recordings. It  would also be useful to cite a review paper or some of the many publications on automatic accompaniment generation in the symbolic domain so that the reader can find references to this problem which has an extensive literature already. \n\n-The authors mention that they use the Demucs algorithm for source separation. However they do not provide any details whatsoever about this approach, especially the downsides. A quick scan of the paper reveals that the algorithm introduces severe artefacts under various conditions. \n\n-The authors mention the low-computational cost of their proposed method, however they do not satisfactorily quantify this claim. Firstly, is computational cost an issue? Does this algorithm have to run on a mobile device? Will it be run in a streaming setting? These questions are not answered in the paper. \n\nRelated Works:\n\n-The authors cite many papers on music generation in the waveform domain however they do not cite any of the extensive literature on music generation in the symbolic domain. This literature is extremely relevant to the work presented in this paper. \n\n-\"Nevertheless, only raw audio representation can produce, at least in the long run, appealing results in view of music production for artistic and commercial purposes.\" Why is this the case? Why is generating music in the symbolic domain and then using state-of-the-art synthesisers not an appealing direction? This point isn't made clear in the paper. \n\nMethod:\n\n-There are no details provided about the Demucs algorithm used to separate the source training data into various channels like vocal, bass, drums etc. How big was the model? Did the authors train the model themselves? Did they use a pre-trained model? Were there any artefacts present in the source separated tracks? Are there any downsides to this algorithm? Are there any alternatives to this algorithm? Do the artefacts not interfere with the  downstream task? \n\n-A reference/citation about the Mel scale would be useful. \n\n-There are no details about the CycleGAN used in the paper. How big is the model? What is the architecture? How was it trained? What flavour of gradient descent was used for training? What are the hyper-parameters? Was the model trained on a single GPU? \n\nExperiments:\n\n-How was the subset of pop music selected? How was the metadata filtered to obtain the 10000 tracks used for training? If the filtering algorithm cannot be outlined, then it would be useful to provide a list of the 10000 tracks used for training, for the purpose of reproducibility. \n\n-How did the authors arrive on the 4 attributes quality, euphony, coherence and intelligibility? Is there some theory that suggests that these 4 attributes would be useful in determining whether the accompaniment is somehow good? These attributes have been presented without justifications and citations. \n\n-The features (STOI, FID) used to compare the automatically generated accompaniment have also been presented without much justification. Why is it that these features  are an adequate representation of the generated audio? \n\n-I found the description of the grades and the subsequent comparison in Figure 3 difficult to follow. I think the description needs to be significantly more rigorous. \n\n", "rating": "1", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Rafael"], "models": [""]}}
{"id": "cwKHmeHuKq", "review": "Overall seems fine to me. Some nits:\n- Any sort of model outside the ResNet family would have been nice to see.\n- Figure 1 seemed crowded and it would have been useful to exclude some of the less necessary lines. Also the two different styles of dashed lines are hard to pick apart at first. Is it necessary to visualize the trends across \\rho, or could you just have used the best \\rho value and put them in a table?", "rating": "1", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Rafael"], "models": [""]}}
{"id": "6aOeaNg6_Ni", "review": "This paper aims at merging Imitation Learning (IL) and Reinforcement Learning (RL) from high dimensional sensory inputs so that the agent can make use of expert trajectories, even when the experts are suboptimal. This paper aims at addressing this by theoretically using the \"Free Energy Principle\", which the authors define in the abstract as a unified brain theory that explains perception. \n\nThe paper tackles an important problem and develops many theoretical functionals. The approach is then tested on a few benchmarks from the DeepMind Control Suite where the approach is reported to work well.\n\nMy main concerns are related to the clarity of the paper, which does not allow me to understand some key parts.\n\nIt is unclear what are the minimized loss functions: it is mentioned that equations 27-29 sum up all objective functions. Since F_t and G_{t+1}^{RL} are appearing twice, does it mean that those losses have a two times more important contribution than for instance G_{t+1}^{IL}? In addition how exactly are equations 27-29 derived from the other equations?\n\nNotations are not always consistent:\n- F_{IL} F_{RL} are sometimes with curly F sometimes not (equations 27-29 and the few lines that follow Figure 1 page 5).\n- First line equation 7 and first line equation 10: Why is there a subscript for F and not G? In other parts of the paper, a subscript is used for G as well.\n\nSome sentences are unclear:\n- \"in RL, using the value function to predict rewards in the long-term future is essential to avoid a local minimum and achieve the desired goal.\" What kind of local minimum does that refer to?\n\nA few typos:\n- \"Then We (...)\"\n- \"the agent easily fall down\"", "rating": "1", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Rafael"], "models": [""]}}
{"id": "QEkFnN7VACb", "review": "Summary: \n\nThis paper proposes Deep Coherent Exploration that unifies step-based exploration and trajectory-based exploration on continuous control. There exists a prior work that bridges a gap between the two exploration methods for linear policies, and this paper generalizes the prior work for various deep RL methods: on-policy (A2C, PPO) and off-policy (SAC). Finally, Deep Coherent Exploration enhances the performance of baseline algorithms and has better performance than prior works (NoisyNet, PNSE) on Mujoco tasks.\n\nPros:\n\n+ For combining the proposed method with on-policy learning, this paper derives the log-likelihood of whole trajectory recursively.\n+ For on-policy methods (A2C, PPO), the proposed method has large performance gain on Mujoco tasks.\n\nCons:\n\n- The idea of this paper directly follows GE [van Hoof et al., 2017] and is not much different from GE.\n- For SAC, the proposed method is not much effective and it even degrades the performance of the HalfCheetah task.\n- The paper focuses on exploration, but the experiments only focus on the return performance of simple Mujoco tasks.\n- In order to show the superiority of the proposed method, additional experiments on pure exploration or sparse rewarded tasks are needed.\n\nMinor concerns:\n\n* In background, there is no explanation about step-based and trajectory-based exploration.\n* For the off-policy case, there is insufficient explanation for why they use single sigma and the connection point of the proposed method and eq (5).\n", "rating": "1", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Rafael"], "models": [""]}}
{"id": "SkxNwZAPFN", "review": "Pros:\n- SOTA results\n\nCons:\n- generally written in an unclear way \n- Title should say \"zero-shot\"\n- The indices of the covariance matrices in Fig 1 appear flipped\n- Figure captions lack important information (for example, in Fig 1 there is no mention that the bottom VAE is for the class embeddings, and they also use c in the figure but c(y) in the text)\n- not clear whether the results in Fig 2 for ImageNet are over multiple seeds, no error bars.\n", "rating": "1", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Rafael"], "models": [""]}}
{"id": "ByYX-qk3BPa", "review": "Strengths:\n\nThis paper evaluates the success of deep neural networks by running extensive experiments that show the tradeoff between space and frequency. \n\nWeakness:\n\nThe meaning of the word frequency is unclear in the context of this paper.\n\nThe main idea behind theorem \"eigenspace restructuring\" which replaces one eigenvector/value computation with a set of part based eigenvalue/eigenvectors  has limited novelty.  The novelty is in its use in neural network architecture analysis.\n\nThe authors might want to consider evaluating the performance of a system based on the amount of data employed and based on how data was acquired (observational vs experimental studies).  Systems that employ data from observational studies are known to be susceptible to selection bias and spurious correlations, as opposed to data from experimental studies that are employed in objective causal inference.  \n\n\nMissing reference:\nAs the authors might recall, Vasilescu etal. in their ICPR 2020 paper have advocated replacing the SVD computation with a set of part based SVDs for which they provided a closed form mathematical derivation.  Based on this mathematical derivation, they developed  the Incremental Hierarchical M-mode Block SVD which was demonstrated experimentally. \n\n@inproceedings{Vasilescu20,\n\nauthor={Vasilescu, M. Alex O. and Kim, Eric and Zeng, Xiao S.}, booktitle={2020 25th International Conference of Pattern Recognition (ICPR 2020)}, title={Causal{X}: {C}ausal e{X}planations and {B}lock {M}ultilinear {F}actor {A}nalysis}, year={2021}, location={Milan, Italy}, month={Jan}, pages={10736--10743} }\n\n\n\n", "rating": "1", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Rafael"], "models": [""]}}
{"id": "SL9g5Bq5-bc", "review": "This paper extends existing casual learning methods to the nonstationary video cases, where the casual structures are changing throughout time. The proposed model leverages a recurrent model GRU to extract temporal information, which is intuitively correct because this will capture the changes of casual graphs over time. Since there is no GT casual graph as direct supervision, the authors employ state-space model and perform variational inference on it. The proposed model achieves superior performance compared to one previous work on one dataset\n\nDetailed comments:\n- I completely agree that the evolution of casual structure in nonstationary videos is indeed an important and new problem. The usage of GRU to tackle this problem seems reasonable to me\n- However, since there is no GT casual graph in the dataset, the author doesn't visualize the casual graph learned in any case. It's unclear to me whether SSM and eq3 can really help the model learn the correct underlying casual graph\n- There are many typos/writing errors in the paper. At the beginning of Section 2, the referred model architecture figure is missed. In Table 1, the baseline V-CDN is mentioned but missed in the table\n\nNonetheless, I still believe the discovery of casual structure in nonstationary videos is an important paper. So I recommend acceptance of this paper to the workshop.\n", "rating": "1", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Rafael"], "models": [""]}}
{"id": "5rxugSbXWL", "review": "This paper proposes an efficient method for survival analysis using neural networks. I'm not familiar with the topic, but the paper is well-written and initial results seem to look good. ", "rating": "0", "sentences": [{"sentence_type": "positive", "sentence": "This paper proposes an efficient method for survival analysis using neural networks. I'm not familiar with the topic, but the paper is well-written and initial results seem to look good. "}], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Rafael"], "models": [""]}}
{"id": "S1lulXBZ5N", "review": "In this paper, the authors investigate generalizing VAEs to problems where correlations can be found between data points. They leverage known results (namely, writing the  distribution on tree-shaped graphical models as a function of pairwise joint distributions and marginals; for general graphs, they use mixture of tree distributions in a fashion reminiscent of tree-reweighted belief propagation) from graphical models to derive tractable evidence lower bound for the problem of interest.\nThey report improved results on a variety of datasets (spectral clustering, collaborative filtering and link prediction) compared to vanilla VAEs and a graph net based approach.\n\nThis is a good paper; the ideas are original, technically interesting, and well presented (there are some issues with language but nothing distracting), and results are convincing.\n\nThere was a slight missed opportunity in explaining in further details how the new bound interacted with sampling from the posterior (this is currently hidden in appendix D: the authors leverage properties of gaussian distributions; in general it would be more complicated to sample from the joint knowing only pairwise/marginals).\n\n\n", "rating": "0", "sentences": [{"sentence_type": "positive", "sentence": "This is a good paper; the ideas are original, technically interesting, and well presented (there are some issues with language but nothing distracting), and results are convincing."}, {"sentence_type": "positive", "sentence": "There was a slight missed opportunity in explaining in further details"}], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Rafael"], "models": [""]}}
{"id": "gRWTAgk8c79", "review": "The paper's presentation is very clear and I appreciate the idea to combine meta-learning and the options framework, but I do not think that the experiments fully demonstrate the potential of the method. First of all, the experiments in the paper focus on navigation. Ant Maze environment is not a challenging task for other hierarchical RL methods. The difficulty comes from forbidding the agent from observing the maze layout. However, since the maze size is rather small, the number of possible layouts is also small. It is possible that that the agent simply remembers the solution to all map layouts and uses the meta-learning trajectories to identify the maze layout. The experiments do not demonstrate the generalizability of the novel map layout. In this case, why not simply learn a recognition model to reconstruct the maze layout [1] and then learn a goal-conditioned policy? The approach is not significant unless it can generalize to a novel layout in a larger maze. Otherwise, I would doubt that we should use search instead of meta-learning for solving complex planning tasks. Besides, since the number of options is small, and the results are not very interpretable, if we need options is questionable to me. The paper lacks a MAML-like meta-learning baseline to demonstrate that options are necessary.  \n\nBesides, I am confused by the plots in the paper. Fig 4 shows that FAMP reaches 1200+ score after 40 episodes, but according to the pseudocode and Table 2, in each update step of the meta-agent, it needs to sample N\\times L+1\\times k episodes, which is larger than 40. Does this mean that there is no need to learn the meta-agent?\n\nIn short, the paper has good motivation, however, the experiments have flaws and are not strong enough to demonstrate the potential of their approach. I hope the authors can apply their approach to more complex domains and show its significance.\n\n[1]Multi-task Batch Reinforcement Learning with Metric Learning", "rating": "0", "sentences": [{"sentence_type": "positive", "sentence": "The paper's presentation is very clear and I appreciate the idea to combine meta-learning and the options framework, but I do not think that the experiments fully demonstrate the potential of the method."}, {"sentence_type": "positive", "sentence": "In short, the paper has good motivation, however, the experiments have flaws and are not strong enough to demonstrate the potential of their approach."}], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Rafael"], "models": [""]}}
{"id": "Hkl_WX9DjE", "review": "This paper describes three alternate approaches for scheduling awake and sleep cycles for a space rover.  While awake the rover uses more energy than it produces and can only recharge its resources when it is asleep.  The three approaches vary in terms of soundness and completeness.   All approaches are sound but only one is complete.  After presenting the approaches an empirical evaluation is given comparing both the quality and the runtime of the solutions.  The evaluation shows that the naïve max duration approach. which is sound but not complete,  performs worst in terms of solution quality and runtime.  While the sound and complete linear approach produces the best quality solutions its runtime is worse than probe approach which has comparable quality.   \n\nPlanning activities for a Mars rover is highly relevant to SPARK. \n\nThe initial figure could explain why it might be useful to extend an awake period while the rover is idle.  The figure clearly shows the two cases but gives no intuition as to why one case might be preferable to the other.\n\nThe probe approach depends critically on the heuristic choice of the probe point.  More discussion and/or an evaluation of strategies for choosing the probe point would be valuable given that the analysis points to the probe algorithm being a good compromise between quality and runtime,\n", "rating": "1", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Rafael"], "models": [""]}}
{"id": "Rh7bYa5xqD", "review": "The paper introduces the idea of using cyclical learning rate schedules to benchmark the performance of different algorithmic and model changes. Usually, benchmarking these methods requires training for multiple different budgets (e.g. for 16, 32, 64, ... epochs), but cyclical learning rates allow an approximation of these results with a single training run (for the full duration).\n\nThe paper introduces a neat and practically helpful method for faster benchmarking of algorithmic and/or model changes. The experiments show that the cyclical training can consistently provide a decent approximation of the \"true\" tradeoff curve while requiring only half of the training costs.\n\nFeedback:\n- Line 67 mentions that the experiments report top-1 validation accuracy. Judging from the results, I would guess that this describes the `final` achieved performance (i.e. what the final iterate produces) and not the `best` achieved performance (i.e. what the `best` parameters observed during training achieve), right? Otherwise, the tradeoff curves should never be decreasing (for the cyclical learning rate). However, in Line 73 you describe \"we then stored maximum accuracy values at the end of each cycle\". I don't understand what set the `maximum` refers to. I would be interested in seeing the results using the `best` performance instead of the `final` (would be sufficient to show it in the appendix). This would not require any further experiments.\n- Could you provide some more details on how you set the (upper) learning rate of both the standard and the cyclical training? Line 72 mentions using a value of 2.048 for the cyclical training. Could it be that some methods just work better with this learning rate (without providing better performance at a tuned value)?\n- I would be interested in hearing your thoughts on whether this type of benchmarking \"trick\" could also be used in other domains, besides ImageNet training. Would you anticipate any problems using the cyclical learning rate schedule in other domains?\n\nNits:\n- Line 56: Should probably use a \\citep, e.g. \"[Huang et al., 2017]\". This occurs multiple times.\n- Line 60: doubling of \"to\".", "rating": "0", "sentences": [{"sentence_type": "positive", "sentence": "The paper introduces a neat and practically helpful method for faster benchmarking of algorithmic and/or model changes."}], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Rafael"], "models": [""]}}
{"id": "K3Hdp5lKQwd", "review": "This paper proposes to simply learn/optimize the deep kernel parameters and hyperparameters of the GP using the data from all tasks (equation 9) and use such a deep GP kernel for BO. Instead of maximizing the log marginal likelihood of a single dataset (as is typically the case for a learning task), they propose to maximize the sum of log marginal likelihoods over the datasets of all tasks, which I view to be an incremental technical contribution. This paper is not about innovations in the acquisition function in BO. Their proposed approach outperforms the tested methods on 3 benchmark datasets.\n\nA drawback of their proposed method (equation 9) is that in contrast to some existing meta-BO algorithms, it cannot exploit the GP posterior means and variances when such information is available from previous BO tasks. \n\nConsidering the diversity of the types of datasets in the AdaBoost experiment, can the authors give an interpretation of the max likelihood estimates of theta and w in equations 8 and 9 in the context of this experiment?\n\nFor the GLMNet and SVM experiments, would it be possible to instead combine all the datasets over tasks and construct a *joint* likelihood over them in equation 9 instead of a sum of likelihoods over tasks? How would the results differ in this case?\n\nFor the experiments conducted, large amounts of data are drawn from many available previous BO tasks, especially for GLMNet and SVM. This seems to be in disagreement with the setting of BO where the unknown objective functions are expected to be costly to evaluate (e.g., in hyperparameter optimization). In the context of BO, it would be meaningful to consider the practical setting where only small amounts of data are available from a few expensive BO tasks. In this case, how would the proposed approach perform compared to the tested methods?\n\nAn empirical comparison with the state-of-the-art meta-BO algorithm: weighted GP ensembles (Feurer et al. 2018) should be included.\n\nFrom the results presented in Table 1 and Fig. 2, it does not seem like only a few shots/BO iterations are needed for the experiments performed in this paper. How do the results compare when only a few shots are used?\n\nEquations 9 and 10: Shouldn't the log marginal likelihood be over y's instead of f's?\n\nEquation 13: Exactly how is the loss function L(f^(t),X) defined? The author mentions that it is a normalized regret. The exact expression is needed here. In particular, I have noticed that f^(t) is not in bold: does this mean that only 1 \"test\" point is selected per task t?\n\nPage 6: Can the authors provide the exact details on \"the settings being sampled in proportion to their performance according to the predictions\"?\n\nThe authors can consider doing experiments on hyperparameter optimization of larger-scale CNNs, which is commonly seen in BO works.\n\n\nMinor issues\n\nPage 3: adaption or adaptation\n\nThe following reference would be relevant to the context of meta/transfer BO:\n\nZ. Dai, B. K. H. Low, and P. Jaillet (2020). Federated Bayesian optimization via Thompson sampling. In Proc. NeurIPS.", "rating": "1", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Rafael"], "models": [""]}}
{"id": "A39AOJMaA0", "review": "### Summary\nThe paper proposes a neuro-symbolic approach to solve reasoning tasks over KBs. The proposed LMLP approach uses logic rule templates and in-context learning of LMs for answering a relational query. To evaluate  LMLP, the authors conducted experiments on two datasets. In doing so, the paper aims to answer: what is the right representation for in-context samples? How does natural language explanation compared with symbolic provenance when acting as prompts? The conducted experiments show that eliciting LMs with logic rules and in-context learning, leveraging LM’s pre-trained knowledge, could be sufficient for solving reasoning tasks over KBs.\n\n\n### Strength\nThe proposed method is well described and supported by illustrations and examples, which makes it easy to understand.\n\nWell-written related work is compressed in the main text and extended in the Appendix.\n\nExperiments seem to be well conducted.\n\n### Weaknesses\nHowever, while the experimental results are displayed in Tables 2 and 3, and further details are discussed in the Appendix, a discussion in the main text is missing. \n\nSince I do not see other major weaknesses and the remaining parts are well written, and the experiments seem to be well conducted, I would still vote for acceptance if the authors add a discussion of the experiments similar to the ones in the appendix. I suggest moving Tables 3 and 4 to the Appendix instead.\n\nMinor:\nLine 171 Typo: We probes\nThe first bullet in the checklist: Section ??\nwhat is the right representations for -> \"representation\" or \"are\"", "rating": "0", "sentences": [{"sentence_type": "positive", "sentence": "The proposed method is well described and supported by illustrations and examples, which makes it easy to understand."}, {"sentence_type": "positive", "sentence": "Since I do not see other major weaknesses and the remaining parts are well written, and the experiments seem to be well conducted"}], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Rafael"], "models": [""]}}
{"id": "S1gn5jrZ2V", "review": "The paper looks at the problem of computing interventions from an external observer in the plan of an actor in the presence of an active or passive adversary. I had some trouble situating the work (though the related works section at the end certainly helps). \n\n> Assumptions: The assumptions made in the framework should be clearly stated. Section 3 starts with a list of them, but this is largely incomplete (e.g. some more appear in later sections and some more are implicit and are never mentioned at all). I would strongly suggest laying out what all the assumptions on each of the three agents are and how that impacts the intervention framework. Are the agents taking turns? \n\nThe actor seems to be a vanilla optimal agent, while the observer is bearing the computational burden of having to reason about attacks. This is different from agents that directly reason about the observer model (c.f. The landscape of Interpretable Agent Behavior [Chakraborti et al. ICAPS 2019]); This is the same trade-off in goal recognition design as well (with behaviors like legibility). I wonder what the differences are in the context of the intervention setting? Would be good to have that discussion in the related works.\n\nWhy are G_d and G_u part of the input? Given that the observer has the full model of both agents, interventions are exactly and precisely computable (c.f. Optimal Interdiction of Attack Plans [Letchford and Vorobeychik]). Why generate features and then classify? Is this purely a computational consideration or am I missing something?\n\nFinally, and this is minor, it feels very strange to see \"we use machine learning to determine...\" in a technical paper. This is akin to \"we used AI\" in a magazine. Why not just say what it is? A classifier, a decision tree, etc. \n", "rating": "1", "sentences": [], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Rafael"], "models": [""]}}
{"id": "-En07bsLWAs", "review": "This work is concerned with developing a model that can produce a manipulation program in order for a robot to manipulate its surrounding to reach a goal state given an initial state. The work focuses here on a neruo-symbolic approach that includes several specialized submodules. Despite these differences all submodules can be trained end-2-end without intermediate supervision. The experimental results are promising.\nOverall this is a well written paper with a very relevant topic and proposed method for this workshop. I am hesitant on giving this paper the highest score due to two reasons that are somewhat correlated. The overall methods section in \"Technical Approach\" is quite minimilistic and scarce in details. E.g. particularly the description of the visual extractor is kept very short. Even if it is mainly based on previous work I believe it would be helpful to have some more information overall in this technical approach section. This leads to one of the more important contributions of the work, the single loss function (section 3.5), to be less obvious, i.e. how the gradient flows through to all initial modules. And particularly in my view this is the most interesting feat of the work, i.e. that training all submodules works via this one loss. Maybe adding more explicit mathematical notations in Section 3 could help for the comprehensability here.  ", "rating": "0", "sentences": [{"sentence_type": "positive", "sentence": "Overall this is a well written paper with a very relevant topic and proposed method for this workshop."}], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Rafael"], "models": [""]}}
{"id": "vUrQcSFhoL5", "review": "I thank the authors for their submission. I believe the investigated content is relevant and timely and would perhaps benefit from the discussion in a community such as the one of ICLR. Please find my comments below, as potential points of discussion.\n\nHigh-level comments:\n* generally, the paper does a good review of existing literature and aims to relate two important subfields (abductive vs contrastive explanations) using rules of logic, particularly using the minimal hitting set relationship \n* beyond showing that such a duality holds between abductors and contrastive explanations, I believe the experimental section should further explore comparisons with related work such as Dhurandhar et al. (as cited in earlier sections) and Rebeiro et al.\n* furthermore, the idea of using FOL for generative contrastive explanations (also sometimes called counterfactual explanations) has been explored before (e.g., [Karimi et al.]);\n* on the presentation of material, there seemed to be an underwritten requirement to be familiar and have a background in logic and verification, which makes me wonder whether ICLR is the right venue here (I also apologize for not being to provide much feedback on the technical front)\n\nMinor comments and nits:\n- the footnotes seemed to contain important details, but the number of footnotes seemed overwhelming and hurt the flow of reading\n- [footnote 9] seems incorrect; e.g., non-linearities in MLPs or RBF kernels in SVMs cannot be encoded as first-order logic\n- perhaps figure 2 can be redone to more visually demonstrate the benefit of using the proposed method; if the provided explanations aren’t visually appealing (relatively), then perhaps consider a non-image-based dataset?\n\n[Ribeiro et al.] https://homes.cs.washington.edu/~marcotcr/aaai18.pdf\n[Karimi et al.] https://arxiv.org/abs/1905.11190 ", "rating": "0", "sentences": [{"sentence_type": "positive", "sentence": "generally, the paper does a good review of existing literature and aims to relate two important subfields"}], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Rafael"], "models": [""]}}
{"id": "S1gCKKWW3N", "review": "This is a well-written, clear, and relevant paper to the conference.  The paper introduces a framework for explaining scheduling and planning failures by generating minimal conflicts and relaxations. The work is well motivated by a description of planners at NASA and is used as a running example throughout the paper. The problem of explaining why a plan fails is a core focus of this workshop thus this paper will be highly relevant to the audience.\n\nSmall comments / grammar:\n\t* 6.1: \"natural language sentences out the explanations\" - \"out of\"?\n\n\n\n", "rating": "0", "sentences": [{"sentence_type": "positive", "sentence": "This is a well-written, clear, and relevant paper to the conference"}, {"sentence_type": "positive", "sentence": "The work is well motivated by a description of planners at NASA and is used as a running example throughout the paper."}, {"sentence_type": "positive", "sentence": "The problem of explaining why a plan fails is a core focus of this workshop thus this paper will be highly relevant to the audience.\n\n"}], "comments": null, "annotation_info": {"type": "Manual", "humans": ["Rafael"], "models": [""]}}
